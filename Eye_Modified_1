step 0 , training  accuracy 0.511728
step 0 , loss : 1.70827
step 100 , training  accuracy 0.511523
step 100 , loss : 0.702653
step 200 , training  accuracy 0.527572
step 200 , loss : 0.695423
step 300 , training  accuracy 0.518313
step 300 , loss : 0.692295
step 400 , training  accuracy 0.514198
step 400 , loss : 0.69316
step 500 , training  accuracy 0.49321
step 500 , loss : 0.69548
step 600 , training  accuracy 0.508642
step 600 , loss : 0.691876
step 700 , training  accuracy 0.552058
step 700 , loss : 0.69306
step 800 , training  accuracy 0.510494
step 800 , loss : 0.693141
step 900 , training  accuracy 0.533539
step 900 , loss : 0.692929
step 1000 , training  accuracy 0.534362
step 1000 , loss : 0.694054
step 1100 , training  accuracy 0.51358
step 1100 , loss : 0.694015
step 1200 , training  accuracy 0.50144
step 1200 , loss : 0.693808
step 1300 , training  accuracy 0.531276
step 1300 , loss : 0.693193
step 1400 , training  accuracy 0.509259
step 1400 , loss : 0.693412
step 1500 , training  accuracy 0.552263
step 1500 , loss : 0.6938
step 1600 , training  accuracy 0.51358
step 1600 , loss : 0.693841
step 1700 , training  accuracy 0.495062
step 1700 , loss : 0.69513
step 1800 , training  accuracy 0.521399
step 1800 , loss : 0.693835
step 1900 , training  accuracy 0.508025
step 1900 , loss : 0.693692
step 2000 , training  accuracy 0.511111
step 2000 , loss : 0.694
step 2100 , training  accuracy 0.50679
step 2100 , loss : 0.692905
step 2200 , training  accuracy 0.514198
step 2200 , loss : 0.691472
step 2300 , training  accuracy 0.514198
step 2300 , loss : 0.691691
step 2400 , training  accuracy 0.496296
step 2400 , loss : 0.692734
step 2500 , training  accuracy 0.527984
step 2500 , loss : 0.691665
step 2600 , training  accuracy 0.550412
step 2600 , loss : 0.692899
step 2700 , training  accuracy 0.551029
step 2700 , loss : 0.692438
step 2800 , training  accuracy 0.504321
step 2800 , loss : 0.689665
step 2900 , training  accuracy 0.550206
step 2900 , loss : 0.690791
step 3000 , training  accuracy 0.530864
step 3000 , loss : 0.69284
step 3100 , training  accuracy 0.511728
step 3100 , loss : 0.692896
step 3200 , training  accuracy 0.554527
step 3200 , loss : 0.692673
step 3300 , training  accuracy 0.510494
step 3300 , loss : 0.690036
step 3400 , training  accuracy 0.495062
step 3400 , loss : 0.695808
step 3500 , training  accuracy 0.514815
step 3500 , loss : 0.690513
step 3600 , training  accuracy 0.55823
step 3600 , loss : 0.692265
step 3700 , training  accuracy 0.50679
step 3700 , loss : 0.692309
step 3800 , training  accuracy 0.512963
step 3800 , loss : 0.690579
step 3900 , training  accuracy 0.533333
step 3900 , loss : 0.692905
step 4000 , training  accuracy 0.511111
step 4000 , loss : 0.691845
step 4100 , training  accuracy 0.528395
step 4100 , loss : 0.688654
step 4200 , training  accuracy 0.503704
step 4200 , loss : 0.689189
step 4300 , training  accuracy 0.505556
step 4300 , loss : 0.689303
step 4400 , training  accuracy 0.509259
step 4400 , loss : 0.68842
step 4500 , training  accuracy 0.546502
step 4500 , loss : 0.687286
step 4600 , training  accuracy 0.5
step 4600 , loss : 0.695129
step 4700 , training  accuracy 0.54856
step 4700 , loss : 0.692589
step 4800 , training  accuracy 0.569959
step 4800 , loss : 0.689167
step 4900 , training  accuracy 0.558436
step 4900 , loss : 0.690689
step 5000 , training  accuracy 0.550412
step 5000 , loss : 0.691124
step 5100 , training  accuracy 0.50679
step 5100 , loss : 0.690731
step 5200 , training  accuracy 0.553909
step 5200 , loss : 0.687398
step 5300 , training  accuracy 0.512963
step 5300 , loss : 0.691165
step 5400 , training  accuracy 0.566872
step 5400 , loss : 0.69028
step 5500 , training  accuracy 0.550823
step 5500 , loss : 0.688969
step 5600 , training  accuracy 0.564403
step 5600 , loss : 0.693123
step 5700 , training  accuracy 0.540535
step 5700 , loss : 0.692147
step 5800 , training  accuracy 0.517284
step 5800 , loss : 0.69191
step 5900 , training  accuracy 0.544856
step 5900 , loss : 0.691788
step 6000 , training  accuracy 0.563169
step 6000 , loss : 0.690719
step 6100 , training  accuracy 0.546091
step 6100 , loss : 0.692945
step 6200 , training  accuracy 0.560082
step 6200 , loss : 0.687765
step 6300 , training  accuracy 0.48642
step 6300 , loss : 0.695109
step 6400 , training  accuracy 0.488889
step 6400 , loss : 0.695223
step 6500 , training  accuracy 0.504321
step 6500 , loss : 0.692981
step 6600 , training  accuracy 0.538683
step 6600 , loss : 0.691438
step 6700 , training  accuracy 0.556379
step 6700 , loss : 0.687379
step 6800 , training  accuracy 0.540535
step 6800 , loss : 0.692677
step 6900 , training  accuracy 0.54177
step 6900 , loss : 0.691162
step 7000 , training  accuracy 0.535185
step 7000 , loss : 0.686215
step 7100 , training  accuracy 0.532099
step 7100 , loss : 0.692139
step 7200 , training  accuracy 0.563169
step 7200 , loss : 0.689648
step 7300 , training  accuracy 0.5393
step 7300 , loss : 0.692638
step 7400 , training  accuracy 0.550206
step 7400 , loss : 0.687037
step 7500 , training  accuracy 0.561934
step 7500 , loss : 0.68992
step 7600 , training  accuracy 0.521605
step 7600 , loss : 0.690109
step 7700 , training  accuracy 0.528395
step 7700 , loss : 0.691575
step 7800 , training  accuracy 0.571193
step 7800 , loss : 0.686144
step 7900 , training  accuracy 0.526543
step 7900 , loss : 0.691872
step 8000 , training  accuracy 0.561317
step 8000 , loss : 0.689652
step 8100 , training  accuracy 0.519753
step 8100 , loss : 0.688854
step 8200 , training  accuracy 0.57428
step 8200 , loss : 0.690936
step 8300 , training  accuracy 0.538889
step 8300 , loss : 0.687328
step 8400 , training  accuracy 0.561317
step 8400 , loss : 0.690953
step 8500 , training  accuracy 0.51358
step 8500 , loss : 0.690565
step 8600 , training  accuracy 0.560082
step 8600 , loss : 0.691192
step 8700 , training  accuracy 0.569342
step 8700 , loss : 0.690966
step 8800 , training  accuracy 0.570576
step 8800 , loss : 0.688117
step 8900 , training  accuracy 0.557613
step 8900 , loss : 0.685544
step 9000 , training  accuracy 0.553909
step 9000 , loss : 0.689945
step 9100 , training  accuracy 0.563786
step 9100 , loss : 0.690671
step 9200 , training  accuracy 0.573045
step 9200 , loss : 0.687789
step 9300 , training  accuracy 0.560082
step 9300 , loss : 0.690317
step 9400 , training  accuracy 0.542387
step 9400 , loss : 0.691205
step 9500 , training  accuracy 0.566255
step 9500 , loss : 0.687355
step 9600 , training  accuracy 0.563786
step 9600 , loss : 0.689315
step 9700 , training  accuracy 0.573663
step 9700 , loss : 0.687667
step 9800 , training  accuracy 0.566667
step 9800 , loss : 0.685994
step 9900 , training  accuracy 0.561317
step 9900 , loss : 0.686048
step 10000 , training  accuracy 0.564403
step 10000 , loss : 0.6874
step 10100 , training  accuracy 0.521605
step 10100 , loss : 0.692474
step 10200 , training  accuracy 0.572428
step 10200 , loss : 0.68903
step 10300 , training  accuracy 0.582099
step 10300 , loss : 0.685419
step 10400 , training  accuracy 0.563786
step 10400 , loss : 0.68668
step 10500 , training  accuracy 0.579835
step 10500 , loss : 0.686874
step 10600 , training  accuracy 0.569959
step 10600 , loss : 0.688575
step 10700 , training  accuracy 0.576132
step 10700 , loss : 0.682917
step 10800 , training  accuracy 0.509259
step 10800 , loss : 0.69258
step 10900 , training  accuracy 0.557202
step 10900 , loss : 0.688286
step 11000 , training  accuracy 0.587037
step 11000 , loss : 0.683298
step 11100 , training  accuracy 0.593827
step 11100 , loss : 0.682193
step 11200 , training  accuracy 0.518519
step 11200 , loss : 0.69197
step 11300 , training  accuracy 0.546708
step 11300 , loss : 0.690474
step 11400 , training  accuracy 0.590329
step 11400 , loss : 0.684398
step 11500 , training  accuracy 0.627161
step 11500 , loss : 0.676894
step 11600 , training  accuracy 0.535185
step 11600 , loss : 0.690832
step 11700 , training  accuracy 0.600412
step 11700 , loss : 0.682796
step 11800 , training  accuracy 0.576749
step 11800 , loss : 0.685128
step 11900 , training  accuracy 0.607613
step 11900 , loss : 0.6822
step 12000 , training  accuracy 0.598354
step 12000 , loss : 0.680232
step 12100 , training  accuracy 0.574486
step 12100 , loss : 0.684692
step 12200 , training  accuracy 0.592181
step 12200 , loss : 0.684105
step 12300 , training  accuracy 0.529012
step 12300 , loss : 0.69141
step 12400 , training  accuracy 0.618519
step 12400 , loss : 0.676695
step 12500 , training  accuracy 0.599177
step 12500 , loss : 0.680986
step 12600 , training  accuracy 0.629012
step 12600 , loss : 0.674552
step 12700 , training  accuracy 0.654321
step 12700 , loss : 0.675561
step 12800 , training  accuracy 0.584156
step 12800 , loss : 0.680993
step 12900 , training  accuracy 0.604527
step 12900 , loss : 0.6819
step 13000 , training  accuracy 0.614815
step 13000 , loss : 0.67878
step 13100 , training  accuracy 0.637037
step 13100 , loss : 0.675102
step 13200 , training  accuracy 0.632099
step 13200 , loss : 0.672457
step 13300 , training  accuracy 0.606173
step 13300 , loss : 0.678665
step 13400 , training  accuracy 0.646296
step 13400 , loss : 0.672366
step 13500 , training  accuracy 0.643827
step 13500 , loss : 0.673306
step 13600 , training  accuracy 0.643827
step 13600 , loss : 0.672425
step 13700 , training  accuracy 0.64321
step 13700 , loss : 0.674523
step 13800 , training  accuracy 0.594444
step 13800 , loss : 0.678885
step 13900 , training  accuracy 0.603704
step 13900 , loss : 0.679538
step 14000 , training  accuracy 0.637037
step 14000 , loss : 0.66962
step 14100 , training  accuracy 0.595268
step 14100 , loss : 0.678012
step 14200 , training  accuracy 0.62428
step 14200 , loss : 0.67673
step 14300 , training  accuracy 0.648765
step 14300 , loss : 0.674348
step 14400 , training  accuracy 0.646296
step 14400 , loss : 0.671279
step 14500 , training  accuracy 0.577984
step 14500 , loss : 0.682746
step 14600 , training  accuracy 0.620988
step 14600 , loss : 0.672399
step 14700 , training  accuracy 0.638889
step 14700 , loss : 0.667985
step 14800 , training  accuracy 0.643416
step 14800 , loss : 0.67622
step 14900 , training  accuracy 0.652469
step 14900 , loss : 0.671849
step 15000 , training  accuracy 0.634568
step 15000 , loss : 0.673229
step 15100 , training  accuracy 0.60144
step 15100 , loss : 0.677397
step 15200 , training  accuracy 0.632716
step 15200 , loss : 0.676055
step 15300 , training  accuracy 0.655556
step 15300 , loss : 0.668481
step 15400 , training  accuracy 0.635185
step 15400 , loss : 0.672755
step 15500 , training  accuracy 0.644444
step 15500 , loss : 0.668321
step 15600 , training  accuracy 0.591564
step 15600 , loss : 0.679781
step 15700 , training  accuracy 0.648148
step 15700 , loss : 0.674177
step 15800 , training  accuracy 0.654321
step 15800 , loss : 0.664398
step 15900 , training  accuracy 0.656173
step 15900 , loss : 0.667495
step 16000 , training  accuracy 0.652469
step 16000 , loss : 0.665124
step 16100 , training  accuracy 0.624074
step 16100 , loss : 0.674182
step 16200 , training  accuracy 0.644444
step 16200 , loss : 0.670868
step 16300 , training  accuracy 0.644444
step 16300 , loss : 0.667308
step 16400 , training  accuracy 0.667901
step 16400 , loss : 0.667681
step 16500 , training  accuracy 0.67037
step 16500 , loss : 0.665845
step 16600 , training  accuracy 0.657407
step 16600 , loss : 0.663063
step 16700 , training  accuracy 0.676543
step 16700 , loss : 0.668882
step 16800 , training  accuracy 0.654321
step 16800 , loss : 0.66815
step 16900 , training  accuracy 0.637654
step 16900 , loss : 0.66586
step 17000 , training  accuracy 0.661728
step 17000 , loss : 0.668056
step 17100 , training  accuracy 0.662963
step 17100 , loss : 0.666438
step 17200 , training  accuracy 0.667901
step 17200 , loss : 0.662599
step 17300 , training  accuracy 0.655556
step 17300 , loss : 0.66664
step 17400 , training  accuracy 0.677161
step 17400 , loss : 0.663982
step 17500 , training  accuracy 0.682099
step 17500 , loss : 0.663566
step 17600 , training  accuracy 0.680864
step 17600 , loss : 0.662407
step 17700 , training  accuracy 0.680864
step 17700 , loss : 0.663366
step 17800 , training  accuracy 0.685185
step 17800 , loss : 0.663519
step 17900 , training  accuracy 0.680864
step 17900 , loss : 0.66113
step 18000 , training  accuracy 0.67963
step 18000 , loss : 0.661434
step 18100 , training  accuracy 0.670988
step 18100 , loss : 0.663344
step 18200 , training  accuracy 0.676543
step 18200 , loss : 0.663928
step 18300 , training  accuracy 0.674691
step 18300 , loss : 0.660536
step 18400 , training  accuracy 0.696296
step 18400 , loss : 0.655179
step 18500 , training  accuracy 0.693827
step 18500 , loss : 0.660104
step 18600 , training  accuracy 0.687037
step 18600 , loss : 0.66068
step 18700 , training  accuracy 0.697531
step 18700 , loss : 0.658031
step 18800 , training  accuracy 0.684568
step 18800 , loss : 0.65504
step 18900 , training  accuracy 0.680247
step 18900 , loss : 0.656994
step 19000 , training  accuracy 0.680864
step 19000 , loss : 0.654164
step 19100 , training  accuracy 0.694444
step 19100 , loss : 0.658153
step 19200 , training  accuracy 0.684568
step 19200 , loss : 0.656019
step 19300 , training  accuracy 0.645062
step 19300 , loss : 0.662482
step 19400 , training  accuracy 0.662346
step 19400 , loss : 0.661582
step 19500 , training  accuracy 0.67037
step 19500 , loss : 0.656714
step 19600 , training  accuracy 0.685185
step 19600 , loss : 0.655235
step 19700 , training  accuracy 0.690741
step 19700 , loss : 0.660275
step 19800 , training  accuracy 0.664815
step 19800 , loss : 0.655245
step 19900 , training  accuracy 0.687654
step 19900 , loss : 0.660573
step 20000 , training  accuracy 0.677161
step 20000 , loss : 0.655984
step 20100 , training  accuracy 0.687654
step 20100 , loss : 0.65008
step 20200 , training  accuracy 0.680864
step 20200 , loss : 0.653739
step 20300 , training  accuracy 0.66358
step 20300 , loss : 0.657848
step 20400 , training  accuracy 0.701235
step 20400 , loss : 0.651528
step 20500 , training  accuracy 0.676543
step 20500 , loss : 0.655912
step 20600 , training  accuracy 0.660082
step 20600 , loss : 0.661516
step 20700 , training  accuracy 0.666667
step 20700 , loss : 0.654802
step 20800 , training  accuracy 0.694444
step 20800 , loss : 0.648852
step 20900 , training  accuracy 0.683951
step 20900 , loss : 0.657429
step 21000 , training  accuracy 0.680864
step 21000 , loss : 0.652804
step 21100 , training  accuracy 0.665432
step 21100 , loss : 0.656186
step 21200 , training  accuracy 0.692593
step 21200 , loss : 0.647812
step 21300 , training  accuracy 0.69321
step 21300 , loss : 0.64966
step 21400 , training  accuracy 0.674691
step 21400 , loss : 0.652922
step 21500 , training  accuracy 0.703086
step 21500 , loss : 0.646325
step 21600 , training  accuracy 0.701852
step 21600 , loss : 0.647951
step 21700 , training  accuracy 0.683951
step 21700 , loss : 0.651991
step 21800 , training  accuracy 0.687654
step 21800 , loss : 0.650949
step 21900 , training  accuracy 0.685802
step 21900 , loss : 0.652103
step 22000 , training  accuracy 0.701235
step 22000 , loss : 0.651726
step 22100 , training  accuracy 0.695062
step 22100 , loss : 0.647706
step 22200 , training  accuracy 0.696296
step 22200 , loss : 0.64977
step 22300 , training  accuracy 0.704938
step 22300 , loss : 0.651745
step 22400 , training  accuracy 0.693827
step 22400 , loss : 0.645464
step 22500 , training  accuracy 0.7
step 22500 , loss : 0.650004
step 22600 , training  accuracy 0.708025
step 22600 , loss : 0.647757
step 22700 , training  accuracy 0.701852
step 22700 , loss : 0.645234
step 22800 , training  accuracy 0.688272
step 22800 , loss : 0.648739
step 22900 , training  accuracy 0.69321
step 22900 , loss : 0.649482
step 23000 , training  accuracy 0.700617
step 23000 , loss : 0.648529
step 23100 , training  accuracy 0.709259
step 23100 , loss : 0.64252
step 23200 , training  accuracy 0.680864
step 23200 , loss : 0.654766
step 23300 , training  accuracy 0.688272
step 23300 , loss : 0.652341
step 23400 , training  accuracy 0.696914
step 23400 , loss : 0.647927
step 23500 , training  accuracy 0.708025
step 23500 , loss : 0.640436
step 23600 , training  accuracy 0.679218
step 23600 , loss : 0.652622
step 23700 , training  accuracy 0.708025
step 23700 , loss : 0.643642
step 23800 , training  accuracy 0.711728
step 23800 , loss : 0.640739
step 23900 , training  accuracy 0.702469
step 23900 , loss : 0.642435
step 24000 , training  accuracy 0.69321
step 24000 , loss : 0.642768
step 24100 , training  accuracy 0.687243
step 24100 , loss : 0.647777
step 24200 , training  accuracy 0.692593
step 24200 , loss : 0.643978
step 24300 , training  accuracy 0.700617
step 24300 , loss : 0.652109
step 24400 , training  accuracy 0.677984
step 24400 , loss : 0.650186
step 24500 , training  accuracy 0.708642
step 24500 , loss : 0.643773
step 24600 , training  accuracy 0.708025
step 24600 , loss : 0.645233
step 24700 , training  accuracy 0.698765
step 24700 , loss : 0.645368
step 24800 , training  accuracy 0.690123
step 24800 , loss : 0.646427
step 24900 , training  accuracy 0.704938
step 24900 , loss : 0.6467
step 25000 , training  accuracy 0.686008
step 25000 , loss : 0.644281
step 25100 , training  accuracy 0.693827
step 25100 , loss : 0.643856
step 25200 , training  accuracy 0.719753
step 25200 , loss : 0.636121
step 25300 , training  accuracy 0.682716
step 25300 , loss : 0.644696
step 25400 , training  accuracy 0.671811
step 25400 , loss : 0.65017
step 25500 , training  accuracy 0.688477
step 25500 , loss : 0.64167
step 25600 , training  accuracy 0.70823
step 25600 , loss : 0.640227
step 25700 , training  accuracy 0.723457
step 25700 , loss : 0.637521
step 25800 , training  accuracy 0.716049
step 25800 , loss : 0.632862
step 25900 , training  accuracy 0.702675
step 25900 , loss : 0.644281
step 26000 , training  accuracy 0.697119
step 26000 , loss : 0.645735
step 26100 , training  accuracy 0.719959
step 26100 , loss : 0.637705
step 26200 , training  accuracy 0.700823
step 26200 , loss : 0.636613
step 26300 , training  accuracy 0.709259
step 26300 , loss : 0.638902
step 26400 , training  accuracy 0.72037
step 26400 , loss : 0.637053
step 26500 , training  accuracy 0.728395
step 26500 , loss : 0.633866
step 26600 , training  accuracy 0.702058
step 26600 , loss : 0.639336
step 26700 , training  accuracy 0.712346
step 26700 , loss : 0.638362
step 26800 , training  accuracy 0.700823
step 26800 , loss : 0.642951
step 26900 , training  accuracy 0.70144
step 26900 , loss : 0.642748
step 27000 , training  accuracy 0.704527
step 27000 , loss : 0.639297
step 27100 , training  accuracy 0.710494
step 27100 , loss : 0.634178
step 27200 , training  accuracy 0.723457
step 27200 , loss : 0.635983
step 27300 , training  accuracy 0.710082
step 27300 , loss : 0.640044
step 27400 , training  accuracy 0.712346
step 27400 , loss : 0.632904
step 27500 , training  accuracy 0.708025
step 27500 , loss : 0.636845
step 27600 , training  accuracy 0.712963
step 27600 , loss : 0.637885
step 27700 , training  accuracy 0.692593
step 27700 , loss : 0.639103
step 27800 , training  accuracy 0.718519
step 27800 , loss : 0.631638
step 27900 , training  accuracy 0.733951
step 27900 , loss : 0.63035
step 28000 , training  accuracy 0.725926
step 28000 , loss : 0.631374
step 28100 , training  accuracy 0.714403
step 28100 , loss : 0.635392
step 28200 , training  accuracy 0.711934
step 28200 , loss : 0.634763
step 28300 , training  accuracy 0.721605
step 28300 , loss : 0.635846
step 28400 , training  accuracy 0.722222
step 28400 , loss : 0.632833
step 28500 , training  accuracy 0.703292
step 28500 , loss : 0.637277
step 28600 , training  accuracy 0.732099
step 28600 , loss : 0.626868
step 28700 , training  accuracy 0.689712
step 28700 , loss : 0.642629
step 28800 , training  accuracy 0.703704
step 28800 , loss : 0.634186
step 28900 , training  accuracy 0.702469
step 28900 , loss : 0.639543
step 29000 , training  accuracy 0.715021
step 29000 , loss : 0.634064
step 29100 , training  accuracy 0.725926
step 29100 , loss : 0.630491
step 29200 , training  accuracy 0.699588
step 29200 , loss : 0.641315
step 29300 , training  accuracy 0.716255
step 29300 , loss : 0.631517
step 29400 , training  accuracy 0.727161
step 29400 , loss : 0.631192
step 29500 , training  accuracy 0.726749
step 29500 , loss : 0.627304
step 29600 , training  accuracy 0.706996
step 29600 , loss : 0.633003
step 29700 , training  accuracy 0.709877
step 29700 , loss : 0.635296
step 29800 , training  accuracy 0.728395
step 29800 , loss : 0.62806
step 29900 , training  accuracy 0.716872
step 29900 , loss : 0.632623
step 30000 , training  accuracy 0.718519
step 30000 , loss : 0.631754
step 30100 , training  accuracy 0.738272
step 30100 , loss : 0.624779
step 30200 , training  accuracy 0.705144
step 30200 , loss : 0.63689
step 30300 , training  accuracy 0.709877
step 30300 , loss : 0.636933
step 30400 , training  accuracy 0.733951
step 30400 , loss : 0.626398
step 30500 , training  accuracy 0.707613
step 30500 , loss : 0.634141
step 30600 , training  accuracy 0.709465
step 30600 , loss : 0.634006
step 30700 , training  accuracy 0.717901
step 30700 , loss : 0.634601
step 30800 , training  accuracy 0.71358
step 30800 , loss : 0.629194
step 30900 , training  accuracy 0.71749
step 30900 , loss : 0.628102
step 31000 , training  accuracy 0.698971
step 31000 , loss : 0.637874
step 31100 , training  accuracy 0.737654
step 31100 , loss : 0.62449
step 31200 , training  accuracy 0.718107
step 31200 , loss : 0.626258
step 31300 , training  accuracy 0.690329
step 31300 , loss : 0.637519
step 31400 , training  accuracy 0.706996
step 31400 , loss : 0.637426
step 31500 , training  accuracy 0.715432
step 31500 , loss : 0.631906
step 31600 , training  accuracy 0.730864
step 31600 , loss : 0.624527
step 31700 , training  accuracy 0.729835
step 31700 , loss : 0.623025
step 31800 , training  accuracy 0.726749
step 31800 , loss : 0.626154
step 31900 , training  accuracy 0.716255
step 31900 , loss : 0.634189
step 32000 , training  accuracy 0.7107
step 32000 , loss : 0.629765
step 32100 , training  accuracy 0.726132
step 32100 , loss : 0.628368
step 32200 , training  accuracy 0.717901
step 32200 , loss : 0.634969
step 32300 , training  accuracy 0.732716
step 32300 , loss : 0.621618
step 32400 , training  accuracy 0.756173
step 32400 , loss : 0.621164
step 32500 , training  accuracy 0.738889
step 32500 , loss : 0.619712
step 32600 , training  accuracy 0.741358
step 32600 , loss : 0.619311
step 32700 , training  accuracy 0.735802
step 32700 , loss : 0.619613
step 32800 , training  accuracy 0.72037
step 32800 , loss : 0.62796
step 32900 , training  accuracy 0.751852
step 32900 , loss : 0.619216
step 33000 , training  accuracy 0.729218
step 33000 , loss : 0.626326
step 33100 , training  accuracy 0.750617
step 33100 , loss : 0.617017
step 33200 , training  accuracy 0.750617
step 33200 , loss : 0.617623
step 33300 , training  accuracy 0.719959
step 33300 , loss : 0.625405
step 33400 , training  accuracy 0.715021
step 33400 , loss : 0.629375
step 33500 , training  accuracy 0.745062
step 33500 , loss : 0.612507
step 33600 , training  accuracy 0.756996
step 33600 , loss : 0.612994
step 33700 , training  accuracy 0.735185
step 33700 , loss : 0.621197
step 33800 , training  accuracy 0.737037
step 33800 , loss : 0.621143
step 33900 , training  accuracy 0.734156
step 33900 , loss : 0.624964
step 34000 , training  accuracy 0.749383
step 34000 , loss : 0.619756
step 34100 , training  accuracy 0.745062
step 34100 , loss : 0.623194
step 34200 , training  accuracy 0.770988
step 34200 , loss : 0.61538
step 34300 , training  accuracy 0.738889
step 34300 , loss : 0.624356
step 34400 , training  accuracy 0.733539
step 34400 , loss : 0.622288
step 34500 , training  accuracy 0.755761
step 34500 , loss : 0.619208
step 34600 , training  accuracy 0.733951
step 34600 , loss : 0.622242
step 34700 , training  accuracy 0.758642
step 34700 , loss : 0.614596
step 34800 , training  accuracy 0.75
step 34800 , loss : 0.622686
step 34900 , training  accuracy 0.750206
step 34900 , loss : 0.614015
step 35000 , training  accuracy 0.75679
step 35000 , loss : 0.613353
step 35100 , training  accuracy 0.766667
step 35100 , loss : 0.605707
step 35200 , training  accuracy 0.762346
step 35200 , loss : 0.612733
step 35300 , training  accuracy 0.745062
step 35300 , loss : 0.614454
step 35400 , training  accuracy 0.747119
step 35400 , loss : 0.61745
step 35500 , training  accuracy 0.748148
step 35500 , loss : 0.618796
step 35600 , training  accuracy 0.758642
step 35600 , loss : 0.610346
step 35700 , training  accuracy 0.75
step 35700 , loss : 0.61371
step 35800 , training  accuracy 0.75
step 35800 , loss : 0.613835
step 35900 , training  accuracy 0.744033
step 35900 , loss : 0.617265
step 36000 , training  accuracy 0.727984
step 36000 , loss : 0.620409
step 36100 , training  accuracy 0.746914
step 36100 , loss : 0.616568
step 36200 , training  accuracy 0.748765
step 36200 , loss : 0.613008
step 36300 , training  accuracy 0.751852
step 36300 , loss : 0.615122
step 36400 , training  accuracy 0.74321
step 36400 , loss : 0.618708
step 36500 , training  accuracy 0.73642
step 36500 , loss : 0.618788
step 36600 , training  accuracy 0.767901
step 36600 , loss : 0.610254
step 36700 , training  accuracy 0.759259
step 36700 , loss : 0.610391
step 36800 , training  accuracy 0.758025
step 36800 , loss : 0.612645
step 36900 , training  accuracy 0.759259
step 36900 , loss : 0.61077
step 37000 , training  accuracy 0.751235
step 37000 , loss : 0.615389
step 37100 , training  accuracy 0.756379
step 37100 , loss : 0.614951
step 37200 , training  accuracy 0.746502
step 37200 , loss : 0.619854
step 37300 , training  accuracy 0.761728
step 37300 , loss : 0.613964
step 37400 , training  accuracy 0.754321
step 37400 , loss : 0.615941
step 37500 , training  accuracy 0.743416
step 37500 , loss : 0.616808
step 37600 , training  accuracy 0.739506
step 37600 , loss : 0.621312
step 37700 , training  accuracy 0.769753
step 37700 , loss : 0.608706
step 37800 , training  accuracy 0.759877
step 37800 , loss : 0.612236
step 37900 , training  accuracy 0.766667
step 37900 , loss : 0.608202
step 38000 , training  accuracy 0.764403
step 38000 , loss : 0.608882
step 38100 , training  accuracy 0.761728
step 38100 , loss : 0.612517
step 38200 , training  accuracy 0.764198
step 38200 , loss : 0.614253
step 38300 , training  accuracy 0.775926
step 38300 , loss : 0.611108
step 38400 , training  accuracy 0.76358
step 38400 , loss : 0.610963
step 38500 , training  accuracy 0.769342
step 38500 , loss : 0.606058
step 38600 , training  accuracy 0.757613
step 38600 , loss : 0.614355
step 38700 , training  accuracy 0.761317
step 38700 , loss : 0.612499
step 38800 , training  accuracy 0.759465
step 38800 , loss : 0.61224
step 38900 , training  accuracy 0.758848
step 38900 , loss : 0.609332
step 39000 , training  accuracy 0.762963
step 39000 , loss : 0.611054
step 39100 , training  accuracy 0.762346
step 39100 , loss : 0.611312
step 39200 , training  accuracy 0.776543
step 39200 , loss : 0.606492
step 39300 , training  accuracy 0.755144
step 39300 , loss : 0.612411
step 39400 , training  accuracy 0.726749
step 39400 , loss : 0.623136
step 39500 , training  accuracy 0.778395
step 39500 , loss : 0.605567
step 39600 , training  accuracy 0.762346
step 39600 , loss : 0.608055
step 39700 , training  accuracy 0.75679
step 39700 , loss : 0.612962
step 39800 , training  accuracy 0.757407
step 39800 , loss : 0.612287
step 39900 , training  accuracy 0.753704
step 39900 , loss : 0.60953
step 40000 , training  accuracy 0.751852
step 40000 , loss : 0.608116
step 40100 , training  accuracy 0.758025
step 40100 , loss : 0.605971
step 40200 , training  accuracy 0.745885
step 40200 , loss : 0.616742
step 40300 , training  accuracy 0.7607
step 40300 , loss : 0.607718
step 40400 , training  accuracy 0.745268
step 40400 , loss : 0.618345
step 40500 , training  accuracy 0.732716
step 40500 , loss : 0.619815
step 40600 , training  accuracy 0.762346
step 40600 , loss : 0.605116
step 40700 , training  accuracy 0.77284
step 40700 , loss : 0.607084
step 40800 , training  accuracy 0.759877
step 40800 , loss : 0.610701
step 40900 , training  accuracy 0.772222
step 40900 , loss : 0.605249
step 41000 , training  accuracy 0.772222
step 41000 , loss : 0.604717
step 41100 , training  accuracy 0.750206
step 41100 , loss : 0.610722
step 41200 , training  accuracy 0.761111
step 41200 , loss : 0.611257
step 41300 , training  accuracy 0.775309
step 41300 , loss : 0.606423
step 41400 , training  accuracy 0.782099
step 41400 , loss : 0.600452
step 41500 , training  accuracy 0.758025
step 41500 , loss : 0.608645
step 41600 , training  accuracy 0.767284
step 41600 , loss : 0.608221
step 41700 , training  accuracy 0.753086
step 41700 , loss : 0.611501
step 41800 , training  accuracy 0.771605
step 41800 , loss : 0.607321
step 41900 , training  accuracy 0.752675
step 41900 , loss : 0.608901
step 42000 , training  accuracy 0.763786
step 42000 , loss : 0.607801
step 42100 , training  accuracy 0.774074
step 42100 , loss : 0.600912
step 42200 , training  accuracy 0.754938
step 42200 , loss : 0.608959
step 42300 , training  accuracy 0.764198
step 42300 , loss : 0.603379
step 42400 , training  accuracy 0.783951
step 42400 , loss : 0.605152
step 42500 , training  accuracy 0.77963
step 42500 , loss : 0.602212
step 42600 , training  accuracy 0.779012
step 42600 , loss : 0.600004
step 42700 , training  accuracy 0.764815
step 42700 , loss : 0.607344
step 42800 , training  accuracy 0.771193
step 42800 , loss : 0.602001
step 42900 , training  accuracy 0.776543
step 42900 , loss : 0.601598
step 43000 , training  accuracy 0.77284
step 43000 , loss : 0.602953
step 43100 , training  accuracy 0.777161
step 43100 , loss : 0.60862
step 43200 , training  accuracy 0.759877
step 43200 , loss : 0.603229
step 43300 , training  accuracy 0.787037
step 43300 , loss : 0.602563
step 43400 , training  accuracy 0.767284
step 43400 , loss : 0.604347
step 43500 , training  accuracy 0.764198
step 43500 , loss : 0.608027
step 43600 , training  accuracy 0.767284
step 43600 , loss : 0.602182
step 43700 , training  accuracy 0.767901
step 43700 , loss : 0.602792
step 43800 , training  accuracy 0.766667
step 43800 , loss : 0.601177
step 43900 , training  accuracy 0.767901
step 43900 , loss : 0.601994
step 44000 , training  accuracy 0.768519
step 44000 , loss : 0.605846
step 44100 , training  accuracy 0.767901
step 44100 , loss : 0.604978
step 44200 , training  accuracy 0.755556
step 44200 , loss : 0.607143
step 44300 , training  accuracy 0.76358
step 44300 , loss : 0.60304
step 44400 , training  accuracy 0.771605
step 44400 , loss : 0.60129
step 44500 , training  accuracy 0.758025
step 44500 , loss : 0.609999
step 44600 , training  accuracy 0.77963
step 44600 , loss : 0.601693
step 44700 , training  accuracy 0.779012
step 44700 , loss : 0.600864
step 44800 , training  accuracy 0.769136
step 44800 , loss : 0.604969
step 44900 , training  accuracy 0.761111
step 44900 , loss : 0.608644
step 45000 , training  accuracy 0.764198
step 45000 , loss : 0.609319
step 45100 , training  accuracy 0.766049
step 45100 , loss : 0.606107
step 45200 , training  accuracy 0.770988
step 45200 , loss : 0.602849
step 45300 , training  accuracy 0.770988
step 45300 , loss : 0.597315
step 45400 , training  accuracy 0.762963
step 45400 , loss : 0.60417
step 45500 , training  accuracy 0.759877
step 45500 , loss : 0.610793
step 45600 , training  accuracy 0.778395
step 45600 , loss : 0.601742
step 45700 , training  accuracy 0.771605
step 45700 , loss : 0.606398
step 45800 , training  accuracy 0.782716
step 45800 , loss : 0.602644
step 45900 , training  accuracy 0.766667
step 45900 , loss : 0.6061
step 46000 , training  accuracy 0.767284
step 46000 , loss : 0.60448
step 46100 , training  accuracy 0.770988
step 46100 , loss : 0.601211
step 46200 , training  accuracy 0.77963
step 46200 , loss : 0.600264
step 46300 , training  accuracy 0.77284
step 46300 , loss : 0.608135
step 46400 , training  accuracy 0.747737
step 46400 , loss : 0.612092
step 46500 , training  accuracy 0.774074
step 46500 , loss : 0.597989
step 46600 , training  accuracy 0.752469
step 46600 , loss : 0.610238
step 46700 , training  accuracy 0.787654
step 46700 , loss : 0.593495
step 46800 , training  accuracy 0.766667
step 46800 , loss : 0.600672
step 46900 , training  accuracy 0.776543
step 46900 , loss : 0.597784
step 47000 , training  accuracy 0.765432
step 47000 , loss : 0.609335
step 47100 , training  accuracy 0.762346
step 47100 , loss : 0.607849
step 47200 , training  accuracy 0.766049
step 47200 , loss : 0.603491
step 47300 , training  accuracy 0.770988
step 47300 , loss : 0.605188
step 47400 , training  accuracy 0.782716
step 47400 , loss : 0.599431
step 47500 , training  accuracy 0.77963
step 47500 , loss : 0.601822
step 47600 , training  accuracy 0.780864
step 47600 , loss : 0.592422
step 47700 , training  accuracy 0.77284
step 47700 , loss : 0.602622
step 47800 , training  accuracy 0.760494
step 47800 , loss : 0.607572
step 47900 , training  accuracy 0.772222
step 47900 , loss : 0.602047
step 48000 , training  accuracy 0.778395
step 48000 , loss : 0.602575
step 48100 , training  accuracy 0.775926
step 48100 , loss : 0.598942
step 48200 , training  accuracy 0.777778
step 48200 , loss : 0.604169
step 48300 , training  accuracy 0.782099
step 48300 , loss : 0.599378
step 48400 , training  accuracy 0.789712
step 48400 , loss : 0.598484
step 48500 , training  accuracy 0.781481
step 48500 , loss : 0.602042
step 48600 , training  accuracy 0.77963
step 48600 , loss : 0.601517
step 48700 , training  accuracy 0.780864
step 48700 , loss : 0.599989
step 48800 , training  accuracy 0.776132
step 48800 , loss : 0.604363
step 48900 , training  accuracy 0.783951
step 48900 , loss : 0.596196
step 49000 , training  accuracy 0.764403
step 49000 , loss : 0.600191
step 49100 , training  accuracy 0.767284
step 49100 , loss : 0.605053
step 49200 , training  accuracy 0.781481
step 49200 , loss : 0.597826
step 49300 , training  accuracy 0.774691
step 49300 , loss : 0.602764
step 49400 , training  accuracy 0.766872
step 49400 , loss : 0.600635
step 49500 , training  accuracy 0.761111
step 49500 , loss : 0.606827
step 49600 , training  accuracy 0.758025
step 49600 , loss : 0.60319
step 49700 , training  accuracy 0.776543
step 49700 , loss : 0.599161
step 49800 , training  accuracy 0.781481
step 49800 , loss : 0.596369
step 49900 , training  accuracy 0.789506
step 49900 , loss : 0.596112
step 50000 , training  accuracy 0.762963
step 50000 , loss : 0.603313
step 50100 , training  accuracy 0.788272
step 50100 , loss : 0.596662
step 50200 , training  accuracy 0.781481
step 50200 , loss : 0.597293
step 50300 , training  accuracy 0.767901
step 50300 , loss : 0.602373
step 50400 , training  accuracy 0.75823
step 50400 , loss : 0.610535
step 50500 , training  accuracy 0.77963
step 50500 , loss : 0.601076
step 50600 , training  accuracy 0.784568
step 50600 , loss : 0.597351
step 50700 , training  accuracy 0.785802
step 50700 , loss : 0.597064
step 50800 , training  accuracy 0.784568
step 50800 , loss : 0.595467
step 50900 , training  accuracy 0.781481
step 50900 , loss : 0.598382
step 51000 , training  accuracy 0.787654
step 51000 , loss : 0.595834
step 51100 , training  accuracy 0.783333
step 51100 , loss : 0.59618
step 51200 , training  accuracy 0.782099
step 51200 , loss : 0.604211
step 51300 , training  accuracy 0.788272
step 51300 , loss : 0.595839
step 51400 , training  accuracy 0.77284
step 51400 , loss : 0.60273
step 51500 , training  accuracy 0.781481
step 51500 , loss : 0.597173
step 51600 , training  accuracy 0.769136
step 51600 , loss : 0.604225
step 51700 , training  accuracy 0.779012
step 51700 , loss : 0.601353
step 51800 , training  accuracy 0.768107
step 51800 , loss : 0.606613
step 51900 , training  accuracy 0.774074
step 51900 , loss : 0.602779
step 52000 , training  accuracy 0.783333
step 52000 , loss : 0.596768
step 52100 , training  accuracy 0.8
step 52100 , loss : 0.594951
step 52200 , training  accuracy 0.784568
step 52200 , loss : 0.598785
step 52300 , training  accuracy 0.783333
step 52300 , loss : 0.597809
step 52400 , training  accuracy 0.776543
step 52400 , loss : 0.600467
step 52500 , training  accuracy 0.785802
step 52500 , loss : 0.59504
step 52600 , training  accuracy 0.762346
step 52600 , loss : 0.604889
step 52700 , training  accuracy 0.780864
step 52700 , loss : 0.595386
step 52800 , training  accuracy 0.776543
step 52800 , loss : 0.598025
step 52900 , training  accuracy 0.77963
step 52900 , loss : 0.602004
step 53000 , training  accuracy 0.779012
step 53000 , loss : 0.603153
step 53100 , training  accuracy 0.765432
step 53100 , loss : 0.603491
step 53200 , training  accuracy 0.773457
step 53200 , loss : 0.600131
step 53300 , training  accuracy 0.772222
step 53300 , loss : 0.600547
step 53400 , training  accuracy 0.761728
step 53400 , loss : 0.608925
step 53500 , training  accuracy 0.772222
step 53500 , loss : 0.600397
step 53600 , training  accuracy 0.762346
step 53600 , loss : 0.604102
step 53700 , training  accuracy 0.777778
step 53700 , loss : 0.60063
step 53800 , training  accuracy 0.79321
step 53800 , loss : 0.596979
step 53900 , training  accuracy 0.77037
step 53900 , loss : 0.60272
step 54000 , training  accuracy 0.783333
step 54000 , loss : 0.597594
step 54100 , training  accuracy 0.774691
step 54100 , loss : 0.603608
step 54200 , training  accuracy 0.780247
step 54200 , loss : 0.594837
step 54300 , training  accuracy 0.773457
step 54300 , loss : 0.598837
step 54400 , training  accuracy 0.778395
step 54400 , loss : 0.600512
step 54500 , training  accuracy 0.774074
step 54500 , loss : 0.603301
step 54600 , training  accuracy 0.783333
step 54600 , loss : 0.596988
step 54700 , training  accuracy 0.766667
step 54700 , loss : 0.603859
step 54800 , training  accuracy 0.793827
step 54800 , loss : 0.598116
step 54900 , training  accuracy 0.791358
step 54900 , loss : 0.598132
step 55000 , training  accuracy 0.770988
step 55000 , loss : 0.604674
step 55100 , training  accuracy 0.766049
step 55100 , loss : 0.600474
step 55200 , training  accuracy 0.782099
step 55200 , loss : 0.597665
step 55300 , training  accuracy 0.79321
step 55300 , loss : 0.597586
step 55400 , training  accuracy 0.787654
step 55400 , loss : 0.596928
step 55500 , training  accuracy 0.785185
step 55500 , loss : 0.596156
step 55600 , training  accuracy 0.789506
step 55600 , loss : 0.596194
step 55700 , training  accuracy 0.784568
step 55700 , loss : 0.596932
step 55800 , training  accuracy 0.775926
step 55800 , loss : 0.601705
step 55900 , training  accuracy 0.774691
step 55900 , loss : 0.601078
step 56000 , training  accuracy 0.798148
step 56000 , loss : 0.594683
step 56100 , training  accuracy 0.778395
step 56100 , loss : 0.599959
step 56200 , training  accuracy 0.779012
step 56200 , loss : 0.599657
step 56300 , training  accuracy 0.787037
step 56300 , loss : 0.59548
step 56400 , training  accuracy 0.791975
step 56400 , loss : 0.594525
step 56500 , training  accuracy 0.767284
step 56500 , loss : 0.602715
step 56600 , training  accuracy 0.77963
step 56600 , loss : 0.593693
step 56700 , training  accuracy 0.791975
step 56700 , loss : 0.597542
step 56800 , training  accuracy 0.787037
step 56800 , loss : 0.594766
step 56900 , training  accuracy 0.762963
step 56900 , loss : 0.607814
step 57000 , training  accuracy 0.766667
step 57000 , loss : 0.604136
step 57100 , training  accuracy 0.792593
step 57100 , loss : 0.596116
step 57200 , training  accuracy 0.780864
step 57200 , loss : 0.599096
step 57300 , training  accuracy 0.785802
step 57300 , loss : 0.594962
step 57400 , training  accuracy 0.777161
step 57400 , loss : 0.59746
step 57500 , training  accuracy 0.78642
step 57500 , loss : 0.592015
step 57600 , training  accuracy 0.780247
step 57600 , loss : 0.599418
step 57700 , training  accuracy 0.783333
step 57700 , loss : 0.596449
step 57800 , training  accuracy 0.783951
step 57800 , loss : 0.598534
step 57900 , training  accuracy 0.774691
step 57900 , loss : 0.599261
step 58000 , training  accuracy 0.782099
step 58000 , loss : 0.599151
step 58100 , training  accuracy 0.784568
step 58100 , loss : 0.599181
step 58200 , training  accuracy 0.783951
step 58200 , loss : 0.597386
step 58300 , training  accuracy 0.788889
step 58300 , loss : 0.59313
step 58400 , training  accuracy 0.784568
step 58400 , loss : 0.602743
step 58500 , training  accuracy 0.781481
step 58500 , loss : 0.596912
step 58600 , training  accuracy 0.770988
step 58600 , loss : 0.59782
step 58700 , training  accuracy 0.783333
step 58700 , loss : 0.595593
step 58800 , training  accuracy 0.787654
step 58800 , loss : 0.596438
step 58900 , training  accuracy 0.781481
step 58900 , loss : 0.598091
step 59000 , training  accuracy 0.795679
step 59000 , loss : 0.593827
step 59100 , training  accuracy 0.785802
step 59100 , loss : 0.594133
step 59200 , training  accuracy 0.78642
step 59200 , loss : 0.601185
step 59300 , training  accuracy 0.781481
step 59300 , loss : 0.598758
step 59400 , training  accuracy 0.785802
step 59400 , loss : 0.594286
step 59500 , training  accuracy 0.791358
step 59500 , loss : 0.596074
step 59600 , training  accuracy 0.781481
step 59600 , loss : 0.595678
step 59700 , training  accuracy 0.775926
step 59700 , loss : 0.603531
step 59800 , training  accuracy 0.783951
step 59800 , loss : 0.596862
step 59900 , training  accuracy 0.789095
step 59900 , loss : 0.597731
step 60000 , training  accuracy 0.785185
step 60000 , loss : 0.592741
step 60100 , training  accuracy 0.788272
step 60100 , loss : 0.595456
step 60200 , training  accuracy 0.773457
step 60200 , loss : 0.598529
step 60300 , training  accuracy 0.777778
step 60300 , loss : 0.598214
step 60400 , training  accuracy 0.772222
step 60400 , loss : 0.603651
step 60500 , training  accuracy 0.780864
step 60500 , loss : 0.598641
step 60600 , training  accuracy 0.759465
step 60600 , loss : 0.602042
step 60700 , training  accuracy 0.764815
step 60700 , loss : 0.60436
step 60800 , training  accuracy 0.754527
step 60800 , loss : 0.607512
step 60900 , training  accuracy 0.76358
step 60900 , loss : 0.601047
step 61000 , training  accuracy 0.767901
step 61000 , loss : 0.599883
step 61100 , training  accuracy 0.765432
step 61100 , loss : 0.606833
step 61200 , training  accuracy 0.77037
step 61200 , loss : 0.603277
step 61300 , training  accuracy 0.777778
step 61300 , loss : 0.597378
step 61400 , training  accuracy 0.795679
step 61400 , loss : 0.595376
step 61500 , training  accuracy 0.782716
step 61500 , loss : 0.594206
step 61600 , training  accuracy 0.776543
step 61600 , loss : 0.599061
step 61700 , training  accuracy 0.775309
step 61700 , loss : 0.598462
step 61800 , training  accuracy 0.769753
step 61800 , loss : 0.598615
step 61900 , training  accuracy 0.775309
step 61900 , loss : 0.599747
step 62000 , training  accuracy 0.780864
step 62000 , loss : 0.597772
step 62100 , training  accuracy 0.779012
step 62100 , loss : 0.597911
step 62200 , training  accuracy 0.769342
step 62200 , loss : 0.603219
step 62300 , training  accuracy 0.78642
step 62300 , loss : 0.591837
step 62400 , training  accuracy 0.784568
step 62400 , loss : 0.597871
step 62500 , training  accuracy 0.778395
step 62500 , loss : 0.599612
step 62600 , training  accuracy 0.777161
step 62600 , loss : 0.595151
step 62700 , training  accuracy 0.792593
step 62700 , loss : 0.593672
step 62800 , training  accuracy 0.773663
step 62800 , loss : 0.598308
step 62900 , training  accuracy 0.781481
step 62900 , loss : 0.596008
step 63000 , training  accuracy 0.765021
step 63000 , loss : 0.605024
step 63100 , training  accuracy 0.779012
step 63100 , loss : 0.597137
step 63200 , training  accuracy 0.780247
step 63200 , loss : 0.597327
step 63300 , training  accuracy 0.775309
step 63300 , loss : 0.599265
step 63400 , training  accuracy 0.768519
step 63400 , loss : 0.602801
step 63500 , training  accuracy 0.780864
step 63500 , loss : 0.59803
step 63600 , training  accuracy 0.782099
step 63600 , loss : 0.597692
step 63700 , training  accuracy 0.777161
step 63700 , loss : 0.598208
step 63800 , training  accuracy 0.787037
step 63800 , loss : 0.592446
step 63900 , training  accuracy 0.76749
step 63900 , loss : 0.600427
step 64000 , training  accuracy 0.77963
step 64000 , loss : 0.598945
step 64100 , training  accuracy 0.783333
step 64100 , loss : 0.596452
step 64200 , training  accuracy 0.771605
step 64200 , loss : 0.599007
step 64300 , training  accuracy 0.770988
step 64300 , loss : 0.602126
step 64400 , training  accuracy 0.769753
step 64400 , loss : 0.598745
step 64500 , training  accuracy 0.777161
step 64500 , loss : 0.601622
step 64600 , training  accuracy 0.777778
step 64600 , loss : 0.596733
step 64700 , training  accuracy 0.77284
step 64700 , loss : 0.598786
step 64800 , training  accuracy 0.778395
step 64800 , loss : 0.598395
step 64900 , training  accuracy 0.783951
step 64900 , loss : 0.596125
step 65000 , training  accuracy 0.785802
step 65000 , loss : 0.596732
step 65100 , training  accuracy 0.781481
step 65100 , loss : 0.596551
step 65200 , training  accuracy 0.773663
step 65200 , loss : 0.598839
step 65300 , training  accuracy 0.789506
step 65300 , loss : 0.594466
step 65400 , training  accuracy 0.775926
step 65400 , loss : 0.598154
step 65500 , training  accuracy 0.780247
step 65500 , loss : 0.598623
step 65600 , training  accuracy 0.782716
step 65600 , loss : 0.594049
step 65700 , training  accuracy 0.778395
step 65700 , loss : 0.599151
step 65800 , training  accuracy 0.788272
step 65800 , loss : 0.595277
step 65900 , training  accuracy 0.795062
step 65900 , loss : 0.593673
step 66000 , training  accuracy 0.791975
step 66000 , loss : 0.590133
step 66100 , training  accuracy 0.780453
step 66100 , loss : 0.598018
step 66200 , training  accuracy 0.76749
step 66200 , loss : 0.602365
step 66300 , training  accuracy 0.77428
step 66300 , loss : 0.596834
step 66400 , training  accuracy 0.788889
step 66400 , loss : 0.594643
step 66500 , training  accuracy 0.794444
step 66500 , loss : 0.594413
step 66600 , training  accuracy 0.780453
step 66600 , loss : 0.600952
step 66700 , training  accuracy 0.796914
step 66700 , loss : 0.59228
step 66800 , training  accuracy 0.79321
step 66800 , loss : 0.59662
step 66900 , training  accuracy 0.785185
step 66900 , loss : 0.595311
step 67000 , training  accuracy 0.777778
step 67000 , loss : 0.601998
step 67100 , training  accuracy 0.779218
step 67100 , loss : 0.601211
step 67200 , training  accuracy 0.776132
step 67200 , loss : 0.601059
step 67300 , training  accuracy 0.790741
step 67300 , loss : 0.598501
step 67400 , training  accuracy 0.790123
step 67400 , loss : 0.595474
step 67500 , training  accuracy 0.787037
step 67500 , loss : 0.598689
step 67600 , training  accuracy 0.787654
step 67600 , loss : 0.596484
step 67700 , training  accuracy 0.796296
step 67700 , loss : 0.59178
step 67800 , training  accuracy 0.77963
step 67800 , loss : 0.599435
step 67900 , training  accuracy 0.782716
step 67900 , loss : 0.596482
step 68000 , training  accuracy 0.781481
step 68000 , loss : 0.597233
step 68100 , training  accuracy 0.779012
step 68100 , loss : 0.599334
step 68200 , training  accuracy 0.788889
step 68200 , loss : 0.593157
step 68300 , training  accuracy 0.78642
step 68300 , loss : 0.594348
step 68400 , training  accuracy 0.785802
step 68400 , loss : 0.597101
step 68500 , training  accuracy 0.790741
step 68500 , loss : 0.595391
step 68600 , training  accuracy 0.8
step 68600 , loss : 0.589848
step 68700 , training  accuracy 0.789506
step 68700 , loss : 0.592547
step 68800 , training  accuracy 0.787654
step 68800 , loss : 0.597856
step 68900 , training  accuracy 0.793827
step 68900 , loss : 0.592646
step 69000 , training  accuracy 0.795679
step 69000 , loss : 0.595841
step 69100 , training  accuracy 0.781481
step 69100 , loss : 0.596702
step 69200 , training  accuracy 0.774691
step 69200 , loss : 0.59906
step 69300 , training  accuracy 0.785802
step 69300 , loss : 0.595615
step 69400 , training  accuracy 0.792593
step 69400 , loss : 0.593948
step 69500 , training  accuracy 0.77284
step 69500 , loss : 0.598724
step 69600 , training  accuracy 0.782716
step 69600 , loss : 0.600869
step 69700 , training  accuracy 0.778395
step 69700 , loss : 0.599254
step 69800 , training  accuracy 0.781481
step 69800 , loss : 0.59219
step 69900 , training  accuracy 0.785185
step 69900 , loss : 0.597848
step 70000 , training  accuracy 0.785185
step 70000 , loss : 0.597965
step 70100 , training  accuracy 0.796296
step 70100 , loss : 0.592787
step 70200 , training  accuracy 0.788272
step 70200 , loss : 0.595226
step 70300 , training  accuracy 0.785185
step 70300 , loss : 0.596237
step 70400 , training  accuracy 0.788272
step 70400 , loss : 0.595542
step 70500 , training  accuracy 0.801235
step 70500 , loss : 0.589925
step 70600 , training  accuracy 0.769753
step 70600 , loss : 0.60547
step 70700 , training  accuracy 0.79321
step 70700 , loss : 0.59308
step 70800 , training  accuracy 0.801235
step 70800 , loss : 0.590445
step 70900 , training  accuracy 0.778395
step 70900 , loss : 0.602723
step 71000 , training  accuracy 0.772222
step 71000 , loss : 0.599874
step 71100 , training  accuracy 0.789506
step 71100 , loss : 0.599253
step 71200 , training  accuracy 0.793827
step 71200 , loss : 0.593406
step 71300 , training  accuracy 0.777366
step 71300 , loss : 0.597151
step 71400 , training  accuracy 0.787654
step 71400 , loss : 0.5964
step 71500 , training  accuracy 0.76749
step 71500 , loss : 0.60106
step 71600 , training  accuracy 0.771193
step 71600 , loss : 0.604993
step 71700 , training  accuracy 0.775926
step 71700 , loss : 0.607277
step 71800 , training  accuracy 0.782716
step 71800 , loss : 0.596977
step 71900 , training  accuracy 0.771605
step 71900 , loss : 0.599609
step 72000 , training  accuracy 0.788272
step 72000 , loss : 0.594555
step 72100 , training  accuracy 0.788889
step 72100 , loss : 0.59327
step 72200 , training  accuracy 0.801235
step 72200 , loss : 0.592437
step 72300 , training  accuracy 0.769342
step 72300 , loss : 0.603612
step 72400 , training  accuracy 0.790123
step 72400 , loss : 0.601225
step 72500 , training  accuracy 0.785185
step 72500 , loss : 0.598998
step 72600 , training  accuracy 0.789506
step 72600 , loss : 0.597395
step 72700 , training  accuracy 0.779012
step 72700 , loss : 0.601655
step 72800 , training  accuracy 0.794444
step 72800 , loss : 0.592755
step 72900 , training  accuracy 0.776543
step 72900 , loss : 0.596319
step 73000 , training  accuracy 0.783333
step 73000 , loss : 0.599845
step 73100 , training  accuracy 0.77428
step 73100 , loss : 0.600175
step 73200 , training  accuracy 0.784568
step 73200 , loss : 0.589819
step 73300 , training  accuracy 0.780247
step 73300 , loss : 0.599268
step 73400 , training  accuracy 0.79321
step 73400 , loss : 0.591196
step 73500 , training  accuracy 0.8
step 73500 , loss : 0.589814
step 73600 , training  accuracy 0.792593
step 73600 , loss : 0.595582
step 73700 , training  accuracy 0.791975
step 73700 , loss : 0.596625
step 73800 , training  accuracy 0.795062
step 73800 , loss : 0.593858
step 73900 , training  accuracy 0.776543
step 73900 , loss : 0.601491
step 74000 , training  accuracy 0.792593
step 74000 , loss : 0.595558
step 74100 , training  accuracy 0.795062
step 74100 , loss : 0.593607
step 74200 , training  accuracy 0.794444
step 74200 , loss : 0.59627
step 74300 , training  accuracy 0.795679
step 74300 , loss : 0.592182
step 74400 , training  accuracy 0.790741
step 74400 , loss : 0.59503
step 74500 , training  accuracy 0.795679
step 74500 , loss : 0.594307
step 74600 , training  accuracy 0.790741
step 74600 , loss : 0.591876
step 74700 , training  accuracy 0.806173
step 74700 , loss : 0.588143
step 74800 , training  accuracy 0.789506
step 74800 , loss : 0.594706
step 74900 , training  accuracy 0.787654
step 74900 , loss : 0.59444
step 75000 , training  accuracy 0.803086
step 75000 , loss : 0.591541
step 75100 , training  accuracy 0.789506
step 75100 , loss : 0.591832
step 75200 , training  accuracy 0.787654
step 75200 , loss : 0.595877
step 75300 , training  accuracy 0.787654
step 75300 , loss : 0.59727
step 75400 , training  accuracy 0.783333
step 75400 , loss : 0.596434
step 75500 , training  accuracy 0.764198
step 75500 , loss : 0.601696
step 75600 , training  accuracy 0.780864
step 75600 , loss : 0.597559
step 75700 , training  accuracy 0.783951
step 75700 , loss : 0.595485
step 75800 , training  accuracy 0.78642
step 75800 , loss : 0.597599
step 75900 , training  accuracy 0.776543
step 75900 , loss : 0.596509
step 76000 , training  accuracy 0.787037
step 76000 , loss : 0.597862
step 76100 , training  accuracy 0.788889
step 76100 , loss : 0.596201
step 76200 , training  accuracy 0.79321
step 76200 , loss : 0.596462
step 76300 , training  accuracy 0.788272
step 76300 , loss : 0.596352
step 76400 , training  accuracy 0.796296
step 76400 , loss : 0.591557
step 76500 , training  accuracy 0.789506
step 76500 , loss : 0.59569
step 76600 , training  accuracy 0.788272
step 76600 , loss : 0.596854
step 76700 , training  accuracy 0.792593
step 76700 , loss : 0.592869
step 76800 , training  accuracy 0.804321
step 76800 , loss : 0.593249
step 76900 , training  accuracy 0.790741
step 76900 , loss : 0.593527
step 77000 , training  accuracy 0.783951
step 77000 , loss : 0.596174
step 77100 , training  accuracy 0.791358
step 77100 , loss : 0.594647
step 77200 , training  accuracy 0.788889
step 77200 , loss : 0.592549
step 77300 , training  accuracy 0.782716
step 77300 , loss : 0.596988
step 77400 , training  accuracy 0.779012
step 77400 , loss : 0.595348
step 77500 , training  accuracy 0.787037
step 77500 , loss : 0.598912
step 77600 , training  accuracy 0.783333
step 77600 , loss : 0.597682
step 77700 , training  accuracy 0.784568
step 77700 , loss : 0.597315
step 77800 , training  accuracy 0.790741
step 77800 , loss : 0.597295
step 77900 , training  accuracy 0.782716
step 77900 , loss : 0.599903
step 78000 , training  accuracy 0.787037
step 78000 , loss : 0.5997
step 78100 , training  accuracy 0.788272
step 78100 , loss : 0.598274
step 78200 , training  accuracy 0.791975
step 78200 , loss : 0.595291
step 78300 , training  accuracy 0.792593
step 78300 , loss : 0.5962
step 78400 , training  accuracy 0.789506
step 78400 , loss : 0.59422
step 78500 , training  accuracy 0.793827
step 78500 , loss : 0.592141
step 78600 , training  accuracy 0.792593
step 78600 , loss : 0.596205
step 78700 , training  accuracy 0.784568
step 78700 , loss : 0.596457
step 78800 , training  accuracy 0.789506
step 78800 , loss : 0.595847
step 78900 , training  accuracy 0.780864
step 78900 , loss : 0.601811
step 79000 , training  accuracy 0.779218
step 79000 , loss : 0.601264
step 79100 , training  accuracy 0.785185
step 79100 , loss : 0.597694
step 79200 , training  accuracy 0.793827
step 79200 , loss : 0.592326
step 79300 , training  accuracy 0.788889
step 79300 , loss : 0.597841
step 79400 , training  accuracy 0.775309
step 79400 , loss : 0.601735
step 79500 , training  accuracy 0.775926
step 79500 , loss : 0.599058
step 79600 , training  accuracy 0.782099
step 79600 , loss : 0.600066
step 79700 , training  accuracy 0.77963
step 79700 , loss : 0.600531
step 79800 , training  accuracy 0.790741
step 79800 , loss : 0.59461
step 79900 , training  accuracy 0.778395
step 79900 , loss : 0.602538
step 80000 , training  accuracy 0.773457
step 80000 , loss : 0.600779
step 80100 , training  accuracy 0.790741
step 80100 , loss : 0.595901
step 80200 , training  accuracy 0.794444
step 80200 , loss : 0.591561
step 80300 , training  accuracy 0.783951
step 80300 , loss : 0.59981
step 80400 , training  accuracy 0.790741
step 80400 , loss : 0.595664
step 80500 , training  accuracy 0.781687
step 80500 , loss : 0.599352
step 80600 , training  accuracy 0.793827
step 80600 , loss : 0.594022
step 80700 , training  accuracy 0.774074
step 80700 , loss : 0.601208
step 80800 , training  accuracy 0.791975
step 80800 , loss : 0.595868
step 80900 , training  accuracy 0.793827
step 80900 , loss : 0.597088
step 81000 , training  accuracy 0.777778
step 81000 , loss : 0.599143
step 81100 , training  accuracy 0.790123
step 81100 , loss : 0.595976
step 81200 , training  accuracy 0.785802
step 81200 , loss : 0.597323
step 81300 , training  accuracy 0.793827
step 81300 , loss : 0.59278
step 81400 , training  accuracy 0.791975
step 81400 , loss : 0.595127
step 81500 , training  accuracy 0.785185
step 81500 , loss : 0.603973
step 81600 , training  accuracy 0.784568
step 81600 , loss : 0.600636
step 81700 , training  accuracy 0.793827
step 81700 , loss : 0.590708
step 81800 , training  accuracy 0.787654
step 81800 , loss : 0.595531
step 81900 , training  accuracy 0.790741
step 81900 , loss : 0.598804
step 82000 , training  accuracy 0.791975
step 82000 , loss : 0.596285
step 82100 , training  accuracy 0.77963
step 82100 , loss : 0.59829
step 82200 , training  accuracy 0.783333
step 82200 , loss : 0.595568
step 82300 , training  accuracy 0.785185
step 82300 , loss : 0.59292
step 82400 , training  accuracy 0.794444
step 82400 , loss : 0.597195
step 82500 , training  accuracy 0.791358
step 82500 , loss : 0.59381
step 82600 , training  accuracy 0.787654
step 82600 , loss : 0.59618
step 82700 , training  accuracy 0.795062
step 82700 , loss : 0.592365
step 82800 , training  accuracy 0.783951
step 82800 , loss : 0.59564
step 82900 , training  accuracy 0.784568
step 82900 , loss : 0.598573
step 83000 , training  accuracy 0.77963
step 83000 , loss : 0.598975
step 83100 , training  accuracy 0.789506
step 83100 , loss : 0.596105
step 83200 , training  accuracy 0.791358
step 83200 , loss : 0.594432
step 83300 , training  accuracy 0.79321
step 83300 , loss : 0.594825
step 83400 , training  accuracy 0.782716
step 83400 , loss : 0.598098
step 83500 , training  accuracy 0.797531
step 83500 , loss : 0.59223
step 83600 , training  accuracy 0.789506
step 83600 , loss : 0.599534
step 83700 , training  accuracy 0.793827
step 83700 , loss : 0.591046
step 83800 , training  accuracy 0.783333
step 83800 , loss : 0.597286
step 83900 , training  accuracy 0.774074
step 83900 , loss : 0.596908
step 84000 , training  accuracy 0.789506
step 84000 , loss : 0.594438
step 84100 , training  accuracy 0.781481
step 84100 , loss : 0.596024
step 84200 , training  accuracy 0.784568
step 84200 , loss : 0.595913
step 84300 , training  accuracy 0.784568
step 84300 , loss : 0.594195
step 84400 , training  accuracy 0.783333
step 84400 , loss : 0.594257
step 84500 , training  accuracy 0.783951
step 84500 , loss : 0.598616
step 84600 , training  accuracy 0.790741
step 84600 , loss : 0.59531
step 84700 , training  accuracy 0.798148
step 84700 , loss : 0.59215
step 84800 , training  accuracy 0.790123
step 84800 , loss : 0.59482
step 84900 , training  accuracy 0.77963
step 84900 , loss : 0.598556
step 85000 , training  accuracy 0.780864
step 85000 , loss : 0.592159
step 85100 , training  accuracy 0.785185
step 85100 , loss : 0.594536
step 85200 , training  accuracy 0.795679
step 85200 , loss : 0.593475
step 85300 , training  accuracy 0.777778
step 85300 , loss : 0.599971
step 85400 , training  accuracy 0.780247
step 85400 , loss : 0.60024
step 85500 , training  accuracy 0.780864
step 85500 , loss : 0.601875
step 85600 , training  accuracy 0.794444
step 85600 , loss : 0.592675
step 85700 , training  accuracy 0.783333
step 85700 , loss : 0.59486
step 85800 , training  accuracy 0.790123
step 85800 , loss : 0.59786
step 85900 , training  accuracy 0.798148
step 85900 , loss : 0.591167
step 86000 , training  accuracy 0.796502
step 86000 , loss : 0.591992
step 86100 , training  accuracy 0.790741
step 86100 , loss : 0.590785
step 86200 , training  accuracy 0.794444
step 86200 , loss : 0.596451
step 86300 , training  accuracy 0.783951
step 86300 , loss : 0.595187
step 86400 , training  accuracy 0.787654
step 86400 , loss : 0.597046
step 86500 , training  accuracy 0.783951
step 86500 , loss : 0.597966
step 86600 , training  accuracy 0.782099
step 86600 , loss : 0.598759
step 86700 , training  accuracy 0.795062
step 86700 , loss : 0.594372
step 86800 , training  accuracy 0.801235
step 86800 , loss : 0.591186
step 86900 , training  accuracy 0.783539
step 86900 , loss : 0.59757
step 87000 , training  accuracy 0.790123
step 87000 , loss : 0.595395
step 87100 , training  accuracy 0.787654
step 87100 , loss : 0.597119
step 87200 , training  accuracy 0.782099
step 87200 , loss : 0.594329
step 87300 , training  accuracy 0.777161
step 87300 , loss : 0.59909
step 87400 , training  accuracy 0.791975
step 87400 , loss : 0.59751
step 87500 , training  accuracy 0.80679
step 87500 , loss : 0.596111
step 87600 , training  accuracy 0.790741
step 87600 , loss : 0.596443
step 87700 , training  accuracy 0.791975
step 87700 , loss : 0.593845
step 87800 , training  accuracy 0.791975
step 87800 , loss : 0.593282
step 87900 , training  accuracy 0.796296
step 87900 , loss : 0.594338
step 88000 , training  accuracy 0.791358
step 88000 , loss : 0.591275
step 88100 , training  accuracy 0.779012
step 88100 , loss : 0.597566
step 88200 , training  accuracy 0.77037
step 88200 , loss : 0.600261
step 88300 , training  accuracy 0.780864
step 88300 , loss : 0.598124
step 88400 , training  accuracy 0.781481
step 88400 , loss : 0.59661
step 88500 , training  accuracy 0.782099
step 88500 , loss : 0.598375
step 88600 , training  accuracy 0.780247
step 88600 , loss : 0.599739
step 88700 , training  accuracy 0.774074
step 88700 , loss : 0.604533
step 88800 , training  accuracy 0.792593
step 88800 , loss : 0.595364
step 88900 , training  accuracy 0.785802
step 88900 , loss : 0.59745
step 89000 , training  accuracy 0.803704
step 89000 , loss : 0.588752
step 89100 , training  accuracy 0.798765
step 89100 , loss : 0.593747
step 89200 , training  accuracy 0.78642
step 89200 , loss : 0.599555
step 89300 , training  accuracy 0.788889
step 89300 , loss : 0.59543
step 89400 , training  accuracy 0.784568
step 89400 , loss : 0.596503
step 89500 , training  accuracy 0.775514
step 89500 , loss : 0.600205
step 89600 , training  accuracy 0.790123
step 89600 , loss : 0.594146
step 89700 , training  accuracy 0.777984
step 89700 , loss : 0.596313
step 89800 , training  accuracy 0.785802
step 89800 , loss : 0.598323
step 89900 , training  accuracy 0.789506
step 89900 , loss : 0.596131
step 90000 , training  accuracy 0.802469
step 90000 , loss : 0.591644
step 90100 , training  accuracy 0.78642
step 90100 , loss : 0.594941
step 90200 , training  accuracy 0.79321
step 90200 , loss : 0.596557
step 90300 , training  accuracy 0.783951
step 90300 , loss : 0.59999
step 90400 , training  accuracy 0.788272
step 90400 , loss : 0.598577
step 90500 , training  accuracy 0.793827
step 90500 , loss : 0.592782
step 90600 , training  accuracy 0.776132
step 90600 , loss : 0.600516
step 90700 , training  accuracy 0.791358
step 90700 , loss : 0.593559
step 90800 , training  accuracy 0.790741
step 90800 , loss : 0.593327
step 90900 , training  accuracy 0.788889
step 90900 , loss : 0.591969
step 91000 , training  accuracy 0.791975
step 91000 , loss : 0.595206
step 91100 , training  accuracy 0.77963
step 91100 , loss : 0.59706
step 91200 , training  accuracy 0.783951
step 91200 , loss : 0.593689
step 91300 , training  accuracy 0.783539
step 91300 , loss : 0.596248
step 91400 , training  accuracy 0.787654
step 91400 , loss : 0.594009
step 91500 , training  accuracy 0.787037
step 91500 , loss : 0.594366
step 91600 , training  accuracy 0.782099
step 91600 , loss : 0.596209
step 91700 , training  accuracy 0.800617
step 91700 , loss : 0.590901
step 91800 , training  accuracy 0.797531
step 91800 , loss : 0.593649
step 91900 , training  accuracy 0.793827
step 91900 , loss : 0.593837
step 92000 , training  accuracy 0.795062
step 92000 , loss : 0.596344
step 92100 , training  accuracy 0.791975
step 92100 , loss : 0.595498
step 92200 , training  accuracy 0.796914
step 92200 , loss : 0.589707
step 92300 , training  accuracy 0.782716
step 92300 , loss : 0.604243
step 92400 , training  accuracy 0.789506
step 92400 , loss : 0.59581
step 92500 , training  accuracy 0.791975
step 92500 , loss : 0.596299
step 92600 , training  accuracy 0.789506
step 92600 , loss : 0.595328
step 92700 , training  accuracy 0.797531
step 92700 , loss : 0.58937
step 92800 , training  accuracy 0.784568
step 92800 , loss : 0.594774
step 92900 , training  accuracy 0.792593
step 92900 , loss : 0.595067
step 93000 , training  accuracy 0.798148
step 93000 , loss : 0.592318
step 93100 , training  accuracy 0.777778
step 93100 , loss : 0.596863
step 93200 , training  accuracy 0.784568
step 93200 , loss : 0.598526
step 93300 , training  accuracy 0.78642
step 93300 , loss : 0.595564
step 93400 , training  accuracy 0.801235
step 93400 , loss : 0.591266
step 93500 , training  accuracy 0.790947
step 93500 , loss : 0.597831
step 93600 , training  accuracy 0.795679
step 93600 , loss : 0.5966
step 93700 , training  accuracy 0.788889
step 93700 , loss : 0.591805
step 93800 , training  accuracy 0.806173
step 93800 , loss : 0.592245
step 93900 , training  accuracy 0.801852
step 93900 , loss : 0.593256
step 94000 , training  accuracy 0.782922
step 94000 , loss : 0.595783
step 94100 , training  accuracy 0.787654
step 94100 , loss : 0.597505
step 94200 , training  accuracy 0.794444
step 94200 , loss : 0.594738
step 94300 , training  accuracy 0.793827
step 94300 , loss : 0.593284
step 94400 , training  accuracy 0.797531
step 94400 , loss : 0.591382
step 94500 , training  accuracy 0.795679
step 94500 , loss : 0.593632
step 94600 , training  accuracy 0.807407
step 94600 , loss : 0.587372
step 94700 , training  accuracy 0.794444
step 94700 , loss : 0.592226
step 94800 , training  accuracy 0.790741
step 94800 , loss : 0.590178
step 94900 , training  accuracy 0.796296
step 94900 , loss : 0.591674
step 95000 , training  accuracy 0.781481
step 95000 , loss : 0.597683
step 95100 , training  accuracy 0.785802
step 95100 , loss : 0.594899
step 95200 , training  accuracy 0.791975
step 95200 , loss : 0.591363
step 95300 , training  accuracy 0.801235
step 95300 , loss : 0.592888
step 95400 , training  accuracy 0.796296
step 95400 , loss : 0.59401
step 95500 , training  accuracy 0.793827
step 95500 , loss : 0.589988
step 95600 , training  accuracy 0.789506
step 95600 , loss : 0.597117
step 95700 , training  accuracy 0.801235
step 95700 , loss : 0.587431
step 95800 , training  accuracy 0.801852
step 95800 , loss : 0.589374
step 95900 , training  accuracy 0.797531
step 95900 , loss : 0.591298
step 96000 , training  accuracy 0.787654
step 96000 , loss : 0.59681
step 96100 , training  accuracy 0.797531
step 96100 , loss : 0.597438
step 96200 , training  accuracy 0.8
step 96200 , loss : 0.591631
step 96300 , training  accuracy 0.79465
step 96300 , loss : 0.592898
step 96400 , training  accuracy 0.771605
step 96400 , loss : 0.600592
step 96500 , training  accuracy 0.793827
step 96500 , loss : 0.596095
step 96600 , training  accuracy 0.787654
step 96600 , loss : 0.595565
step 96700 , training  accuracy 0.804938
step 96700 , loss : 0.593009
step 96800 , training  accuracy 0.798765
step 96800 , loss : 0.591944
step 96900 , training  accuracy 0.796914
step 96900 , loss : 0.594529
step 97000 , training  accuracy 0.799383
step 97000 , loss : 0.592654
step 97100 , training  accuracy 0.794444
step 97100 , loss : 0.593297
step 97200 , training  accuracy 0.791358
step 97200 , loss : 0.596273
step 97300 , training  accuracy 0.793827
step 97300 , loss : 0.593518
step 97400 , training  accuracy 0.797531
step 97400 , loss : 0.593818
step 97500 , training  accuracy 0.799383
step 97500 , loss : 0.592149
step 97600 , training  accuracy 0.8
step 97600 , loss : 0.590234
step 97700 , training  accuracy 0.795679
step 97700 , loss : 0.592285
step 97800 , training  accuracy 0.797531
step 97800 , loss : 0.59144
step 97900 , training  accuracy 0.794444
step 97900 , loss : 0.59012
step 98000 , training  accuracy 0.804321
step 98000 , loss : 0.593181
step 98100 , training  accuracy 0.804321
step 98100 , loss : 0.589423
step 98200 , training  accuracy 0.799383
step 98200 , loss : 0.591338
step 98300 , training  accuracy 0.807407
step 98300 , loss : 0.588688
step 98400 , training  accuracy 0.805556
step 98400 , loss : 0.588808
step 98500 , training  accuracy 0.795679
step 98500 , loss : 0.591958
step 98600 , training  accuracy 0.790123
step 98600 , loss : 0.595857
step 98700 , training  accuracy 0.806173
step 98700 , loss : 0.588789
step 98800 , training  accuracy 0.785185
step 98800 , loss : 0.596139
step 98900 , training  accuracy 0.805556
step 98900 , loss : 0.589811
step 99000 , training  accuracy 0.789506
step 99000 , loss : 0.590236
step 99100 , training  accuracy 0.788889
step 99100 , loss : 0.597338
step 99200 , training  accuracy 0.798148
step 99200 , loss : 0.589645
step 99300 , training  accuracy 0.794444
step 99300 , loss : 0.59317
step 99400 , training  accuracy 0.795062
step 99400 , loss : 0.593637
step 99500 , training  accuracy 0.798148
step 99500 , loss : 0.592351
step 99600 , training  accuracy 0.801852
step 99600 , loss : 0.591051
step 99700 , training  accuracy 0.79321
step 99700 , loss : 0.590859
step 99800 , training  accuracy 0.800617
step 99800 , loss : 0.588718
step 99900 , training  accuracy 0.791975
step 99900 , loss : 0.591043
step 100000 , training  accuracy 0.802469
step 100000 , loss : 0.591377
step 100100 , training  accuracy 0.788889
step 100100 , loss : 0.593036
step 100200 , training  accuracy 0.796296
step 100200 , loss : 0.591298
step 100300 , training  accuracy 0.787037
step 100300 , loss : 0.594316
step 100400 , training  accuracy 0.79321
step 100400 , loss : 0.592614
step 100500 , training  accuracy 0.785802
step 100500 , loss : 0.595942
step 100600 , training  accuracy 0.805556
step 100600 , loss : 0.593263
step 100700 , training  accuracy 0.793827
step 100700 , loss : 0.598778
step 100800 , training  accuracy 0.794444
step 100800 , loss : 0.592029
step 100900 , training  accuracy 0.792593
step 100900 , loss : 0.593761
step 101000 , training  accuracy 0.801235
step 101000 , loss : 0.591633
step 101100 , training  accuracy 0.801235
step 101100 , loss : 0.591514
step 101200 , training  accuracy 0.782716
step 101200 , loss : 0.598554
step 101300 , training  accuracy 0.788272
step 101300 , loss : 0.590719
step 101400 , training  accuracy 0.793827
step 101400 , loss : 0.594564
step 101500 , training  accuracy 0.809877
step 101500 , loss : 0.587327
step 101600 , training  accuracy 0.809877
step 101600 , loss : 0.588738
step 101700 , training  accuracy 0.800617
step 101700 , loss : 0.59329
step 101800 , training  accuracy 0.793827
step 101800 , loss : 0.591358
step 101900 , training  accuracy 0.794444
step 101900 , loss : 0.591315
step 102000 , training  accuracy 0.8
step 102000 , loss : 0.592277
step 102100 , training  accuracy 0.796914
step 102100 , loss : 0.593269
step 102200 , training  accuracy 0.798148
step 102200 , loss : 0.59076
step 102300 , training  accuracy 0.789506
step 102300 , loss : 0.593913
step 102400 , training  accuracy 0.788889
step 102400 , loss : 0.595519
step 102500 , training  accuracy 0.79321
step 102500 , loss : 0.5919
step 102600 , training  accuracy 0.79321
step 102600 , loss : 0.591162
step 102700 , training  accuracy 0.782099
step 102700 , loss : 0.59713
step 102800 , training  accuracy 0.8
step 102800 , loss : 0.593825
step 102900 , training  accuracy 0.788889
step 102900 , loss : 0.595903
step 103000 , training  accuracy 0.788889
step 103000 , loss : 0.594414
step 103100 , training  accuracy 0.80679
step 103100 , loss : 0.589681
step 103200 , training  accuracy 0.798148
step 103200 , loss : 0.591689
step 103300 , training  accuracy 0.784568
step 103300 , loss : 0.599502
step 103400 , training  accuracy 0.798148
step 103400 , loss : 0.592221
step 103500 , training  accuracy 0.799383
step 103500 , loss : 0.589696
step 103600 , training  accuracy 0.795679
step 103600 , loss : 0.595592
step 103700 , training  accuracy 0.791358
step 103700 , loss : 0.594656
step 103800 , training  accuracy 0.809877
step 103800 , loss : 0.590052
step 103900 , training  accuracy 0.788272
step 103900 , loss : 0.598764
step 104000 , training  accuracy 0.803086
step 104000 , loss : 0.58985
step 104100 , training  accuracy 0.785802
step 104100 , loss : 0.597153
step 104200 , training  accuracy 0.797531
step 104200 , loss : 0.591571
step 104300 , training  accuracy 0.794444
step 104300 , loss : 0.595497
step 104400 , training  accuracy 0.795062
step 104400 , loss : 0.592049
step 104500 , training  accuracy 0.787654
step 104500 , loss : 0.596418
step 104600 , training  accuracy 0.788272
step 104600 , loss : 0.5964
step 104700 , training  accuracy 0.801235
step 104700 , loss : 0.591652
step 104800 , training  accuracy 0.796296
step 104800 , loss : 0.592481
step 104900 , training  accuracy 0.80679
step 104900 , loss : 0.59152
step 105000 , training  accuracy 0.775309
step 105000 , loss : 0.600144
step 105100 , training  accuracy 0.795679
step 105100 , loss : 0.593657
step 105200 , training  accuracy 0.789095
step 105200 , loss : 0.59383
step 105300 , training  accuracy 0.782099
step 105300 , loss : 0.594829
step 105400 , training  accuracy 0.788272
step 105400 , loss : 0.596575
step 105500 , training  accuracy 0.788889
step 105500 , loss : 0.597224
step 105600 , training  accuracy 0.798148
step 105600 , loss : 0.589272
step 105700 , training  accuracy 0.796296
step 105700 , loss : 0.591783
step 105800 , training  accuracy 0.804321
step 105800 , loss : 0.592587
step 105900 , training  accuracy 0.805556
step 105900 , loss : 0.589948
step 106000 , training  accuracy 0.799383
step 106000 , loss : 0.589497
step 106100 , training  accuracy 0.801852
step 106100 , loss : 0.587207
step 106200 , training  accuracy 0.798148
step 106200 , loss : 0.590183
step 106300 , training  accuracy 0.795885
step 106300 , loss : 0.59104
step 106400 , training  accuracy 0.789506
step 106400 , loss : 0.595304
step 106500 , training  accuracy 0.799383
step 106500 , loss : 0.592067
step 106600 , training  accuracy 0.795679
step 106600 , loss : 0.591159
step 106700 , training  accuracy 0.790947
step 106700 , loss : 0.592883
step 106800 , training  accuracy 0.801852
step 106800 , loss : 0.587107
step 106900 , training  accuracy 0.801235
step 106900 , loss : 0.589849
step 107000 , training  accuracy 0.79321
step 107000 , loss : 0.59538
step 107100 , training  accuracy 0.793827
step 107100 , loss : 0.594241
step 107200 , training  accuracy 0.78642
step 107200 , loss : 0.595116
step 107300 , training  accuracy 0.80679
step 107300 , loss : 0.588888
step 107400 , training  accuracy 0.806173
step 107400 , loss : 0.587622
step 107500 , training  accuracy 0.787654
step 107500 , loss : 0.591966
step 107600 , training  accuracy 0.785185
step 107600 , loss : 0.59519
step 107700 , training  accuracy 0.78642
step 107700 , loss : 0.594813
step 107800 , training  accuracy 0.790741
step 107800 , loss : 0.590923
step 107900 , training  accuracy 0.790741
step 107900 , loss : 0.595116
step 108000 , training  accuracy 0.796296
step 108000 , loss : 0.59368
step 108100 , training  accuracy 0.790123
step 108100 , loss : 0.592988
step 108200 , training  accuracy 0.807407
step 108200 , loss : 0.589633
step 108300 , training  accuracy 0.801852
step 108300 , loss : 0.592062
step 108400 , training  accuracy 0.806173
step 108400 , loss : 0.590754
step 108500 , training  accuracy 0.796914
step 108500 , loss : 0.591405
step 108600 , training  accuracy 0.795062
step 108600 , loss : 0.592179
step 108700 , training  accuracy 0.797531
step 108700 , loss : 0.58843
step 108800 , training  accuracy 0.799383
step 108800 , loss : 0.592103
step 108900 , training  accuracy 0.788477
step 108900 , loss : 0.596675
step 109000 , training  accuracy 0.797531
step 109000 , loss : 0.591728
step 109100 , training  accuracy 0.796914
step 109100 , loss : 0.591388
step 109200 , training  accuracy 0.793827
step 109200 , loss : 0.591941
step 109300 , training  accuracy 0.798765
step 109300 , loss : 0.590936
step 109400 , training  accuracy 0.791975
step 109400 , loss : 0.595148
step 109500 , training  accuracy 0.797531
step 109500 , loss : 0.592155
step 109600 , training  accuracy 0.792593
step 109600 , loss : 0.594086
step 109700 , training  accuracy 0.8
step 109700 , loss : 0.594438
step 109800 , training  accuracy 0.796914
step 109800 , loss : 0.594528
step 109900 , training  accuracy 0.805556
step 109900 , loss : 0.589253
step 110000 , training  accuracy 0.787654
step 110000 , loss : 0.597584
step 110100 , training  accuracy 0.803086
step 110100 , loss : 0.593208
step 110200 , training  accuracy 0.798148
step 110200 , loss : 0.592139
step 110300 , training  accuracy 0.801235
step 110300 , loss : 0.59023
step 110400 , training  accuracy 0.791975
step 110400 , loss : 0.590421
step 110500 , training  accuracy 0.790123
step 110500 , loss : 0.59552
step 110600 , training  accuracy 0.798148
step 110600 , loss : 0.593307
step 110700 , training  accuracy 0.796914
step 110700 , loss : 0.588639
step 110800 , training  accuracy 0.788889
step 110800 , loss : 0.59129
step 110900 , training  accuracy 0.795679
step 110900 , loss : 0.592041
step 111000 , training  accuracy 0.802469
step 111000 , loss : 0.591302
step 111100 , training  accuracy 0.796914
step 111100 , loss : 0.595831
step 111200 , training  accuracy 0.795062
step 111200 , loss : 0.595177
step 111300 , training  accuracy 0.797531
step 111300 , loss : 0.593474
step 111400 , training  accuracy 0.809259
step 111400 , loss : 0.591126
step 111500 , training  accuracy 0.807407
step 111500 , loss : 0.591016
step 111600 , training  accuracy 0.793827
step 111600 , loss : 0.594933
step 111700 , training  accuracy 0.795679
step 111700 , loss : 0.591013
step 111800 , training  accuracy 0.808025
step 111800 , loss : 0.590051
step 111900 , training  accuracy 0.803704
step 111900 , loss : 0.596137
step 112000 , training  accuracy 0.795062
step 112000 , loss : 0.592127
step 112100 , training  accuracy 0.804938
step 112100 , loss : 0.593928
step 112200 , training  accuracy 0.798765
step 112200 , loss : 0.589882
step 112300 , training  accuracy 0.800617
step 112300 , loss : 0.591094
step 112400 , training  accuracy 0.801852
step 112400 , loss : 0.587262
step 112500 , training  accuracy 0.806173
step 112500 , loss : 0.591328
step 112600 , training  accuracy 0.808642
step 112600 , loss : 0.591893
step 112700 , training  accuracy 0.807407
step 112700 , loss : 0.590862
step 112800 , training  accuracy 0.804321
step 112800 , loss : 0.586879
step 112900 , training  accuracy 0.800617
step 112900 , loss : 0.589824
step 113000 , training  accuracy 0.793827
step 113000 , loss : 0.593559
step 113100 , training  accuracy 0.792593
step 113100 , loss : 0.593391
step 113200 , training  accuracy 0.791358
step 113200 , loss : 0.586679
step 113300 , training  accuracy 0.801235
step 113300 , loss : 0.590308
step 113400 , training  accuracy 0.802469
step 113400 , loss : 0.590319
step 113500 , training  accuracy 0.794444
step 113500 , loss : 0.594327
step 113600 , training  accuracy 0.793416
step 113600 , loss : 0.591512
step 113700 , training  accuracy 0.797531
step 113700 , loss : 0.590525
step 113800 , training  accuracy 0.797531
step 113800 , loss : 0.590002
step 113900 , training  accuracy 0.807407
step 113900 , loss : 0.589839
step 114000 , training  accuracy 0.79321
step 114000 , loss : 0.591425
step 114100 , training  accuracy 0.800617
step 114100 , loss : 0.588917
step 114200 , training  accuracy 0.794444
step 114200 , loss : 0.593571
step 114300 , training  accuracy 0.802469
step 114300 , loss : 0.587657
step 114400 , training  accuracy 0.8
step 114400 , loss : 0.592304
step 114500 , training  accuracy 0.796296
step 114500 , loss : 0.588709
step 114600 , training  accuracy 0.801235
step 114600 , loss : 0.589697
step 114700 , training  accuracy 0.804938
step 114700 , loss : 0.58759
step 114800 , training  accuracy 0.795062
step 114800 , loss : 0.590383
step 114900 , training  accuracy 0.79321
step 114900 , loss : 0.590123
step 115000 , training  accuracy 0.792593
step 115000 , loss : 0.588741
step 115100 , training  accuracy 0.796502
step 115100 , loss : 0.594938
step 115200 , training  accuracy 0.801235
step 115200 , loss : 0.587051
step 115300 , training  accuracy 0.803086
step 115300 , loss : 0.588938
step 115400 , training  accuracy 0.799383
step 115400 , loss : 0.589421
step 115500 , training  accuracy 0.798148
step 115500 , loss : 0.588049
step 115600 , training  accuracy 0.810494
step 115600 , loss : 0.587065
step 115700 , training  accuracy 0.804527
step 115700 , loss : 0.58909
step 115800 , training  accuracy 0.800823
step 115800 , loss : 0.590764
step 115900 , training  accuracy 0.796914
step 115900 , loss : 0.59295
step 116000 , training  accuracy 0.805556
step 116000 , loss : 0.592284
step 116100 , training  accuracy 0.801852
step 116100 , loss : 0.589143
step 116200 , training  accuracy 0.807407
step 116200 , loss : 0.588519
step 116300 , training  accuracy 0.803086
step 116300 , loss : 0.591478
step 116400 , training  accuracy 0.801235
step 116400 , loss : 0.58919
step 116500 , training  accuracy 0.811111
step 116500 , loss : 0.588472
step 116600 , training  accuracy 0.811728
step 116600 , loss : 0.589492
step 116700 , training  accuracy 0.803086
step 116700 , loss : 0.584527
step 116800 , training  accuracy 0.803704
step 116800 , loss : 0.591503
step 116900 , training  accuracy 0.798148
step 116900 , loss : 0.592911
step 117000 , training  accuracy 0.780247
step 117000 , loss : 0.597175
step 117100 , training  accuracy 0.803086
step 117100 , loss : 0.590525
step 117200 , training  accuracy 0.782716
step 117200 , loss : 0.592769
step 117300 , training  accuracy 0.801852
step 117300 , loss : 0.587832
step 117400 , training  accuracy 0.798148
step 117400 , loss : 0.591459
step 117500 , training  accuracy 0.805556
step 117500 , loss : 0.586875
step 117600 , training  accuracy 0.801235
step 117600 , loss : 0.591646
step 117700 , training  accuracy 0.8
step 117700 , loss : 0.595426
step 117800 , training  accuracy 0.789506
step 117800 , loss : 0.595229
step 117900 , training  accuracy 0.804321
step 117900 , loss : 0.590149
step 118000 , training  accuracy 0.802469
step 118000 , loss : 0.588978
step 118100 , training  accuracy 0.807407
step 118100 , loss : 0.590031
step 118200 , training  accuracy 0.801852
step 118200 , loss : 0.589157
step 118300 , training  accuracy 0.819136
step 118300 , loss : 0.585555
step 118400 , training  accuracy 0.818519
step 118400 , loss : 0.587248
step 118500 , training  accuracy 0.797531
step 118500 , loss : 0.588281
step 118600 , training  accuracy 0.798148
step 118600 , loss : 0.58921
step 118700 , training  accuracy 0.801852
step 118700 , loss : 0.59167
step 118800 , training  accuracy 0.812346
step 118800 , loss : 0.591477
step 118900 , training  accuracy 0.798148
step 118900 , loss : 0.59213
step 119000 , training  accuracy 0.796296
step 119000 , loss : 0.588659
step 119100 , training  accuracy 0.795062
step 119100 , loss : 0.589465
step 119200 , training  accuracy 0.804938
step 119200 , loss : 0.587436
step 119300 , training  accuracy 0.801852
step 119300 , loss : 0.592344
step 119400 , training  accuracy 0.805556
step 119400 , loss : 0.589736
step 119500 , training  accuracy 0.796296
step 119500 , loss : 0.592669
step 119600 , training  accuracy 0.801235
step 119600 , loss : 0.587074
step 119700 , training  accuracy 0.806173
step 119700 , loss : 0.587493
step 119800 , training  accuracy 0.790741
step 119800 , loss : 0.593727
step 119900 , training  accuracy 0.801852
step 119900 , loss : 0.589502
step 120000 , training  accuracy 0.796914
step 120000 , loss : 0.593946
step 120100 , training  accuracy 0.809877
step 120100 , loss : 0.590444
step 120200 , training  accuracy 0.809877
step 120200 , loss : 0.586981
step 120300 , training  accuracy 0.809877
step 120300 , loss : 0.587041
step 120400 , training  accuracy 0.811728
step 120400 , loss : 0.587585
step 120500 , training  accuracy 0.805556
step 120500 , loss : 0.58939
step 120600 , training  accuracy 0.806173
step 120600 , loss : 0.587542
step 120700 , training  accuracy 0.814815
step 120700 , loss : 0.584114
step 120800 , training  accuracy 0.804321
step 120800 , loss : 0.589444
step 120900 , training  accuracy 0.791975
step 120900 , loss : 0.59325
step 121000 , training  accuracy 0.814815
step 121000 , loss : 0.585994
step 121100 , training  accuracy 0.809877
step 121100 , loss : 0.587072
step 121200 , training  accuracy 0.8
step 121200 , loss : 0.589003
step 121300 , training  accuracy 0.80679
step 121300 , loss : 0.591086
step 121400 , training  accuracy 0.807407
step 121400 , loss : 0.584475
step 121500 , training  accuracy 0.784568
step 121500 , loss : 0.590293
step 121600 , training  accuracy 0.804938
step 121600 , loss : 0.587587
step 121700 , training  accuracy 0.808025
step 121700 , loss : 0.589995
step 121800 , training  accuracy 0.807407
step 121800 , loss : 0.587135
step 121900 , training  accuracy 0.803704
step 121900 , loss : 0.588178
step 122000 , training  accuracy 0.803086
step 122000 , loss : 0.589299
step 122100 , training  accuracy 0.822222
step 122100 , loss : 0.586149
step 122200 , training  accuracy 0.812346
step 122200 , loss : 0.584868
step 122300 , training  accuracy 0.791564
step 122300 , loss : 0.596024
step 122400 , training  accuracy 0.803704
step 122400 , loss : 0.589468
step 122500 , training  accuracy 0.81358
step 122500 , loss : 0.587898
step 122600 , training  accuracy 0.810494
step 122600 , loss : 0.586695
step 122700 , training  accuracy 0.802469
step 122700 , loss : 0.587895
step 122800 , training  accuracy 0.814198
step 122800 , loss : 0.591313
step 122900 , training  accuracy 0.810494
step 122900 , loss : 0.585824
step 123000 , training  accuracy 0.803086
step 123000 , loss : 0.591867
step 123100 , training  accuracy 0.801852
step 123100 , loss : 0.591346
step 123200 , training  accuracy 0.817284
step 123200 , loss : 0.587029
step 123300 , training  accuracy 0.82037
step 123300 , loss : 0.586473
step 123400 , training  accuracy 0.802469
step 123400 , loss : 0.590514
step 123500 , training  accuracy 0.821605
step 123500 , loss : 0.582722
step 123600 , training  accuracy 0.797531
step 123600 , loss : 0.591296
step 123700 , training  accuracy 0.803086
step 123700 , loss : 0.588461
step 123800 , training  accuracy 0.803086
step 123800 , loss : 0.59032
step 123900 , training  accuracy 0.801235
step 123900 , loss : 0.592423
step 124000 , training  accuracy 0.801852
step 124000 , loss : 0.593854
step 124100 , training  accuracy 0.804938
step 124100 , loss : 0.590007
step 124200 , training  accuracy 0.800617
step 124200 , loss : 0.59096
step 124300 , training  accuracy 0.812963
step 124300 , loss : 0.589271
step 124400 , training  accuracy 0.808642
step 124400 , loss : 0.588696
step 124500 , training  accuracy 0.801852
step 124500 , loss : 0.591122
step 124600 , training  accuracy 0.815432
step 124600 , loss : 0.583019
step 124700 , training  accuracy 0.807407
step 124700 , loss : 0.585412
step 124800 , training  accuracy 0.795679
step 124800 , loss : 0.588377
step 124900 , training  accuracy 0.802675
step 124900 , loss : 0.590049
step 125000 , training  accuracy 0.797531
step 125000 , loss : 0.591206
step 125100 , training  accuracy 0.805761
step 125100 , loss : 0.588728
step 125200 , training  accuracy 0.811728
step 125200 , loss : 0.588935
step 125300 , training  accuracy 0.804938
step 125300 , loss : 0.591488
step 125400 , training  accuracy 0.803704
step 125400 , loss : 0.585575
step 125500 , training  accuracy 0.81358
step 125500 , loss : 0.587159
step 125600 , training  accuracy 0.812346
step 125600 , loss : 0.585194
step 125700 , training  accuracy 0.79321
step 125700 , loss : 0.594609
step 125800 , training  accuracy 0.804321
step 125800 , loss : 0.589259
step 125900 , training  accuracy 0.805556
step 125900 , loss : 0.588951
step 126000 , training  accuracy 0.803086
step 126000 , loss : 0.589654
step 126100 , training  accuracy 0.81358
step 126100 , loss : 0.585836
step 126200 , training  accuracy 0.801852
step 126200 , loss : 0.592414
step 126300 , training  accuracy 0.808025
step 126300 , loss : 0.58558
step 126400 , training  accuracy 0.803086
step 126400 , loss : 0.589256
step 126500 , training  accuracy 0.80679
step 126500 , loss : 0.587215
step 126600 , training  accuracy 0.801852
step 126600 , loss : 0.590122
step 126700 , training  accuracy 0.807407
step 126700 , loss : 0.586794
step 126800 , training  accuracy 0.798765
step 126800 , loss : 0.589997
step 126900 , training  accuracy 0.803704
step 126900 , loss : 0.589519
step 127000 , training  accuracy 0.80679
step 127000 , loss : 0.590571
step 127100 , training  accuracy 0.808642
step 127100 , loss : 0.587612
step 127200 , training  accuracy 0.801852
step 127200 , loss : 0.591716
step 127300 , training  accuracy 0.798765
step 127300 , loss : 0.59032
step 127400 , training  accuracy 0.79321
step 127400 , loss : 0.593457
step 127500 , training  accuracy 0.8
step 127500 , loss : 0.591345
step 127600 , training  accuracy 0.789506
step 127600 , loss : 0.596108
step 127700 , training  accuracy 0.799383
step 127700 , loss : 0.590106
step 127800 , training  accuracy 0.809259
step 127800 , loss : 0.587176
step 127900 , training  accuracy 0.811111
step 127900 , loss : 0.587469
step 128000 , training  accuracy 0.810494
step 128000 , loss : 0.584229
step 128100 , training  accuracy 0.801852
step 128100 , loss : 0.590223
step 128200 , training  accuracy 0.798148
step 128200 , loss : 0.583938
step 128300 , training  accuracy 0.8
step 128300 , loss : 0.589855
step 128400 , training  accuracy 0.810494
step 128400 , loss : 0.584603
step 128500 , training  accuracy 0.8
step 128500 , loss : 0.588307
step 128600 , training  accuracy 0.803086
step 128600 , loss : 0.587926
step 128700 , training  accuracy 0.805556
step 128700 , loss : 0.582927
step 128800 , training  accuracy 0.803704
step 128800 , loss : 0.585388
step 128900 , training  accuracy 0.811111
step 128900 , loss : 0.58439
step 129000 , training  accuracy 0.823457
step 129000 , loss : 0.581476
step 129100 , training  accuracy 0.809259
step 129100 , loss : 0.582129
step 129200 , training  accuracy 0.814815
step 129200 , loss : 0.587422
step 129300 , training  accuracy 0.812963
step 129300 , loss : 0.584889
step 129400 , training  accuracy 0.804321
step 129400 , loss : 0.58345
step 129500 , training  accuracy 0.803704
step 129500 , loss : 0.590985
step 129600 , training  accuracy 0.809877
step 129600 , loss : 0.585369
step 129700 , training  accuracy 0.804938
step 129700 , loss : 0.585384
step 129800 , training  accuracy 0.809259
step 129800 , loss : 0.583162
step 129900 , training  accuracy 0.815432
step 129900 , loss : 0.584498
step 130000 , training  accuracy 0.809877
step 130000 , loss : 0.588501
step 130100 , training  accuracy 0.791975
step 130100 , loss : 0.593146
step 130200 , training  accuracy 0.803704
step 130200 , loss : 0.592975
step 130300 , training  accuracy 0.812346
step 130300 , loss : 0.584449
step 130400 , training  accuracy 0.800617
step 130400 , loss : 0.586121
step 130500 , training  accuracy 0.8
step 130500 , loss : 0.585938
step 130600 , training  accuracy 0.802058
step 130600 , loss : 0.587259
step 130700 , training  accuracy 0.799383
step 130700 , loss : 0.590703
step 130800 , training  accuracy 0.811111
step 130800 , loss : 0.586321
step 130900 , training  accuracy 0.817284
step 130900 , loss : 0.583368
step 131000 , training  accuracy 0.816049
step 131000 , loss : 0.584391
step 131100 , training  accuracy 0.804321
step 131100 , loss : 0.58966
step 131200 , training  accuracy 0.808025
step 131200 , loss : 0.585938
step 131300 , training  accuracy 0.801235
step 131300 , loss : 0.589836
step 131400 , training  accuracy 0.812963
step 131400 , loss : 0.58365
step 131500 , training  accuracy 0.807407
step 131500 , loss : 0.585418
step 131600 , training  accuracy 0.808025
step 131600 , loss : 0.589401
step 131700 , training  accuracy 0.804938
step 131700 , loss : 0.586404
step 131800 , training  accuracy 0.804321
step 131800 , loss : 0.589767
step 131900 , training  accuracy 0.81358
step 131900 , loss : 0.587519
step 132000 , training  accuracy 0.808025
step 132000 , loss : 0.590485
step 132100 , training  accuracy 0.798765
step 132100 , loss : 0.588876
step 132200 , training  accuracy 0.801235
step 132200 , loss : 0.586337
step 132300 , training  accuracy 0.801852
step 132300 , loss : 0.586236
step 132400 , training  accuracy 0.808025
step 132400 , loss : 0.586803
step 132500 , training  accuracy 0.798148
step 132500 , loss : 0.590239
step 132600 , training  accuracy 0.803086
step 132600 , loss : 0.58634
step 132700 , training  accuracy 0.809259
step 132700 , loss : 0.588716
step 132800 , training  accuracy 0.811111
step 132800 , loss : 0.587114
step 132900 , training  accuracy 0.812963
step 132900 , loss : 0.58182
step 133000 , training  accuracy 0.820988
step 133000 , loss : 0.58215
step 133100 , training  accuracy 0.801852
step 133100 , loss : 0.583765
step 133200 , training  accuracy 0.808642
step 133200 , loss : 0.586255
step 133300 , training  accuracy 0.80679
step 133300 , loss : 0.582903
step 133400 , training  accuracy 0.807407
step 133400 , loss : 0.58318
step 133500 , training  accuracy 0.803086
step 133500 , loss : 0.588588
step 133600 , training  accuracy 0.807407
step 133600 , loss : 0.585037
step 133700 , training  accuracy 0.809259
step 133700 , loss : 0.588693
step 133800 , training  accuracy 0.808025
step 133800 , loss : 0.587655
step 133900 , training  accuracy 0.81358
step 133900 , loss : 0.583493
step 134000 , training  accuracy 0.811728
step 134000 , loss : 0.586798
step 134100 , training  accuracy 0.805556
step 134100 , loss : 0.588752
step 134200 , training  accuracy 0.807407
step 134200 , loss : 0.585422
step 134300 , training  accuracy 0.802469
step 134300 , loss : 0.589627
step 134400 , training  accuracy 0.811728
step 134400 , loss : 0.58873
step 134500 , training  accuracy 0.822222
step 134500 , loss : 0.587286
step 134600 , training  accuracy 0.808025
step 134600 , loss : 0.590156
step 134700 , training  accuracy 0.809877
step 134700 , loss : 0.58744
step 134800 , training  accuracy 0.798765
step 134800 , loss : 0.586924
step 134900 , training  accuracy 0.816049
step 134900 , loss : 0.584356
step 135000 , training  accuracy 0.809259
step 135000 , loss : 0.590706
step 135100 , training  accuracy 0.805556
step 135100 , loss : 0.589109
step 135200 , training  accuracy 0.809877
step 135200 , loss : 0.582845
step 135300 , training  accuracy 0.803704
step 135300 , loss : 0.585697
step 135400 , training  accuracy 0.795679
step 135400 , loss : 0.588068
step 135500 , training  accuracy 0.800617
step 135500 , loss : 0.583188
step 135600 , training  accuracy 0.805556
step 135600 , loss : 0.583353
step 135700 , training  accuracy 0.799383
step 135700 , loss : 0.585376
step 135800 , training  accuracy 0.791975
step 135800 , loss : 0.587507
step 135900 , training  accuracy 0.805556
step 135900 , loss : 0.587947
step 136000 , training  accuracy 0.80679
step 136000 , loss : 0.587147
step 136100 , training  accuracy 0.798148
step 136100 , loss : 0.588875
step 136200 , training  accuracy 0.808642
step 136200 , loss : 0.587177
step 136300 , training  accuracy 0.792593
step 136300 , loss : 0.592928
step 136400 , training  accuracy 0.81358
step 136400 , loss : 0.586594
step 136500 , training  accuracy 0.814198
step 136500 , loss : 0.580485
step 136600 , training  accuracy 0.812963
step 136600 , loss : 0.583053
step 136700 , training  accuracy 0.803704
step 136700 , loss : 0.58944
step 136800 , training  accuracy 0.808642
step 136800 , loss : 0.588293
step 136900 , training  accuracy 0.816049
step 136900 , loss : 0.579569
step 137000 , training  accuracy 0.811111
step 137000 , loss : 0.586567
step 137100 , training  accuracy 0.804321
step 137100 , loss : 0.589726
step 137200 , training  accuracy 0.811728
step 137200 , loss : 0.585556
step 137300 , training  accuracy 0.810494
step 137300 , loss : 0.584132
step 137400 , training  accuracy 0.805556
step 137400 , loss : 0.587952
step 137500 , training  accuracy 0.812346
step 137500 , loss : 0.58472
step 137600 , training  accuracy 0.804321
step 137600 , loss : 0.585186
step 137700 , training  accuracy 0.808025
step 137700 , loss : 0.581571
step 137800 , training  accuracy 0.808642
step 137800 , loss : 0.583957
step 137900 , training  accuracy 0.77284
step 137900 , loss : 0.603987
step 138000 , training  accuracy 0.808642
step 138000 , loss : 0.584789
step 138100 , training  accuracy 0.802469
step 138100 , loss : 0.586423
step 138200 , training  accuracy 0.8
step 138200 , loss : 0.58705
step 138300 , training  accuracy 0.804938
step 138300 , loss : 0.584874
step 138400 , training  accuracy 0.802469
step 138400 , loss : 0.58706
step 138500 , training  accuracy 0.802469
step 138500 , loss : 0.584941
step 138600 , training  accuracy 0.814198
step 138600 , loss : 0.583868
step 138700 , training  accuracy 0.803704
step 138700 , loss : 0.588425
step 138800 , training  accuracy 0.803704
step 138800 , loss : 0.586442
step 138900 , training  accuracy 0.805556
step 138900 , loss : 0.582584
step 139000 , training  accuracy 0.805556
step 139000 , loss : 0.587049
step 139100 , training  accuracy 0.812963
step 139100 , loss : 0.586923
step 139200 , training  accuracy 0.812963
step 139200 , loss : 0.584717
step 139300 , training  accuracy 0.81358
step 139300 , loss : 0.583245
step 139400 , training  accuracy 0.809259
step 139400 , loss : 0.584152
step 139500 , training  accuracy 0.807407
step 139500 , loss : 0.586977
step 139600 , training  accuracy 0.804938
step 139600 , loss : 0.586949
step 139700 , training  accuracy 0.801852
step 139700 , loss : 0.585845
step 139800 , training  accuracy 0.81358
step 139800 , loss : 0.585629
step 139900 , training  accuracy 0.808642
step 139900 , loss : 0.583507
step 140000 , training  accuracy 0.811111
step 140000 , loss : 0.588307
step 140100 , training  accuracy 0.816049
step 140100 , loss : 0.585441
step 140200 , training  accuracy 0.799383
step 140200 , loss : 0.588417
step 140300 , training  accuracy 0.798765
step 140300 , loss : 0.58963
step 140400 , training  accuracy 0.814198
step 140400 , loss : 0.585288
step 140500 , training  accuracy 0.814815
step 140500 , loss : 0.581605
step 140600 , training  accuracy 0.812346
step 140600 , loss : 0.584606
step 140700 , training  accuracy 0.804321
step 140700 , loss : 0.587062
step 140800 , training  accuracy 0.809877
step 140800 , loss : 0.585618
step 140900 , training  accuracy 0.806173
step 140900 , loss : 0.58664
step 141000 , training  accuracy 0.808025
step 141000 , loss : 0.585406
step 141100 , training  accuracy 0.814815
step 141100 , loss : 0.584069
step 141200 , training  accuracy 0.801852
step 141200 , loss : 0.587221
step 141300 , training  accuracy 0.801852
step 141300 , loss : 0.585738
step 141400 , training  accuracy 0.802469
step 141400 , loss : 0.586577
step 141500 , training  accuracy 0.783333
step 141500 , loss : 0.593932
step 141600 , training  accuracy 0.796296
step 141600 , loss : 0.586528
step 141700 , training  accuracy 0.811728
step 141700 , loss : 0.580299
step 141800 , training  accuracy 0.801852
step 141800 , loss : 0.589385
step 141900 , training  accuracy 0.815432
step 141900 , loss : 0.580601
step 142000 , training  accuracy 0.815432
step 142000 , loss : 0.584834
step 142100 , training  accuracy 0.822222
step 142100 , loss : 0.581291
step 142200 , training  accuracy 0.800617
step 142200 , loss : 0.583972
step 142300 , training  accuracy 0.803704
step 142300 , loss : 0.584641
step 142400 , training  accuracy 0.796914
step 142400 , loss : 0.5893
step 142500 , training  accuracy 0.804321
step 142500 , loss : 0.582847
step 142600 , training  accuracy 0.816667
step 142600 , loss : 0.583226
step 142700 , training  accuracy 0.81358
step 142700 , loss : 0.581099
step 142800 , training  accuracy 0.808025
step 142800 , loss : 0.58798
step 142900 , training  accuracy 0.803704
step 142900 , loss : 0.589196
step 143000 , training  accuracy 0.801235
step 143000 , loss : 0.587926
step 143100 , training  accuracy 0.811728
step 143100 , loss : 0.585602
step 143200 , training  accuracy 0.819136
step 143200 , loss : 0.583673
step 143300 , training  accuracy 0.803704
step 143300 , loss : 0.585004
step 143400 , training  accuracy 0.808642
step 143400 , loss : 0.583873
step 143500 , training  accuracy 0.801235
step 143500 , loss : 0.589719
step 143600 , training  accuracy 0.810494
step 143600 , loss : 0.586242
step 143700 , training  accuracy 0.798148
step 143700 , loss : 0.586951
step 143800 , training  accuracy 0.807407
step 143800 , loss : 0.583585
step 143900 , training  accuracy 0.806173
step 143900 , loss : 0.583157
step 144000 , training  accuracy 0.800617
step 144000 , loss : 0.583532
step 144100 , training  accuracy 0.810494
step 144100 , loss : 0.581398
step 144200 , training  accuracy 0.811111
step 144200 , loss : 0.580283
step 144300 , training  accuracy 0.809259
step 144300 , loss : 0.582929
step 144400 , training  accuracy 0.803704
step 144400 , loss : 0.58316
step 144500 , training  accuracy 0.809877
step 144500 , loss : 0.583109
step 144600 , training  accuracy 0.804321
step 144600 , loss : 0.584715
step 144700 , training  accuracy 0.805556
step 144700 , loss : 0.586824
step 144800 , training  accuracy 0.800617
step 144800 , loss : 0.591138
step 144900 , training  accuracy 0.81358
step 144900 , loss : 0.582906
step 145000 , training  accuracy 0.817284
step 145000 , loss : 0.581186
step 145100 , training  accuracy 0.819753
step 145100 , loss : 0.581123
step 145200 , training  accuracy 0.816667
step 145200 , loss : 0.583373
step 145300 , training  accuracy 0.810494
step 145300 , loss : 0.586414
step 145400 , training  accuracy 0.805556
step 145400 , loss : 0.584659
step 145500 , training  accuracy 0.80679
step 145500 , loss : 0.584941
step 145600 , training  accuracy 0.810494
step 145600 , loss : 0.58213
step 145700 , training  accuracy 0.805556
step 145700 , loss : 0.584573
step 145800 , training  accuracy 0.811728
step 145800 , loss : 0.582904
step 145900 , training  accuracy 0.81358
step 145900 , loss : 0.581647
step 146000 , training  accuracy 0.806173
step 146000 , loss : 0.585367
step 146100 , training  accuracy 0.815432
step 146100 , loss : 0.585943
step 146200 , training  accuracy 0.795679
step 146200 , loss : 0.588474
step 146300 , training  accuracy 0.800617
step 146300 , loss : 0.584618
step 146400 , training  accuracy 0.811728
step 146400 , loss : 0.582118
step 146500 , training  accuracy 0.804321
step 146500 , loss : 0.584633
step 146600 , training  accuracy 0.812346
step 146600 , loss : 0.580431
step 146700 , training  accuracy 0.806173
step 146700 , loss : 0.579891
step 146800 , training  accuracy 0.811728
step 146800 , loss : 0.584244
step 146900 , training  accuracy 0.808025
step 146900 , loss : 0.582698
step 147000 , training  accuracy 0.815432
step 147000 , loss : 0.582555
step 147100 , training  accuracy 0.814815
step 147100 , loss : 0.584574
step 147200 , training  accuracy 0.816049
step 147200 , loss : 0.582835
step 147300 , training  accuracy 0.812963
step 147300 , loss : 0.581017
step 147400 , training  accuracy 0.804938
step 147400 , loss : 0.58554
step 147500 , training  accuracy 0.820988
step 147500 , loss : 0.581692
step 147600 , training  accuracy 0.800617
step 147600 , loss : 0.585513
step 147700 , training  accuracy 0.80679
step 147700 , loss : 0.581322
step 147800 , training  accuracy 0.807407
step 147800 , loss : 0.582749
step 147900 , training  accuracy 0.801235
step 147900 , loss : 0.583956
step 148000 , training  accuracy 0.811111
step 148000 , loss : 0.582838
step 148100 , training  accuracy 0.786008
step 148100 , loss : 0.591129
step 148200 , training  accuracy 0.797531
step 148200 , loss : 0.584847
step 148300 , training  accuracy 0.809259
step 148300 , loss : 0.585179
step 148400 , training  accuracy 0.794444
step 148400 , loss : 0.589998
step 148500 , training  accuracy 0.805556
step 148500 , loss : 0.587206
step 148600 , training  accuracy 0.812963
step 148600 , loss : 0.583437
step 148700 , training  accuracy 0.812963
step 148700 , loss : 0.580239
step 148800 , training  accuracy 0.811728
step 148800 , loss : 0.58426
step 148900 , training  accuracy 0.815432
step 148900 , loss : 0.582928
step 149000 , training  accuracy 0.819753
step 149000 , loss : 0.58314
step 149100 , training  accuracy 0.82037
step 149100 , loss : 0.58444
step 149200 , training  accuracy 0.809259
step 149200 , loss : 0.585367
step 149300 , training  accuracy 0.816049
step 149300 , loss : 0.581779
step 149400 , training  accuracy 0.810494
step 149400 , loss : 0.57948
step 149500 , training  accuracy 0.806173
step 149500 , loss : 0.579517
step 149600 , training  accuracy 0.805556
step 149600 , loss : 0.583188
step 149700 , training  accuracy 0.8
step 149700 , loss : 0.588696
step 149800 , training  accuracy 0.80679
step 149800 , loss : 0.585245
step 149900 , training  accuracy 0.8
step 149900 , loss : 0.587041
step 150000 , training  accuracy 0.804938
step 150000 , loss : 0.584201
step 150100 , training  accuracy 0.804321
step 150100 , loss : 0.584932
step 150200 , training  accuracy 0.791975
step 150200 , loss : 0.592053
step 150300 , training  accuracy 0.811111
step 150300 , loss : 0.583301
step 150400 , training  accuracy 0.821605
step 150400 , loss : 0.583388
step 150500 , training  accuracy 0.811111
step 150500 , loss : 0.585537
step 150600 , training  accuracy 0.812346
step 150600 , loss : 0.57955
step 150700 , training  accuracy 0.803086
step 150700 , loss : 0.585508
step 150800 , training  accuracy 0.818519
step 150800 , loss : 0.581736
step 150900 , training  accuracy 0.808025
step 150900 , loss : 0.586141
step 151000 , training  accuracy 0.797531
step 151000 , loss : 0.585193
step 151100 , training  accuracy 0.809259
step 151100 , loss : 0.581771
step 151200 , training  accuracy 0.811111
step 151200 , loss : 0.581476
step 151300 , training  accuracy 0.803704
step 151300 , loss : 0.58311
step 151400 , training  accuracy 0.816049
step 151400 , loss : 0.581621
step 151500 , training  accuracy 0.809877
step 151500 , loss : 0.581633
step 151600 , training  accuracy 0.80679
step 151600 , loss : 0.58551
step 151700 , training  accuracy 0.80679
step 151700 , loss : 0.586783
step 151800 , training  accuracy 0.812963
step 151800 , loss : 0.581716
step 151900 , training  accuracy 0.809877
step 151900 , loss : 0.582481
step 152000 , training  accuracy 0.815432
step 152000 , loss : 0.580698
step 152100 , training  accuracy 0.815432
step 152100 , loss : 0.583597
step 152200 , training  accuracy 0.814198
step 152200 , loss : 0.581012
step 152300 , training  accuracy 0.809259
step 152300 , loss : 0.583475
step 152400 , training  accuracy 0.816049
step 152400 , loss : 0.581484
step 152500 , training  accuracy 0.812963
step 152500 , loss : 0.581298
step 152600 , training  accuracy 0.822222
step 152600 , loss : 0.579181
step 152700 , training  accuracy 0.809877
step 152700 , loss : 0.580959
step 152800 , training  accuracy 0.80679
step 152800 , loss : 0.582116
step 152900 , training  accuracy 0.818519
step 152900 , loss : 0.579702
step 153000 , training  accuracy 0.822222
step 153000 , loss : 0.583275
step 153100 , training  accuracy 0.812963
step 153100 , loss : 0.58224
step 153200 , training  accuracy 0.816049
step 153200 , loss : 0.577077
step 153300 , training  accuracy 0.809877
step 153300 , loss : 0.583562
step 153400 , training  accuracy 0.801235
step 153400 , loss : 0.585946
step 153500 , training  accuracy 0.801235
step 153500 , loss : 0.580962
step 153600 , training  accuracy 0.809259
step 153600 , loss : 0.583748
step 153700 , training  accuracy 0.804321
step 153700 , loss : 0.580213
step 153800 , training  accuracy 0.798148
step 153800 , loss : 0.582099
step 153900 , training  accuracy 0.807407
step 153900 , loss : 0.579754
step 154000 , training  accuracy 0.806173
step 154000 , loss : 0.586937
step 154100 , training  accuracy 0.815432
step 154100 , loss : 0.581142
step 154200 , training  accuracy 0.811728
step 154200 , loss : 0.580553
step 154300 , training  accuracy 0.814198
step 154300 , loss : 0.57821
step 154400 , training  accuracy 0.804321
step 154400 , loss : 0.583057
step 154500 , training  accuracy 0.803704
step 154500 , loss : 0.586669
step 154600 , training  accuracy 0.808025
step 154600 , loss : 0.583102
step 154700 , training  accuracy 0.811728
step 154700 , loss : 0.582698
step 154800 , training  accuracy 0.807407
step 154800 , loss : 0.581351
step 154900 , training  accuracy 0.810494
step 154900 , loss : 0.578795
step 155000 , training  accuracy 0.808025
step 155000 , loss : 0.583656
step 155100 , training  accuracy 0.806173
step 155100 , loss : 0.586025
step 155200 , training  accuracy 0.806173
step 155200 , loss : 0.582347
step 155300 , training  accuracy 0.808025
step 155300 , loss : 0.584259
step 155400 , training  accuracy 0.805556
step 155400 , loss : 0.586238
step 155500 , training  accuracy 0.805556
step 155500 , loss : 0.585792
step 155600 , training  accuracy 0.812963
step 155600 , loss : 0.582898
step 155700 , training  accuracy 0.808642
step 155700 , loss : 0.581324
step 155800 , training  accuracy 0.800617
step 155800 , loss : 0.578751
step 155900 , training  accuracy 0.811728
step 155900 , loss : 0.583131
step 156000 , training  accuracy 0.799383
step 156000 , loss : 0.585907
step 156100 , training  accuracy 0.8
step 156100 , loss : 0.58336
step 156200 , training  accuracy 0.784568
step 156200 , loss : 0.59095
step 156300 , training  accuracy 0.814815
step 156300 , loss : 0.579454
step 156400 , training  accuracy 0.803086
step 156400 , loss : 0.585876
step 156500 , training  accuracy 0.81358
step 156500 , loss : 0.577487
step 156600 , training  accuracy 0.811111
step 156600 , loss : 0.585618
step 156700 , training  accuracy 0.789506
step 156700 , loss : 0.58509
step 156800 , training  accuracy 0.803704
step 156800 , loss : 0.583034
step 156900 , training  accuracy 0.799383
step 156900 , loss : 0.583294
step 157000 , training  accuracy 0.797531
step 157000 , loss : 0.589589
step 157100 , training  accuracy 0.799383
step 157100 , loss : 0.587646
step 157200 , training  accuracy 0.812963
step 157200 , loss : 0.585116
step 157300 , training  accuracy 0.808025
step 157300 , loss : 0.582229
step 157400 , training  accuracy 0.811111
step 157400 , loss : 0.582858
step 157500 , training  accuracy 0.812963
step 157500 , loss : 0.585042
step 157600 , training  accuracy 0.814815
step 157600 , loss : 0.58033
step 157700 , training  accuracy 0.816049
step 157700 , loss : 0.583277
step 157800 , training  accuracy 0.815432
step 157800 , loss : 0.582946
step 157900 , training  accuracy 0.796296
step 157900 , loss : 0.584699
step 158000 , training  accuracy 0.816667
step 158000 , loss : 0.579041
step 158100 , training  accuracy 0.803704
step 158100 , loss : 0.586498
step 158200 , training  accuracy 0.812963
step 158200 , loss : 0.580825
step 158300 , training  accuracy 0.795062
step 158300 , loss : 0.582454
step 158400 , training  accuracy 0.809877
step 158400 , loss : 0.582103
step 158500 , training  accuracy 0.810494
step 158500 , loss : 0.581067
step 158600 , training  accuracy 0.814815
step 158600 , loss : 0.579009
step 158700 , training  accuracy 0.818519
step 158700 , loss : 0.578924
step 158800 , training  accuracy 0.816667
step 158800 , loss : 0.582269
step 158900 , training  accuracy 0.801852
step 158900 , loss : 0.586472
step 159000 , training  accuracy 0.812346
step 159000 , loss : 0.581261
step 159100 , training  accuracy 0.801235
step 159100 , loss : 0.583175
step 159200 , training  accuracy 0.798148
step 159200 , loss : 0.581607
step 159300 , training  accuracy 0.802469
step 159300 , loss : 0.58308
step 159400 , training  accuracy 0.809259
step 159400 , loss : 0.584786
step 159500 , training  accuracy 0.811111
step 159500 , loss : 0.585984
step 159600 , training  accuracy 0.809259
step 159600 , loss : 0.581657
step 159700 , training  accuracy 0.816049
step 159700 , loss : 0.580676
step 159800 , training  accuracy 0.79321
step 159800 , loss : 0.581572
step 159900 , training  accuracy 0.797531
step 159900 , loss : 0.583935
step 160000 , training  accuracy 0.796914
step 160000 , loss : 0.58531
step 160100 , training  accuracy 0.801852
step 160100 , loss : 0.585886
step 160200 , training  accuracy 0.812346
step 160200 , loss : 0.578049
step 160300 , training  accuracy 0.819136
step 160300 , loss : 0.581422
step 160400 , training  accuracy 0.80679
step 160400 , loss : 0.582705
step 160500 , training  accuracy 0.803086
step 160500 , loss : 0.581138
step 160600 , training  accuracy 0.800617
step 160600 , loss : 0.582336
step 160700 , training  accuracy 0.81358
step 160700 , loss : 0.579361
step 160800 , training  accuracy 0.816667
step 160800 , loss : 0.580542
step 160900 , training  accuracy 0.814815
step 160900 , loss : 0.58099
step 161000 , training  accuracy 0.80679
step 161000 , loss : 0.582079
step 161100 , training  accuracy 0.802469
step 161100 , loss : 0.583232
step 161200 , training  accuracy 0.798765
step 161200 , loss : 0.585789
step 161300 , training  accuracy 0.809877
step 161300 , loss : 0.584321
step 161400 , training  accuracy 0.803086
step 161400 , loss : 0.577994
step 161500 , training  accuracy 0.811111
step 161500 , loss : 0.582789
step 161600 , training  accuracy 0.805556
step 161600 , loss : 0.582384
step 161700 , training  accuracy 0.810494
step 161700 , loss : 0.579701
step 161800 , training  accuracy 0.808642
step 161800 , loss : 0.582803
step 161900 , training  accuracy 0.8
step 161900 , loss : 0.586137
step 162000 , training  accuracy 0.825309
step 162000 , loss : 0.579613
step 162100 , training  accuracy 0.804321
step 162100 , loss : 0.583609
step 162200 , training  accuracy 0.811111
step 162200 , loss : 0.579822
step 162300 , training  accuracy 0.804938
step 162300 , loss : 0.585077
step 162400 , training  accuracy 0.807407
step 162400 , loss : 0.58022
step 162500 , training  accuracy 0.793827
step 162500 , loss : 0.58847
step 162600 , training  accuracy 0.81358
step 162600 , loss : 0.583496
step 162700 , training  accuracy 0.804938
step 162700 , loss : 0.582639
step 162800 , training  accuracy 0.812963
step 162800 , loss : 0.580462
step 162900 , training  accuracy 0.811728
step 162900 , loss : 0.581955
step 163000 , training  accuracy 0.798148
step 163000 , loss : 0.58434
step 163100 , training  accuracy 0.798765
step 163100 , loss : 0.583082
step 163200 , training  accuracy 0.800617
step 163200 , loss : 0.584288
step 163300 , training  accuracy 0.804321
step 163300 , loss : 0.580465
step 163400 , training  accuracy 0.817284
step 163400 , loss : 0.579619
step 163500 , training  accuracy 0.805556
step 163500 , loss : 0.582315
step 163600 , training  accuracy 0.804321
step 163600 , loss : 0.584425
step 163700 , training  accuracy 0.807407
step 163700 , loss : 0.579711
step 163800 , training  accuracy 0.808025
step 163800 , loss : 0.581607
step 163900 , training  accuracy 0.796296
step 163900 , loss : 0.586564
step 164000 , training  accuracy 0.801852
step 164000 , loss : 0.58357
step 164100 , training  accuracy 0.807407
step 164100 , loss : 0.58227
step 164200 , training  accuracy 0.808642
step 164200 , loss : 0.580502
step 164300 , training  accuracy 0.806173
step 164300 , loss : 0.580965
step 164400 , training  accuracy 0.799383
step 164400 , loss : 0.582321
step 164500 , training  accuracy 0.808642
step 164500 , loss : 0.5811
step 164600 , training  accuracy 0.816049
step 164600 , loss : 0.577484
step 164700 , training  accuracy 0.811728
step 164700 , loss : 0.58094
step 164800 , training  accuracy 0.80679
step 164800 , loss : 0.57876
step 164900 , training  accuracy 0.80679
step 164900 , loss : 0.581972
step 165000 , training  accuracy 0.805556
step 165000 , loss : 0.581797
step 165100 , training  accuracy 0.810494
step 165100 , loss : 0.578857
step 165200 , training  accuracy 0.811111
step 165200 , loss : 0.579151
step 165300 , training  accuracy 0.806173
step 165300 , loss : 0.583592
step 165400 , training  accuracy 0.816049
step 165400 , loss : 0.581322
step 165500 , training  accuracy 0.811728
step 165500 , loss : 0.580012
step 165600 , training  accuracy 0.809877
step 165600 , loss : 0.584291
step 165700 , training  accuracy 0.790947
step 165700 , loss : 0.588607
step 165800 , training  accuracy 0.808025
step 165800 , loss : 0.581414
step 165900 , training  accuracy 0.804938
step 165900 , loss : 0.579202
step 166000 , training  accuracy 0.807407
step 166000 , loss : 0.581251
step 166100 , training  accuracy 0.801235
step 166100 , loss : 0.587924
step 166200 , training  accuracy 0.815432
step 166200 , loss : 0.578444
step 166300 , training  accuracy 0.800617
step 166300 , loss : 0.582489
step 166400 , training  accuracy 0.803086
step 166400 , loss : 0.579757
step 166500 , training  accuracy 0.794444
step 166500 , loss : 0.583147
step 166600 , training  accuracy 0.803704
step 166600 , loss : 0.580172
step 166700 , training  accuracy 0.801235
step 166700 , loss : 0.581349
step 166800 , training  accuracy 0.80679
step 166800 , loss : 0.582224
step 166900 , training  accuracy 0.793827
step 166900 , loss : 0.58748
step 167000 , training  accuracy 0.816667
step 167000 , loss : 0.582622
step 167100 , training  accuracy 0.809877
step 167100 , loss : 0.576868
step 167200 , training  accuracy 0.809877
step 167200 , loss : 0.577753
step 167300 , training  accuracy 0.814815
step 167300 , loss : 0.579245
step 167400 , training  accuracy 0.809259
step 167400 , loss : 0.580284
step 167500 , training  accuracy 0.798148
step 167500 , loss : 0.582658
step 167600 , training  accuracy 0.811728
step 167600 , loss : 0.580065
step 167700 , training  accuracy 0.808642
step 167700 , loss : 0.580269
step 167800 , training  accuracy 0.801235
step 167800 , loss : 0.583936
step 167900 , training  accuracy 0.788889
step 167900 , loss : 0.584813
step 168000 , training  accuracy 0.799383
step 168000 , loss : 0.579882
step 168100 , training  accuracy 0.802469
step 168100 , loss : 0.583728
step 168200 , training  accuracy 0.805556
step 168200 , loss : 0.582614
step 168300 , training  accuracy 0.810494
step 168300 , loss : 0.580404
step 168400 , training  accuracy 0.800617
step 168400 , loss : 0.583086
step 168500 , training  accuracy 0.789506
step 168500 , loss : 0.583184
step 168600 , training  accuracy 0.793827
step 168600 , loss : 0.584975
step 168700 , training  accuracy 0.809259
step 168700 , loss : 0.583286
step 168800 , training  accuracy 0.807407
step 168800 , loss : 0.581931
step 168900 , training  accuracy 0.810494
step 168900 , loss : 0.580894
step 169000 , training  accuracy 0.816667
step 169000 , loss : 0.579191
step 169100 , training  accuracy 0.812346
step 169100 , loss : 0.579454
step 169200 , training  accuracy 0.805556
step 169200 , loss : 0.579697
step 169300 , training  accuracy 0.815432
step 169300 , loss : 0.574138
step 169400 , training  accuracy 0.799383
step 169400 , loss : 0.581058
step 169500 , training  accuracy 0.799383
step 169500 , loss : 0.583107
step 169600 , training  accuracy 0.800617
step 169600 , loss : 0.581542
step 169700 , training  accuracy 0.812963
step 169700 , loss : 0.580667
step 169800 , training  accuracy 0.805556
step 169800 , loss : 0.579666
step 169900 , training  accuracy 0.803086
step 169900 , loss : 0.586781
step 170000 , training  accuracy 0.819136
step 170000 , loss : 0.579431
step 170100 , training  accuracy 0.803704
step 170100 , loss : 0.581197
step 170200 , training  accuracy 0.807407
step 170200 , loss : 0.582468
step 170300 , training  accuracy 0.808025
step 170300 , loss : 0.580908
step 170400 , training  accuracy 0.807407
step 170400 , loss : 0.576347
step 170500 , training  accuracy 0.800617
step 170500 , loss : 0.579785
step 170600 , training  accuracy 0.805556
step 170600 , loss : 0.578816
step 170700 , training  accuracy 0.809877
step 170700 , loss : 0.579225
step 170800 , training  accuracy 0.788477
step 170800 , loss : 0.586844
step 170900 , training  accuracy 0.791975
step 170900 , loss : 0.578697
step 171000 , training  accuracy 0.814815
step 171000 , loss : 0.579294
step 171100 , training  accuracy 0.817901
step 171100 , loss : 0.574337
step 171200 , training  accuracy 0.799383
step 171200 , loss : 0.580542
step 171300 , training  accuracy 0.798765
step 171300 , loss : 0.583766
step 171400 , training  accuracy 0.818519
step 171400 , loss : 0.584436
step 171500 , training  accuracy 0.812963
step 171500 , loss : 0.578496
step 171600 , training  accuracy 0.796296
step 171600 , loss : 0.587934
step 171700 , training  accuracy 0.814198
step 171700 , loss : 0.578454
step 171800 , training  accuracy 0.807407
step 171800 , loss : 0.580844
step 171900 , training  accuracy 0.803704
step 171900 , loss : 0.579797
step 172000 , training  accuracy 0.799383
step 172000 , loss : 0.584266
step 172100 , training  accuracy 0.801852
step 172100 , loss : 0.582312
step 172200 , training  accuracy 0.801235
step 172200 , loss : 0.580317
step 172300 , training  accuracy 0.812963
step 172300 , loss : 0.580104
step 172400 , training  accuracy 0.805556
step 172400 , loss : 0.581425
step 172500 , training  accuracy 0.798148
step 172500 , loss : 0.583598
step 172600 , training  accuracy 0.808642
step 172600 , loss : 0.579789
step 172700 , training  accuracy 0.811111
step 172700 , loss : 0.577627
step 172800 , training  accuracy 0.801235
step 172800 , loss : 0.580148
step 172900 , training  accuracy 0.807407
step 172900 , loss : 0.576463
step 173000 , training  accuracy 0.816049
step 173000 , loss : 0.579866
step 173100 , training  accuracy 0.816049
step 173100 , loss : 0.577165
step 173200 , training  accuracy 0.804938
step 173200 , loss : 0.584855
step 173300 , training  accuracy 0.814815
step 173300 , loss : 0.576037
step 173400 , training  accuracy 0.814815
step 173400 , loss : 0.576903
step 173500 , training  accuracy 0.812346
step 173500 , loss : 0.579081
step 173600 , training  accuracy 0.814198
step 173600 , loss : 0.581152
step 173700 , training  accuracy 0.817284
step 173700 , loss : 0.577889
step 173800 , training  accuracy 0.803704
step 173800 , loss : 0.58142
step 173900 , training  accuracy 0.816667
step 173900 , loss : 0.572262
step 174000 , training  accuracy 0.811728
step 174000 , loss : 0.578423
step 174100 , training  accuracy 0.812346
step 174100 , loss : 0.579483
step 174200 , training  accuracy 0.809877
step 174200 , loss : 0.582684
step 174300 , training  accuracy 0.804321
step 174300 , loss : 0.579535
step 174400 , training  accuracy 0.820988
step 174400 , loss : 0.572966
step 174500 , training  accuracy 0.815432
step 174500 , loss : 0.575483
step 174600 , training  accuracy 0.817284
step 174600 , loss : 0.575469
step 174700 , training  accuracy 0.811728
step 174700 , loss : 0.578253
step 174800 , training  accuracy 0.815432
step 174800 , loss : 0.575033
step 174900 , training  accuracy 0.804938
step 174900 , loss : 0.580325
step 175000 , training  accuracy 0.805556
step 175000 , loss : 0.578249
step 175100 , training  accuracy 0.808642
step 175100 , loss : 0.581168
step 175200 , training  accuracy 0.815432
step 175200 , loss : 0.576122
step 175300 , training  accuracy 0.804321
step 175300 , loss : 0.579652
step 175400 , training  accuracy 0.819753
step 175400 , loss : 0.579073
step 175500 , training  accuracy 0.812346
step 175500 , loss : 0.576325
step 175600 , training  accuracy 0.816049
step 175600 , loss : 0.576624
step 175700 , training  accuracy 0.803086
step 175700 , loss : 0.582949
step 175800 , training  accuracy 0.820988
step 175800 , loss : 0.575263
step 175900 , training  accuracy 0.790741
step 175900 , loss : 0.590986
step 176000 , training  accuracy 0.810494
step 176000 , loss : 0.576496
step 176100 , training  accuracy 0.817901
step 176100 , loss : 0.577468
step 176200 , training  accuracy 0.821605
step 176200 , loss : 0.576363
step 176300 , training  accuracy 0.816049
step 176300 , loss : 0.577607
step 176400 , training  accuracy 0.804321
step 176400 , loss : 0.579605
step 176500 , training  accuracy 0.814198
step 176500 , loss : 0.580263
step 176600 , training  accuracy 0.809259
step 176600 , loss : 0.576727
step 176700 , training  accuracy 0.801852
step 176700 , loss : 0.579137
step 176800 , training  accuracy 0.810494
step 176800 , loss : 0.578489
step 176900 , training  accuracy 0.820988
step 176900 , loss : 0.575573
step 177000 , training  accuracy 0.817901
step 177000 , loss : 0.578567
step 177100 , training  accuracy 0.812346
step 177100 , loss : 0.58013
step 177200 , training  accuracy 0.812963
step 177200 , loss : 0.575956
step 177300 , training  accuracy 0.798148
step 177300 , loss : 0.582184
step 177400 , training  accuracy 0.811728
step 177400 , loss : 0.579681
step 177500 , training  accuracy 0.802469
step 177500 , loss : 0.581212
step 177600 , training  accuracy 0.816667
step 177600 , loss : 0.577766
step 177700 , training  accuracy 0.810494
step 177700 , loss : 0.577803
step 177800 , training  accuracy 0.806173
step 177800 , loss : 0.585264
step 177900 , training  accuracy 0.820988
step 177900 , loss : 0.57583
step 178000 , training  accuracy 0.830864
step 178000 , loss : 0.576052
step 178100 , training  accuracy 0.820988
step 178100 , loss : 0.575483
step 178200 , training  accuracy 0.820988
step 178200 , loss : 0.577525
step 178300 , training  accuracy 0.818519
step 178300 , loss : 0.576585
step 178400 , training  accuracy 0.816049
step 178400 , loss : 0.579207
step 178500 , training  accuracy 0.795267
step 178500 , loss : 0.586525
step 178600 , training  accuracy 0.819753
step 178600 , loss : 0.574813
step 178700 , training  accuracy 0.837037
step 178700 , loss : 0.57215
step 178800 , training  accuracy 0.811728
step 178800 , loss : 0.580434
step 178900 , training  accuracy 0.828395
step 178900 , loss : 0.578816
step 179000 , training  accuracy 0.798148
step 179000 , loss : 0.581088
step 179100 , training  accuracy 0.82963
step 179100 , loss : 0.573089
step 179200 , training  accuracy 0.819753
step 179200 , loss : 0.572426
step 179300 , training  accuracy 0.807407
step 179300 , loss : 0.580556
step 179400 , training  accuracy 0.814815
step 179400 , loss : 0.577433
step 179500 , training  accuracy 0.812346
step 179500 , loss : 0.576894
step 179600 , training  accuracy 0.816667
step 179600 , loss : 0.574277
step 179700 , training  accuracy 0.8
step 179700 , loss : 0.58482
step 179800 , training  accuracy 0.822222
step 179800 , loss : 0.577893
step 179900 , training  accuracy 0.809259
step 179900 , loss : 0.580679
step 180000 , training  accuracy 0.825309
step 180000 , loss : 0.575012
step 180100 , training  accuracy 0.828395
step 180100 , loss : 0.573433
step 180200 , training  accuracy 0.820988
step 180200 , loss : 0.57412
step 180300 , training  accuracy 0.826543
step 180300 , loss : 0.572741
step 180400 , training  accuracy 0.812963
step 180400 , loss : 0.577308
step 180500 , training  accuracy 0.816667
step 180500 , loss : 0.577541
step 180600 , training  accuracy 0.815432
step 180600 , loss : 0.578217
step 180700 , training  accuracy 0.809877
step 180700 , loss : 0.579932
step 180800 , training  accuracy 0.788889
step 180800 , loss : 0.588087
step 180900 , training  accuracy 0.809259
step 180900 , loss : 0.579917
step 181000 , training  accuracy 0.817901
step 181000 , loss : 0.57539
step 181100 , training  accuracy 0.823457
step 181100 , loss : 0.573684
step 181200 , training  accuracy 0.815432
step 181200 , loss : 0.580323
step 181300 , training  accuracy 0.808025
step 181300 , loss : 0.579881
step 181400 , training  accuracy 0.82037
step 181400 , loss : 0.574692
step 181500 , training  accuracy 0.830247
step 181500 , loss : 0.570378
step 181600 , training  accuracy 0.826543
step 181600 , loss : 0.571869
step 181700 , training  accuracy 0.824074
step 181700 , loss : 0.574529
step 181800 , training  accuracy 0.81358
step 181800 , loss : 0.577118
step 181900 , training  accuracy 0.801235
step 181900 , loss : 0.576099
step 182000 , training  accuracy 0.81358
step 182000 , loss : 0.57844
step 182100 , training  accuracy 0.827778
step 182100 , loss : 0.574451
step 182200 , training  accuracy 0.823457
step 182200 , loss : 0.571516
step 182300 , training  accuracy 0.799588
step 182300 , loss : 0.577804
step 182400 , training  accuracy 0.814815
step 182400 , loss : 0.577528
step 182500 , training  accuracy 0.817284
step 182500 , loss : 0.578309
step 182600 , training  accuracy 0.821605
step 182600 , loss : 0.574106
step 182700 , training  accuracy 0.816667
step 182700 , loss : 0.577034
step 182800 , training  accuracy 0.816049
step 182800 , loss : 0.574194
step 182900 , training  accuracy 0.812963
step 182900 , loss : 0.575751
step 183000 , training  accuracy 0.809877
step 183000 , loss : 0.574957
step 183100 , training  accuracy 0.816049
step 183100 , loss : 0.578993
step 183200 , training  accuracy 0.791975
step 183200 , loss : 0.584711
step 183300 , training  accuracy 0.812346
step 183300 , loss : 0.577511
step 183400 , training  accuracy 0.806173
step 183400 , loss : 0.581394
step 183500 , training  accuracy 0.810494
step 183500 , loss : 0.581113
step 183600 , training  accuracy 0.819136
step 183600 , loss : 0.57649
step 183700 , training  accuracy 0.807407
step 183700 , loss : 0.579851
step 183800 , training  accuracy 0.811728
step 183800 , loss : 0.575356
step 183900 , training  accuracy 0.818519
step 183900 , loss : 0.575063
step 184000 , training  accuracy 0.818519
step 184000 , loss : 0.578533
step 184100 , training  accuracy 0.809259
step 184100 , loss : 0.579574
step 184200 , training  accuracy 0.800823
step 184200 , loss : 0.583568
step 184300 , training  accuracy 0.81358
step 184300 , loss : 0.57982
step 184400 , training  accuracy 0.804938
step 184400 , loss : 0.580571
step 184500 , training  accuracy 0.821605
step 184500 , loss : 0.576458
step 184600 , training  accuracy 0.826543
step 184600 , loss : 0.575857
step 184700 , training  accuracy 0.823457
step 184700 , loss : 0.580237
step 184800 , training  accuracy 0.810494
step 184800 , loss : 0.58035
step 184900 , training  accuracy 0.814815
step 184900 , loss : 0.577602
step 185000 , training  accuracy 0.804321
step 185000 , loss : 0.581145
step 185100 , training  accuracy 0.802469
step 185100 , loss : 0.578961
step 185200 , training  accuracy 0.814198
step 185200 , loss : 0.579612
step 185300 , training  accuracy 0.804321
step 185300 , loss : 0.578822
step 185400 , training  accuracy 0.811728
step 185400 , loss : 0.577092
step 185500 , training  accuracy 0.814198
step 185500 , loss : 0.57891
step 185600 , training  accuracy 0.819136
step 185600 , loss : 0.579702
step 185700 , training  accuracy 0.81358
step 185700 , loss : 0.577285
step 185800 , training  accuracy 0.82963
step 185800 , loss : 0.571364
step 185900 , training  accuracy 0.814198
step 185900 , loss : 0.573482
step 186000 , training  accuracy 0.801235
step 186000 , loss : 0.578087
step 186100 , training  accuracy 0.811111
step 186100 , loss : 0.574497
step 186200 , training  accuracy 0.807407
step 186200 , loss : 0.576842
step 186300 , training  accuracy 0.818519
step 186300 , loss : 0.575804
step 186400 , training  accuracy 0.815432
step 186400 , loss : 0.57164
step 186500 , training  accuracy 0.804938
step 186500 , loss : 0.579338
step 186600 , training  accuracy 0.810494
step 186600 , loss : 0.575541
step 186700 , training  accuracy 0.821605
step 186700 , loss : 0.57026
step 186800 , training  accuracy 0.817901
step 186800 , loss : 0.574902
step 186900 , training  accuracy 0.807407
step 186900 , loss : 0.579762
step 187000 , training  accuracy 0.790947
step 187000 , loss : 0.584343
step 187100 , training  accuracy 0.817284
step 187100 , loss : 0.573371
step 187200 , training  accuracy 0.815432
step 187200 , loss : 0.575488
step 187300 , training  accuracy 0.814815
step 187300 , loss : 0.572228
step 187400 , training  accuracy 0.819136
step 187400 , loss : 0.574094
step 187500 , training  accuracy 0.805556
step 187500 , loss : 0.580597
step 187600 , training  accuracy 0.818519
step 187600 , loss : 0.572776
step 187700 , training  accuracy 0.82284
step 187700 , loss : 0.574758
step 187800 , training  accuracy 0.82284
step 187800 , loss : 0.572661
step 187900 , training  accuracy 0.823457
step 187900 , loss : 0.574949
step 188000 , training  accuracy 0.819753
step 188000 , loss : 0.574114
step 188100 , training  accuracy 0.80679
step 188100 , loss : 0.574609
step 188200 , training  accuracy 0.812963
step 188200 , loss : 0.576273
step 188300 , training  accuracy 0.82284
step 188300 , loss : 0.574473
step 188400 , training  accuracy 0.816049
step 188400 , loss : 0.577022
step 188500 , training  accuracy 0.808025
step 188500 , loss : 0.577204
step 188600 , training  accuracy 0.807407
step 188600 , loss : 0.579444
step 188700 , training  accuracy 0.810494
step 188700 , loss : 0.578563
step 188800 , training  accuracy 0.808642
step 188800 , loss : 0.57926
step 188900 , training  accuracy 0.817901
step 188900 , loss : 0.576476
step 189000 , training  accuracy 0.814198
step 189000 , loss : 0.576828
step 189100 , training  accuracy 0.817284
step 189100 , loss : 0.572636
step 189200 , training  accuracy 0.806996
step 189200 , loss : 0.5819
step 189300 , training  accuracy 0.816049
step 189300 , loss : 0.576165
step 189400 , training  accuracy 0.818519
step 189400 , loss : 0.574826
step 189500 , training  accuracy 0.800617
step 189500 , loss : 0.577641
step 189600 , training  accuracy 0.808642
step 189600 , loss : 0.578308
step 189700 , training  accuracy 0.806173
step 189700 , loss : 0.573336
step 189800 , training  accuracy 0.82037
step 189800 , loss : 0.573553
step 189900 , training  accuracy 0.8
step 189900 , loss : 0.577988
step 190000 , training  accuracy 0.798354
step 190000 , loss : 0.57902
step 190100 , training  accuracy 0.811728
step 190100 , loss : 0.573704
step 190200 , training  accuracy 0.801235
step 190200 , loss : 0.575616
step 190300 , training  accuracy 0.817284
step 190300 , loss : 0.571139
step 190400 , training  accuracy 0.815432
step 190400 , loss : 0.574111
step 190500 , training  accuracy 0.811728
step 190500 , loss : 0.578456
step 190600 , training  accuracy 0.809877
step 190600 , loss : 0.572161
step 190700 , training  accuracy 0.809259
step 190700 , loss : 0.577591
step 190800 , training  accuracy 0.783333
step 190800 , loss : 0.589704
step 190900 , training  accuracy 0.814815
step 190900 , loss : 0.574881
step 191000 , training  accuracy 0.803086
step 191000 , loss : 0.578432
step 191100 , training  accuracy 0.812963
step 191100 , loss : 0.5733
step 191200 , training  accuracy 0.824074
step 191200 , loss : 0.574492
step 191300 , training  accuracy 0.815432
step 191300 , loss : 0.575946
step 191400 , training  accuracy 0.81358
step 191400 , loss : 0.570331
step 191500 , training  accuracy 0.812963
step 191500 , loss : 0.577743
step 191600 , training  accuracy 0.814198
step 191600 , loss : 0.574011
step 191700 , training  accuracy 0.810494
step 191700 , loss : 0.576978
step 191800 , training  accuracy 0.806173
step 191800 , loss : 0.579425
step 191900 , training  accuracy 0.80679
step 191900 , loss : 0.574782
step 192000 , training  accuracy 0.797531
step 192000 , loss : 0.582195
step 192100 , training  accuracy 0.809259
step 192100 , loss : 0.574056
step 192200 , training  accuracy 0.808025
step 192200 , loss : 0.57772
step 192300 , training  accuracy 0.811111
step 192300 , loss : 0.578686
step 192400 , training  accuracy 0.814198
step 192400 , loss : 0.578658
step 192500 , training  accuracy 0.80679
step 192500 , loss : 0.577547
step 192600 , training  accuracy 0.818519
step 192600 , loss : 0.572287
step 192700 , training  accuracy 0.79321
step 192700 , loss : 0.582651
step 192800 , training  accuracy 0.811728
step 192800 , loss : 0.578964
step 192900 , training  accuracy 0.807407
step 192900 , loss : 0.578559
step 193000 , training  accuracy 0.819753
step 193000 , loss : 0.574361
step 193100 , training  accuracy 0.81358
step 193100 , loss : 0.577864
step 193200 , training  accuracy 0.81358
step 193200 , loss : 0.578251
step 193300 , training  accuracy 0.812346
step 193300 , loss : 0.575325
step 193400 , training  accuracy 0.815432
step 193400 , loss : 0.573085
step 193500 , training  accuracy 0.815432
step 193500 , loss : 0.575824
step 193600 , training  accuracy 0.810082
step 193600 , loss : 0.577582
step 193700 , training  accuracy 0.80823
step 193700 , loss : 0.575606
step 193800 , training  accuracy 0.809465
step 193800 , loss : 0.576724
step 193900 , training  accuracy 0.804527
step 193900 , loss : 0.576512
step 194000 , training  accuracy 0.808642
step 194000 , loss : 0.572485
step 194100 , training  accuracy 0.817284
step 194100 , loss : 0.572824
step 194200 , training  accuracy 0.821605
step 194200 , loss : 0.573113
step 194300 , training  accuracy 0.811728
step 194300 , loss : 0.577979
step 194400 , training  accuracy 0.815432
step 194400 , loss : 0.57317
step 194500 , training  accuracy 0.831481
step 194500 , loss : 0.57228
step 194600 , training  accuracy 0.811728
step 194600 , loss : 0.576069
step 194700 , training  accuracy 0.808025
step 194700 , loss : 0.572952
step 194800 , training  accuracy 0.818519
step 194800 , loss : 0.573934
step 194900 , training  accuracy 0.817901
step 194900 , loss : 0.574849
step 195000 , training  accuracy 0.817901
step 195000 , loss : 0.574168
step 195100 , training  accuracy 0.810494
step 195100 , loss : 0.580128
step 195200 , training  accuracy 0.816667
step 195200 , loss : 0.573202
step 195300 , training  accuracy 0.808025
step 195300 , loss : 0.575255
step 195400 , training  accuracy 0.801852
step 195400 , loss : 0.574463
step 195500 , training  accuracy 0.805556
step 195500 , loss : 0.574024
step 195600 , training  accuracy 0.808642
step 195600 , loss : 0.570064
step 195700 , training  accuracy 0.819753
step 195700 , loss : 0.572945
step 195800 , training  accuracy 0.811111
step 195800 , loss : 0.573847
step 195900 , training  accuracy 0.817284
step 195900 , loss : 0.569994
step 196000 , training  accuracy 0.822222
step 196000 , loss : 0.57519
step 196100 , training  accuracy 0.820988
step 196100 , loss : 0.571388
step 196200 , training  accuracy 0.821605
step 196200 , loss : 0.573596
step 196300 , training  accuracy 0.817901
step 196300 , loss : 0.571626
step 196400 , training  accuracy 0.815432
step 196400 , loss : 0.573606
step 196500 , training  accuracy 0.798765
step 196500 , loss : 0.578376
step 196600 , training  accuracy 0.808642
step 196600 , loss : 0.579568
step 196700 , training  accuracy 0.817284
step 196700 , loss : 0.574276
step 196800 , training  accuracy 0.815432
step 196800 , loss : 0.57531
step 196900 , training  accuracy 0.814198
step 196900 , loss : 0.574221
step 197000 , training  accuracy 0.800617
step 197000 , loss : 0.575275
step 197100 , training  accuracy 0.799383
step 197100 , loss : 0.576327
step 197200 , training  accuracy 0.812963
step 197200 , loss : 0.57615
step 197300 , training  accuracy 0.811111
step 197300 , loss : 0.573047
step 197400 , training  accuracy 0.811111
step 197400 , loss : 0.571064
step 197500 , training  accuracy 0.810494
step 197500 , loss : 0.569997
step 197600 , training  accuracy 0.806173
step 197600 , loss : 0.578353
step 197700 , training  accuracy 0.811111
step 197700 , loss : 0.577216
step 197800 , training  accuracy 0.801852
step 197800 , loss : 0.580674
step 197900 , training  accuracy 0.811111
step 197900 , loss : 0.575119
step 198000 , training  accuracy 0.820988
step 198000 , loss : 0.577413
step 198100 , training  accuracy 0.822222
step 198100 , loss : 0.572477
step 198200 , training  accuracy 0.817901
step 198200 , loss : 0.575476
step 198300 , training  accuracy 0.82284
step 198300 , loss : 0.573705
step 198400 , training  accuracy 0.812346
step 198400 , loss : 0.575744
step 198500 , training  accuracy 0.809877
step 198500 , loss : 0.574407
step 198600 , training  accuracy 0.803704
step 198600 , loss : 0.577773
step 198700 , training  accuracy 0.811111
step 198700 , loss : 0.57548
step 198800 , training  accuracy 0.816049
step 198800 , loss : 0.57429
step 198900 , training  accuracy 0.809877
step 198900 , loss : 0.573871
step 199000 , training  accuracy 0.817901
step 199000 , loss : 0.572755
step 199100 , training  accuracy 0.817284
step 199100 , loss : 0.571591
step 199200 , training  accuracy 0.814198
step 199200 , loss : 0.570059
step 199300 , training  accuracy 0.814198
step 199300 , loss : 0.572229
step 199400 , training  accuracy 0.807407
step 199400 , loss : 0.57314
step 199500 , training  accuracy 0.806379
step 199500 , loss : 0.574496
step 199600 , training  accuracy 0.821605
step 199600 , loss : 0.57073
step 199700 , training  accuracy 0.810494
step 199700 , loss : 0.574747
step 199800 , training  accuracy 0.804938
step 199800 , loss : 0.575268
step 199900 , training  accuracy 0.800206
step 199900 , loss : 0.578131
step 200000 , training  accuracy 0.814198
step 200000 , loss : 0.569564
step 200100 , training  accuracy 0.811111
step 200100 , loss : 0.576994
step 200200 , training  accuracy 0.803704
step 200200 , loss : 0.575629
step 200300 , training  accuracy 0.809259
step 200300 , loss : 0.578017
step 200400 , training  accuracy 0.814198
step 200400 , loss : 0.572525
step 200500 , training  accuracy 0.796502
step 200500 , loss : 0.578189
step 200600 , training  accuracy 0.803704
step 200600 , loss : 0.577432
step 200700 , training  accuracy 0.810494
step 200700 , loss : 0.577174
step 200800 , training  accuracy 0.817284
step 200800 , loss : 0.57308
step 200900 , training  accuracy 0.809259
step 200900 , loss : 0.572945
step 201000 , training  accuracy 0.808025
step 201000 , loss : 0.572323
step 201100 , training  accuracy 0.816667
step 201100 , loss : 0.572684
step 201200 , training  accuracy 0.796914
step 201200 , loss : 0.579761
step 201300 , training  accuracy 0.823457
step 201300 , loss : 0.575469
step 201400 , training  accuracy 0.824691
step 201400 , loss : 0.570907
step 201500 , training  accuracy 0.798765
step 201500 , loss : 0.577308
step 201600 , training  accuracy 0.819753
step 201600 , loss : 0.573834
step 201700 , training  accuracy 0.81358
step 201700 , loss : 0.573924
step 201800 , training  accuracy 0.816049
step 201800 , loss : 0.572749
step 201900 , training  accuracy 0.807407
step 201900 , loss : 0.575131
step 202000 , training  accuracy 0.811111
step 202000 , loss : 0.577376
step 202100 , training  accuracy 0.803704
step 202100 , loss : 0.578414
step 202200 , training  accuracy 0.82037
step 202200 , loss : 0.572988
step 202300 , training  accuracy 0.80144
step 202300 , loss : 0.579325
step 202400 , training  accuracy 0.796914
step 202400 , loss : 0.578229
step 202500 , training  accuracy 0.810082
step 202500 , loss : 0.575458
step 202600 , training  accuracy 0.796502
step 202600 , loss : 0.580296
step 202700 , training  accuracy 0.795062
step 202700 , loss : 0.580554
step 202800 , training  accuracy 0.803704
step 202800 , loss : 0.576776
step 202900 , training  accuracy 0.800823
step 202900 , loss : 0.577819
step 203000 , training  accuracy 0.808642
step 203000 , loss : 0.576801
step 203100 , training  accuracy 0.801235
step 203100 , loss : 0.575557
step 203200 , training  accuracy 0.822222
step 203200 , loss : 0.571921
step 203300 , training  accuracy 0.820988
step 203300 , loss : 0.572969
step 203400 , training  accuracy 0.804938
step 203400 , loss : 0.577701
step 203500 , training  accuracy 0.809259
step 203500 , loss : 0.574567
step 203600 , training  accuracy 0.800617
step 203600 , loss : 0.581679
step 203700 , training  accuracy 0.806173
step 203700 , loss : 0.579624
step 203800 , training  accuracy 0.808025
step 203800 , loss : 0.579313
step 203900 , training  accuracy 0.812346
step 203900 , loss : 0.572335
step 204000 , training  accuracy 0.816667
step 204000 , loss : 0.573203
step 204100 , training  accuracy 0.816667
step 204100 , loss : 0.572986
step 204200 , training  accuracy 0.817901
step 204200 , loss : 0.573704
step 204300 , training  accuracy 0.819753
step 204300 , loss : 0.572634
step 204400 , training  accuracy 0.809877
step 204400 , loss : 0.576551
step 204500 , training  accuracy 0.82037
step 204500 , loss : 0.573843
step 204600 , training  accuracy 0.821605
step 204600 , loss : 0.571717
step 204700 , training  accuracy 0.806173
step 204700 , loss : 0.575854
step 204800 , training  accuracy 0.805556
step 204800 , loss : 0.576615
step 204900 , training  accuracy 0.823457
step 204900 , loss : 0.570284
step 205000 , training  accuracy 0.805556
step 205000 , loss : 0.578999
step 205100 , training  accuracy 0.826543
step 205100 , loss : 0.568498
step 205200 , training  accuracy 0.82284
step 205200 , loss : 0.573713
step 205300 , training  accuracy 0.82037
step 205300 , loss : 0.571108
step 205400 , training  accuracy 0.82037
step 205400 , loss : 0.578194
step 205500 , training  accuracy 0.819753
step 205500 , loss : 0.568748
step 205600 , training  accuracy 0.816667
step 205600 , loss : 0.573646
step 205700 , training  accuracy 0.828395
step 205700 , loss : 0.566447
step 205800 , training  accuracy 0.803704
step 205800 , loss : 0.578839
step 205900 , training  accuracy 0.817901
step 205900 , loss : 0.571748
step 206000 , training  accuracy 0.817901
step 206000 , loss : 0.571985
step 206100 , training  accuracy 0.82716
step 206100 , loss : 0.566408
step 206200 , training  accuracy 0.812963
step 206200 , loss : 0.569363
step 206300 , training  accuracy 0.829012
step 206300 , loss : 0.567487
step 206400 , training  accuracy 0.795885
step 206400 , loss : 0.585666
step 206500 , training  accuracy 0.825309
step 206500 , loss : 0.570111
step 206600 , training  accuracy 0.816049
step 206600 , loss : 0.570217
step 206700 , training  accuracy 0.821605
step 206700 , loss : 0.573454
step 206800 , training  accuracy 0.811728
step 206800 , loss : 0.577147
step 206900 , training  accuracy 0.814815
step 206900 , loss : 0.571269
step 207000 , training  accuracy 0.819753
step 207000 , loss : 0.571635
step 207100 , training  accuracy 0.821605
step 207100 , loss : 0.570113
step 207200 , training  accuracy 0.808642
step 207200 , loss : 0.576214
step 207300 , training  accuracy 0.795267
step 207300 , loss : 0.580287
step 207400 , training  accuracy 0.819753
step 207400 , loss : 0.568057
step 207500 , training  accuracy 0.805556
step 207500 , loss : 0.575108
step 207600 , training  accuracy 0.818519
step 207600 , loss : 0.568793
step 207700 , training  accuracy 0.819136
step 207700 , loss : 0.573273
step 207800 , training  accuracy 0.810494
step 207800 , loss : 0.5743
step 207900 , training  accuracy 0.814198
step 207900 , loss : 0.572755
step 208000 , training  accuracy 0.809259
step 208000 , loss : 0.575932
step 208100 , training  accuracy 0.811111
step 208100 , loss : 0.571685
step 208200 , training  accuracy 0.819136
step 208200 , loss : 0.569637
step 208300 , training  accuracy 0.809259
step 208300 , loss : 0.574716
step 208400 , training  accuracy 0.814198
step 208400 , loss : 0.57084
step 208500 , training  accuracy 0.810494
step 208500 , loss : 0.570201
step 208600 , training  accuracy 0.798148
step 208600 , loss : 0.575956
step 208700 , training  accuracy 0.802469
step 208700 , loss : 0.581442
step 208800 , training  accuracy 0.811317
step 208800 , loss : 0.574328
step 208900 , training  accuracy 0.80144
step 208900 , loss : 0.576607
step 209000 , training  accuracy 0.817284
step 209000 , loss : 0.5716
step 209100 , training  accuracy 0.821605
step 209100 , loss : 0.567769
step 209200 , training  accuracy 0.81358
step 209200 , loss : 0.573383
step 209300 , training  accuracy 0.820988
step 209300 , loss : 0.574624
step 209400 , training  accuracy 0.824074
step 209400 , loss : 0.573699
step 209500 , training  accuracy 0.821605
step 209500 , loss : 0.571333
step 209600 , training  accuracy 0.819753
step 209600 , loss : 0.570216
step 209700 , training  accuracy 0.817901
step 209700 , loss : 0.57642
step 209800 , training  accuracy 0.797119
step 209800 , loss : 0.575192
step 209900 , training  accuracy 0.808642
step 209900 , loss : 0.57713
step 210000 , training  accuracy 0.814198
step 210000 , loss : 0.56794
step 210100 , training  accuracy 0.809259
step 210100 , loss : 0.572958
step 210200 , training  accuracy 0.814198
step 210200 , loss : 0.573524
step 210300 , training  accuracy 0.81358
step 210300 , loss : 0.574417
step 210400 , training  accuracy 0.791975
step 210400 , loss : 0.581593
step 210500 , training  accuracy 0.817901
step 210500 , loss : 0.573855
step 210600 , training  accuracy 0.80144
step 210600 , loss : 0.576219
step 210700 , training  accuracy 0.826543
step 210700 , loss : 0.568487
step 210800 , training  accuracy 0.824074
step 210800 , loss : 0.56825
step 210900 , training  accuracy 0.821605
step 210900 , loss : 0.573223
step 211000 , training  accuracy 0.817901
step 211000 , loss : 0.571074
step 211100 , training  accuracy 0.806173
step 211100 , loss : 0.57658
step 211200 , training  accuracy 0.817284
step 211200 , loss : 0.570645
step 211300 , training  accuracy 0.812346
step 211300 , loss : 0.573603
step 211400 , training  accuracy 0.818519
step 211400 , loss : 0.570822
step 211500 , training  accuracy 0.824074
step 211500 , loss : 0.574199
step 211600 , training  accuracy 0.809259
step 211600 , loss : 0.572925
step 211700 , training  accuracy 0.817284
step 211700 , loss : 0.572692
step 211800 , training  accuracy 0.824691
step 211800 , loss : 0.56939
step 211900 , training  accuracy 0.823457
step 211900 , loss : 0.571301
step 212000 , training  accuracy 0.82037
step 212000 , loss : 0.56907
step 212100 , training  accuracy 0.82037
step 212100 , loss : 0.569346
step 212200 , training  accuracy 0.814198
step 212200 , loss : 0.573021
step 212300 , training  accuracy 0.826543
step 212300 , loss : 0.572442
step 212400 , training  accuracy 0.827778
step 212400 , loss : 0.569686
step 212500 , training  accuracy 0.820988
step 212500 , loss : 0.573357
step 212600 , training  accuracy 0.817901
step 212600 , loss : 0.573882
step 212700 , training  accuracy 0.816667
step 212700 , loss : 0.575676
step 212800 , training  accuracy 0.808025
step 212800 , loss : 0.572484
step 212900 , training  accuracy 0.81358
step 212900 , loss : 0.571796
step 213000 , training  accuracy 0.811728
step 213000 , loss : 0.575272
step 213100 , training  accuracy 0.819753
step 213100 , loss : 0.569693
step 213200 , training  accuracy 0.821605
step 213200 , loss : 0.57329
step 213300 , training  accuracy 0.823457
step 213300 , loss : 0.573751
step 213400 , training  accuracy 0.824074
step 213400 , loss : 0.569364
step 213500 , training  accuracy 0.817284
step 213500 , loss : 0.572653
step 213600 , training  accuracy 0.810494
step 213600 , loss : 0.572197
step 213700 , training  accuracy 0.821605
step 213700 , loss : 0.569408
step 213800 , training  accuracy 0.816667
step 213800 , loss : 0.574018
step 213900 , training  accuracy 0.804321
step 213900 , loss : 0.577397
step 214000 , training  accuracy 0.819753
step 214000 , loss : 0.575283
step 214100 , training  accuracy 0.816667
step 214100 , loss : 0.574153
step 214200 , training  accuracy 0.82037
step 214200 , loss : 0.569744
step 214300 , training  accuracy 0.819753
step 214300 , loss : 0.572703
step 214400 , training  accuracy 0.822222
step 214400 , loss : 0.568846
step 214500 , training  accuracy 0.808642
step 214500 , loss : 0.576803
step 214600 , training  accuracy 0.801235
step 214600 , loss : 0.57749
step 214700 , training  accuracy 0.805761
step 214700 , loss : 0.575029
step 214800 , training  accuracy 0.819136
step 214800 , loss : 0.570126
step 214900 , training  accuracy 0.802058
step 214900 , loss : 0.576811
step 215000 , training  accuracy 0.821605
step 215000 , loss : 0.568952
step 215100 , training  accuracy 0.817284
step 215100 , loss : 0.571823
step 215200 , training  accuracy 0.822222
step 215200 , loss : 0.567942
step 215300 , training  accuracy 0.812963
step 215300 , loss : 0.573448
step 215400 , training  accuracy 0.805144
step 215400 , loss : 0.571306
step 215500 , training  accuracy 0.805761
step 215500 , loss : 0.573061
step 215600 , training  accuracy 0.810494
step 215600 , loss : 0.570801
step 215700 , training  accuracy 0.814198
step 215700 , loss : 0.571465
step 215800 , training  accuracy 0.819136
step 215800 , loss : 0.572373
step 215900 , training  accuracy 0.824691
step 215900 , loss : 0.568327
step 216000 , training  accuracy 0.819753
step 216000 , loss : 0.56747
step 216100 , training  accuracy 0.807407
step 216100 , loss : 0.572484
step 216200 , training  accuracy 0.811111
step 216200 , loss : 0.571054
step 216300 , training  accuracy 0.817901
step 216300 , loss : 0.568908
step 216400 , training  accuracy 0.826543
step 216400 , loss : 0.56316
step 216500 , training  accuracy 0.814815
step 216500 , loss : 0.568185
step 216600 , training  accuracy 0.811111
step 216600 , loss : 0.569846
step 216700 , training  accuracy 0.811934
step 216700 , loss : 0.568581
step 216800 , training  accuracy 0.819136
step 216800 , loss : 0.567849
step 216900 , training  accuracy 0.82037
step 216900 , loss : 0.570907
step 217000 , training  accuracy 0.819136
step 217000 , loss : 0.570364
step 217100 , training  accuracy 0.803292
step 217100 , loss : 0.577256
step 217200 , training  accuracy 0.817284
step 217200 , loss : 0.568133
step 217300 , training  accuracy 0.810494
step 217300 , loss : 0.572183
step 217400 , training  accuracy 0.810082
step 217400 , loss : 0.572273
step 217500 , training  accuracy 0.809259
step 217500 , loss : 0.572933
step 217600 , training  accuracy 0.825926
step 217600 , loss : 0.569119
step 217700 , training  accuracy 0.817284
step 217700 , loss : 0.572165
step 217800 , training  accuracy 0.821605
step 217800 , loss : 0.567372
step 217900 , training  accuracy 0.809259
step 217900 , loss : 0.56901
step 218000 , training  accuracy 0.815432
step 218000 , loss : 0.57426
step 218100 , training  accuracy 0.808642
step 218100 , loss : 0.572957
step 218200 , training  accuracy 0.816667
step 218200 , loss : 0.568908
step 218300 , training  accuracy 0.816049
step 218300 , loss : 0.570035
step 218400 , training  accuracy 0.815432
step 218400 , loss : 0.570667
step 218500 , training  accuracy 0.810494
step 218500 , loss : 0.57316
step 218600 , training  accuracy 0.805761
step 218600 , loss : 0.576097
step 218700 , training  accuracy 0.821605
step 218700 , loss : 0.56825
step 218800 , training  accuracy 0.816049
step 218800 , loss : 0.570486
step 218900 , training  accuracy 0.809877
step 218900 , loss : 0.573827
step 219000 , training  accuracy 0.826543
step 219000 , loss : 0.568868
step 219100 , training  accuracy 0.818519
step 219100 , loss : 0.568723
step 219200 , training  accuracy 0.81358
step 219200 , loss : 0.569861
step 219300 , training  accuracy 0.802058
step 219300 , loss : 0.578618
step 219400 , training  accuracy 0.824691
step 219400 , loss : 0.567679
step 219500 , training  accuracy 0.819136
step 219500 , loss : 0.571879
step 219600 , training  accuracy 0.815432
step 219600 , loss : 0.574161
step 219700 , training  accuracy 0.817901
step 219700 , loss : 0.564323
step 219800 , training  accuracy 0.80679
step 219800 , loss : 0.571947
step 219900 , training  accuracy 0.814198
step 219900 , loss : 0.568994
step 220000 , training  accuracy 0.828395
step 220000 , loss : 0.565203
step 220100 , training  accuracy 0.807613
step 220100 , loss : 0.571224
step 220200 , training  accuracy 0.815432
step 220200 , loss : 0.568684
step 220300 , training  accuracy 0.812963
step 220300 , loss : 0.575053
step 220400 , training  accuracy 0.798354
step 220400 , loss : 0.579156
step 220500 , training  accuracy 0.806996
step 220500 , loss : 0.577592
step 220600 , training  accuracy 0.816667
step 220600 , loss : 0.571151
step 220700 , training  accuracy 0.814815
step 220700 , loss : 0.567878
step 220800 , training  accuracy 0.819136
step 220800 , loss : 0.566856
step 220900 , training  accuracy 0.814815
step 220900 , loss : 0.570143
step 221000 , training  accuracy 0.814198
step 221000 , loss : 0.570104
step 221100 , training  accuracy 0.815432
step 221100 , loss : 0.570767
step 221200 , training  accuracy 0.802469
step 221200 , loss : 0.571561
step 221300 , training  accuracy 0.828395
step 221300 , loss : 0.564867
step 221400 , training  accuracy 0.80823
step 221400 , loss : 0.572771
step 221500 , training  accuracy 0.811111
step 221500 , loss : 0.570641
step 221600 , training  accuracy 0.809877
step 221600 , loss : 0.56942
step 221700 , training  accuracy 0.820988
step 221700 , loss : 0.568361
step 221800 , training  accuracy 0.819136
step 221800 , loss : 0.570985
step 221900 , training  accuracy 0.812963
step 221900 , loss : 0.572461
step 222000 , training  accuracy 0.814815
step 222000 , loss : 0.569501
step 222100 , training  accuracy 0.820988
step 222100 , loss : 0.570394
step 222200 , training  accuracy 0.82037
step 222200 , loss : 0.567405
step 222300 , training  accuracy 0.809877
step 222300 , loss : 0.571443
step 222400 , training  accuracy 0.823457
step 222400 , loss : 0.568954
step 222500 , training  accuracy 0.82037
step 222500 , loss : 0.565104
step 222600 , training  accuracy 0.809259
step 222600 , loss : 0.571304
step 222700 , training  accuracy 0.823457
step 222700 , loss : 0.568417
step 222800 , training  accuracy 0.816667
step 222800 , loss : 0.569938
step 222900 , training  accuracy 0.82284
step 222900 , loss : 0.566603
step 223000 , training  accuracy 0.817901
step 223000 , loss : 0.570544
step 223100 , training  accuracy 0.807407
step 223100 , loss : 0.570885
step 223200 , training  accuracy 0.821605
step 223200 , loss : 0.568068
step 223300 , training  accuracy 0.81358
step 223300 , loss : 0.568847
step 223400 , training  accuracy 0.807407
step 223400 , loss : 0.571828
step 223500 , training  accuracy 0.821605
step 223500 , loss : 0.564915
step 223600 , training  accuracy 0.811728
step 223600 , loss : 0.571925
step 223700 , training  accuracy 0.816049
step 223700 , loss : 0.568317
step 223800 , training  accuracy 0.814815
step 223800 , loss : 0.573772
step 223900 , training  accuracy 0.817284
step 223900 , loss : 0.56968
step 224000 , training  accuracy 0.820988
step 224000 , loss : 0.569339
step 224100 , training  accuracy 0.819136
step 224100 , loss : 0.570183
step 224200 , training  accuracy 0.821605
step 224200 , loss : 0.567673
step 224300 , training  accuracy 0.819136
step 224300 , loss : 0.569595
step 224400 , training  accuracy 0.812346
step 224400 , loss : 0.570221
step 224500 , training  accuracy 0.819136
step 224500 , loss : 0.570752
step 224600 , training  accuracy 0.817284
step 224600 , loss : 0.56575
step 224700 , training  accuracy 0.826543
step 224700 , loss : 0.567385
step 224800 , training  accuracy 0.811111
step 224800 , loss : 0.573389
step 224900 , training  accuracy 0.81358
step 224900 , loss : 0.567803
step 225000 , training  accuracy 0.816667
step 225000 , loss : 0.568313
step 225100 , training  accuracy 0.82037
step 225100 , loss : 0.567848
step 225200 , training  accuracy 0.81358
step 225200 , loss : 0.56909
step 225300 , training  accuracy 0.817284
step 225300 , loss : 0.566422
step 225400 , training  accuracy 0.824074
step 225400 , loss : 0.56638
step 225500 , training  accuracy 0.819136
step 225500 , loss : 0.568726
step 225600 , training  accuracy 0.82284
step 225600 , loss : 0.568139
step 225700 , training  accuracy 0.834568
step 225700 , loss : 0.568179
step 225800 , training  accuracy 0.821605
step 225800 , loss : 0.570266
step 225900 , training  accuracy 0.819136
step 225900 , loss : 0.566767
step 226000 , training  accuracy 0.82284
step 226000 , loss : 0.567936
step 226100 , training  accuracy 0.809877
step 226100 , loss : 0.567756
step 226200 , training  accuracy 0.809259
step 226200 , loss : 0.566994
step 226300 , training  accuracy 0.819753
step 226300 , loss : 0.564127
step 226400 , training  accuracy 0.798148
step 226400 , loss : 0.576563
step 226500 , training  accuracy 0.801852
step 226500 , loss : 0.572234
step 226600 , training  accuracy 0.824074
step 226600 , loss : 0.564093
step 226700 , training  accuracy 0.812346
step 226700 , loss : 0.566761
step 226800 , training  accuracy 0.82716
step 226800 , loss : 0.561567
step 226900 , training  accuracy 0.811111
step 226900 , loss : 0.564603
step 227000 , training  accuracy 0.818519
step 227000 , loss : 0.564361
step 227100 , training  accuracy 0.82037
step 227100 , loss : 0.565129
step 227200 , training  accuracy 0.821605
step 227200 , loss : 0.564904
step 227300 , training  accuracy 0.82716
step 227300 , loss : 0.569051
step 227400 , training  accuracy 0.801235
step 227400 , loss : 0.573291
step 227500 , training  accuracy 0.826543
step 227500 , loss : 0.566851
step 227600 , training  accuracy 0.822222
step 227600 , loss : 0.5706
step 227700 , training  accuracy 0.830247
step 227700 , loss : 0.56394
step 227800 , training  accuracy 0.835185
step 227800 , loss : 0.562733
step 227900 , training  accuracy 0.830247
step 227900 , loss : 0.561026
step 228000 , training  accuracy 0.816667
step 228000 , loss : 0.568707
step 228100 , training  accuracy 0.831481
step 228100 , loss : 0.565274
step 228200 , training  accuracy 0.832716
step 228200 , loss : 0.564203
step 228300 , training  accuracy 0.823457
step 228300 , loss : 0.567963
step 228400 , training  accuracy 0.814198
step 228400 , loss : 0.568469
step 228500 , training  accuracy 0.829012
step 228500 , loss : 0.566942
step 228600 , training  accuracy 0.818519
step 228600 , loss : 0.569552
step 228700 , training  accuracy 0.822222
step 228700 , loss : 0.567795
step 228800 , training  accuracy 0.819136
step 228800 , loss : 0.569542
step 228900 , training  accuracy 0.805556
step 228900 , loss : 0.573418
step 229000 , training  accuracy 0.817901
step 229000 , loss : 0.56894
step 229100 , training  accuracy 0.815432
step 229100 , loss : 0.567276
step 229200 , training  accuracy 0.816049
step 229200 , loss : 0.567604
step 229300 , training  accuracy 0.819136
step 229300 , loss : 0.568247
step 229400 , training  accuracy 0.812963
step 229400 , loss : 0.566429
step 229500 , training  accuracy 0.819136
step 229500 , loss : 0.568048
step 229600 , training  accuracy 0.819753
step 229600 , loss : 0.568974
step 229700 , training  accuracy 0.829012
step 229700 , loss : 0.566369
step 229800 , training  accuracy 0.820988
step 229800 , loss : 0.565689
step 229900 , training  accuracy 0.825309
step 229900 , loss : 0.567513
step 230000 , training  accuracy 0.822222
step 230000 , loss : 0.565427
step 230100 , training  accuracy 0.817901
step 230100 , loss : 0.566161
step 230200 , training  accuracy 0.809877
step 230200 , loss : 0.56921
step 230300 , training  accuracy 0.822222
step 230300 , loss : 0.565987
step 230400 , training  accuracy 0.811728
step 230400 , loss : 0.570032
step 230500 , training  accuracy 0.812963
step 230500 , loss : 0.574251
step 230600 , training  accuracy 0.826543
step 230600 , loss : 0.564829
step 230700 , training  accuracy 0.812346
step 230700 , loss : 0.57172
step 230800 , training  accuracy 0.820988
step 230800 , loss : 0.562608
step 230900 , training  accuracy 0.812963
step 230900 , loss : 0.571708
step 231000 , training  accuracy 0.823457
step 231000 , loss : 0.5698
step 231100 , training  accuracy 0.811728
step 231100 , loss : 0.571875
step 231200 , training  accuracy 0.832099
step 231200 , loss : 0.566405
step 231300 , training  accuracy 0.823457
step 231300 , loss : 0.566146
step 231400 , training  accuracy 0.825926
step 231400 , loss : 0.565095
step 231500 , training  accuracy 0.825926
step 231500 , loss : 0.566546
step 231600 , training  accuracy 0.82716
step 231600 , loss : 0.561731
step 231700 , training  accuracy 0.825926
step 231700 , loss : 0.569484
step 231800 , training  accuracy 0.812963
step 231800 , loss : 0.571503
step 231900 , training  accuracy 0.835802
step 231900 , loss : 0.562423
step 232000 , training  accuracy 0.815432
step 232000 , loss : 0.566474
step 232100 , training  accuracy 0.817901
step 232100 , loss : 0.567274
step 232200 , training  accuracy 0.822222
step 232200 , loss : 0.564736
step 232300 , training  accuracy 0.829012
step 232300 , loss : 0.563469
step 232400 , training  accuracy 0.824074
step 232400 , loss : 0.568917
step 232500 , training  accuracy 0.822222
step 232500 , loss : 0.564146
step 232600 , training  accuracy 0.833951
step 232600 , loss : 0.562394
step 232700 , training  accuracy 0.820988
step 232700 , loss : 0.568445
step 232800 , training  accuracy 0.822222
step 232800 , loss : 0.566108
step 232900 , training  accuracy 0.819136
step 232900 , loss : 0.568001
step 233000 , training  accuracy 0.82284
step 233000 , loss : 0.567913
step 233100 , training  accuracy 0.827778
step 233100 , loss : 0.563009
step 233200 , training  accuracy 0.831481
step 233200 , loss : 0.562867
step 233300 , training  accuracy 0.830247
step 233300 , loss : 0.563536
step 233400 , training  accuracy 0.825926
step 233400 , loss : 0.564812
step 233500 , training  accuracy 0.816049
step 233500 , loss : 0.567295
step 233600 , training  accuracy 0.829012
step 233600 , loss : 0.566964
step 233700 , training  accuracy 0.82037
step 233700 , loss : 0.565859
step 233800 , training  accuracy 0.819753
step 233800 , loss : 0.567164
step 233900 , training  accuracy 0.823457
step 233900 , loss : 0.561478
step 234000 , training  accuracy 0.82037
step 234000 , loss : 0.566828
step 234100 , training  accuracy 0.808642
step 234100 , loss : 0.574849
step 234200 , training  accuracy 0.811111
step 234200 , loss : 0.568356
step 234300 , training  accuracy 0.816667
step 234300 , loss : 0.566508
step 234400 , training  accuracy 0.824074
step 234400 , loss : 0.562445
step 234500 , training  accuracy 0.800617
step 234500 , loss : 0.575049
step 234600 , training  accuracy 0.81358
step 234600 , loss : 0.571097
step 234700 , training  accuracy 0.819136
step 234700 , loss : 0.563179
step 234800 , training  accuracy 0.824074
step 234800 , loss : 0.5647
step 234900 , training  accuracy 0.824691
step 234900 , loss : 0.565456
step 235000 , training  accuracy 0.824691
step 235000 , loss : 0.564762
step 235100 , training  accuracy 0.815432
step 235100 , loss : 0.567919
step 235200 , training  accuracy 0.829012
step 235200 , loss : 0.565624
step 235300 , training  accuracy 0.817284
step 235300 , loss : 0.564474
step 235400 , training  accuracy 0.817901
step 235400 , loss : 0.568475
step 235500 , training  accuracy 0.803292
step 235500 , loss : 0.573456
step 235600 , training  accuracy 0.819753
step 235600 , loss : 0.566127
step 235700 , training  accuracy 0.816667
step 235700 , loss : 0.565397
step 235800 , training  accuracy 0.814815
step 235800 , loss : 0.567532
step 235900 , training  accuracy 0.819136
step 235900 , loss : 0.566902
step 236000 , training  accuracy 0.815432
step 236000 , loss : 0.568578
step 236100 , training  accuracy 0.811111
step 236100 , loss : 0.573266
step 236200 , training  accuracy 0.821605
step 236200 , loss : 0.565758
step 236300 , training  accuracy 0.817284
step 236300 , loss : 0.563795
step 236400 , training  accuracy 0.816667
step 236400 , loss : 0.570661
step 236500 , training  accuracy 0.832099
step 236500 , loss : 0.565056
step 236600 , training  accuracy 0.811317
step 236600 , loss : 0.568346
step 236700 , training  accuracy 0.812346
step 236700 , loss : 0.568032
step 236800 , training  accuracy 0.82284
step 236800 , loss : 0.569094
step 236900 , training  accuracy 0.81358
step 236900 , loss : 0.567942
step 237000 , training  accuracy 0.824074
step 237000 , loss : 0.564479
step 237100 , training  accuracy 0.795267
step 237100 , loss : 0.580209
step 237200 , training  accuracy 0.822222
step 237200 , loss : 0.563407
step 237300 , training  accuracy 0.825926
step 237300 , loss : 0.562547
step 237400 , training  accuracy 0.821605
step 237400 , loss : 0.564853
step 237500 , training  accuracy 0.829012
step 237500 , loss : 0.566006
step 237600 , training  accuracy 0.82284
step 237600 , loss : 0.567429
step 237700 , training  accuracy 0.828395
step 237700 , loss : 0.56158
step 237800 , training  accuracy 0.82284
step 237800 , loss : 0.563255
step 237900 , training  accuracy 0.812963
step 237900 , loss : 0.569266
step 238000 , training  accuracy 0.825926
step 238000 , loss : 0.564054
step 238100 , training  accuracy 0.819136
step 238100 , loss : 0.566841
step 238200 , training  accuracy 0.811317
step 238200 , loss : 0.569395
step 238300 , training  accuracy 0.82037
step 238300 , loss : 0.56842
step 238400 , training  accuracy 0.819342
step 238400 , loss : 0.56575
step 238500 , training  accuracy 0.82716
step 238500 , loss : 0.562419
step 238600 , training  accuracy 0.819342
step 238600 , loss : 0.566179
step 238700 , training  accuracy 0.827778
step 238700 , loss : 0.562961
step 238800 , training  accuracy 0.830864
step 238800 , loss : 0.561097
step 238900 , training  accuracy 0.82037
step 238900 , loss : 0.564378
step 239000 , training  accuracy 0.831481
step 239000 , loss : 0.564437
step 239100 , training  accuracy 0.832099
step 239100 , loss : 0.562904
step 239200 , training  accuracy 0.817284
step 239200 , loss : 0.564438
step 239300 , training  accuracy 0.815432
step 239300 , loss : 0.566254
step 239400 , training  accuracy 0.827778
step 239400 , loss : 0.563428
step 239500 , training  accuracy 0.825309
step 239500 , loss : 0.567638
step 239600 , training  accuracy 0.816049
step 239600 , loss : 0.563776
step 239700 , training  accuracy 0.82716
step 239700 , loss : 0.564726
step 239800 , training  accuracy 0.81358
step 239800 , loss : 0.569325
step 239900 , training  accuracy 0.812346
step 239900 , loss : 0.570075
step 240000 , training  accuracy 0.80679
step 240000 , loss : 0.568149
step 240100 , training  accuracy 0.81358
step 240100 , loss : 0.570494
step 240200 , training  accuracy 0.81358
step 240200 , loss : 0.571557
step 240300 , training  accuracy 0.820988
step 240300 , loss : 0.563563
step 240400 , training  accuracy 0.817901
step 240400 , loss : 0.564689
step 240500 , training  accuracy 0.804938
step 240500 , loss : 0.569431
step 240600 , training  accuracy 0.816049
step 240600 , loss : 0.57026
step 240700 , training  accuracy 0.824074
step 240700 , loss : 0.568114
step 240800 , training  accuracy 0.800617
step 240800 , loss : 0.574215
step 240900 , training  accuracy 0.793827
step 240900 , loss : 0.571365
step 241000 , training  accuracy 0.810494
step 241000 , loss : 0.567971
step 241100 , training  accuracy 0.803086
step 241100 , loss : 0.570305
step 241200 , training  accuracy 0.826543
step 241200 , loss : 0.563857
step 241300 , training  accuracy 0.824074
step 241300 , loss : 0.564555
step 241400 , training  accuracy 0.817901
step 241400 , loss : 0.571024
step 241500 , training  accuracy 0.809877
step 241500 , loss : 0.568421
step 241600 , training  accuracy 0.819136
step 241600 , loss : 0.564881
step 241700 , training  accuracy 0.818519
step 241700 , loss : 0.56801
step 241800 , training  accuracy 0.802469
step 241800 , loss : 0.571819
step 241900 , training  accuracy 0.808025
step 241900 , loss : 0.564524
step 242000 , training  accuracy 0.811111
step 242000 , loss : 0.568527
step 242100 , training  accuracy 0.822222
step 242100 , loss : 0.567091
step 242200 , training  accuracy 0.819753
step 242200 , loss : 0.567517
step 242300 , training  accuracy 0.808642
step 242300 , loss : 0.569993
step 242400 , training  accuracy 0.824691
step 242400 , loss : 0.564665
step 242500 , training  accuracy 0.817901
step 242500 , loss : 0.565466
step 242600 , training  accuracy 0.814815
step 242600 , loss : 0.571808
step 242700 , training  accuracy 0.810494
step 242700 , loss : 0.573048
step 242800 , training  accuracy 0.82284
step 242800 , loss : 0.562674
step 242900 , training  accuracy 0.798148
step 242900 , loss : 0.571672
step 243000 , training  accuracy 0.807407
step 243000 , loss : 0.575095
step 243100 , training  accuracy 0.823457
step 243100 , loss : 0.563763
step 243200 , training  accuracy 0.811728
step 243200 , loss : 0.566038
step 243300 , training  accuracy 0.806173
step 243300 , loss : 0.5696
step 243400 , training  accuracy 0.821605
step 243400 , loss : 0.566401
step 243500 , training  accuracy 0.806173
step 243500 , loss : 0.572961
step 243600 , training  accuracy 0.810494
step 243600 , loss : 0.568607
step 243700 , training  accuracy 0.811728
step 243700 , loss : 0.565902
step 243800 , training  accuracy 0.807407
step 243800 , loss : 0.569506
step 243900 , training  accuracy 0.8
step 243900 , loss : 0.57041
step 244000 , training  accuracy 0.801235
step 244000 , loss : 0.575405
step 244100 , training  accuracy 0.812963
step 244100 , loss : 0.571662
step 244200 , training  accuracy 0.814198
step 244200 , loss : 0.568554
step 244300 , training  accuracy 0.824074
step 244300 , loss : 0.565796
step 244400 , training  accuracy 0.80679
step 244400 , loss : 0.569865
step 244500 , training  accuracy 0.817284
step 244500 , loss : 0.569935
step 244600 , training  accuracy 0.814198
step 244600 , loss : 0.567087
step 244700 , training  accuracy 0.816667
step 244700 , loss : 0.564885
step 244800 , training  accuracy 0.829012
step 244800 , loss : 0.566536
step 244900 , training  accuracy 0.826543
step 244900 , loss : 0.564869
step 245000 , training  accuracy 0.81358
step 245000 , loss : 0.566815
step 245100 , training  accuracy 0.825309
step 245100 , loss : 0.562129
step 245200 , training  accuracy 0.806173
step 245200 , loss : 0.575195
step 245300 , training  accuracy 0.816667
step 245300 , loss : 0.569228
step 245400 , training  accuracy 0.80679
step 245400 , loss : 0.572559
step 245500 , training  accuracy 0.819753
step 245500 , loss : 0.565574
step 245600 , training  accuracy 0.821605
step 245600 , loss : 0.568665
step 245700 , training  accuracy 0.808642
step 245700 , loss : 0.569199
step 245800 , training  accuracy 0.811728
step 245800 , loss : 0.568479
step 245900 , training  accuracy 0.820988
step 245900 , loss : 0.562982
step 246000 , training  accuracy 0.812346
step 246000 , loss : 0.569618
step 246100 , training  accuracy 0.812346
step 246100 , loss : 0.566031
step 246200 , training  accuracy 0.817284
step 246200 , loss : 0.568569
step 246300 , training  accuracy 0.817284
step 246300 , loss : 0.566316
step 246400 , training  accuracy 0.820988
step 246400 , loss : 0.561486
step 246500 , training  accuracy 0.824691
step 246500 , loss : 0.563417
step 246600 , training  accuracy 0.809259
step 246600 , loss : 0.568803
step 246700 , training  accuracy 0.816667
step 246700 , loss : 0.562124
step 246800 , training  accuracy 0.814815
step 246800 , loss : 0.562791
step 246900 , training  accuracy 0.817284
step 246900 , loss : 0.565506
step 247000 , training  accuracy 0.815432
step 247000 , loss : 0.569985
step 247100 , training  accuracy 0.830864
step 247100 , loss : 0.560059
step 247200 , training  accuracy 0.818519
step 247200 , loss : 0.565219
step 247300 , training  accuracy 0.820988
step 247300 , loss : 0.565653
step 247400 , training  accuracy 0.82284
step 247400 , loss : 0.565203
step 247500 , training  accuracy 0.809259
step 247500 , loss : 0.570274
step 247600 , training  accuracy 0.815432
step 247600 , loss : 0.567755
step 247700 , training  accuracy 0.830864
step 247700 , loss : 0.558821
step 247800 , training  accuracy 0.816667
step 247800 , loss : 0.562591
step 247900 , training  accuracy 0.830864
step 247900 , loss : 0.562826
step 248000 , training  accuracy 0.822222
step 248000 , loss : 0.566667
step 248100 , training  accuracy 0.812346
step 248100 , loss : 0.570024
step 248200 , training  accuracy 0.812346
step 248200 , loss : 0.568292
step 248300 , training  accuracy 0.825926
step 248300 , loss : 0.562835
step 248400 , training  accuracy 0.812963
step 248400 , loss : 0.570207
step 248500 , training  accuracy 0.812963
step 248500 , loss : 0.565272
step 248600 , training  accuracy 0.824074
step 248600 , loss : 0.564066
step 248700 , training  accuracy 0.816667
step 248700 , loss : 0.564792
step 248800 , training  accuracy 0.82037
step 248800 , loss : 0.564709
step 248900 , training  accuracy 0.824691
step 248900 , loss : 0.561292
step 249000 , training  accuracy 0.82037
step 249000 , loss : 0.562925
step 249100 , training  accuracy 0.824691
step 249100 , loss : 0.561123
step 249200 , training  accuracy 0.82037
step 249200 , loss : 0.561764
step 249300 , training  accuracy 0.824074
step 249300 , loss : 0.565711
step 249400 , training  accuracy 0.821605
step 249400 , loss : 0.564058
step 249500 , training  accuracy 0.829012
step 249500 , loss : 0.56499
step 249600 , training  accuracy 0.809877
step 249600 , loss : 0.56678
step 249700 , training  accuracy 0.822222
step 249700 , loss : 0.5665
step 249800 , training  accuracy 0.822222
step 249800 , loss : 0.560417
step 249900 , training  accuracy 0.817284
step 249900 , loss : 0.566594
step 250000 , training  accuracy 0.821605
step 250000 , loss : 0.562763
step 250100 , training  accuracy 0.829012
step 250100 , loss : 0.563065
step 250200 , training  accuracy 0.832099
step 250200 , loss : 0.560885
step 250300 , training  accuracy 0.834568
step 250300 , loss : 0.561681
step 250400 , training  accuracy 0.825926
step 250400 , loss : 0.562809
step 250500 , training  accuracy 0.816049
step 250500 , loss : 0.562367
step 250600 , training  accuracy 0.823457
step 250600 , loss : 0.562538
step 250700 , training  accuracy 0.819753
step 250700 , loss : 0.563133
step 250800 , training  accuracy 0.814815
step 250800 , loss : 0.568117
step 250900 , training  accuracy 0.808025
step 250900 , loss : 0.567186
step 251000 , training  accuracy 0.816667
step 251000 , loss : 0.563359
step 251100 , training  accuracy 0.815432
step 251100 , loss : 0.566926
step 251200 , training  accuracy 0.804321
step 251200 , loss : 0.564519
step 251300 , training  accuracy 0.817901
step 251300 , loss : 0.563778
step 251400 , training  accuracy 0.812963
step 251400 , loss : 0.566938
step 251500 , training  accuracy 0.816049
step 251500 , loss : 0.562286
step 251600 , training  accuracy 0.808642
step 251600 , loss : 0.563438
step 251700 , training  accuracy 0.821605
step 251700 , loss : 0.557221
step 251800 , training  accuracy 0.811728
step 251800 , loss : 0.559728
step 251900 , training  accuracy 0.821605
step 251900 , loss : 0.56333
step 252000 , training  accuracy 0.824691
step 252000 , loss : 0.561877
step 252100 , training  accuracy 0.825926
step 252100 , loss : 0.559007
step 252200 , training  accuracy 0.808025
step 252200 , loss : 0.563214
step 252300 , training  accuracy 0.812963
step 252300 , loss : 0.568392
step 252400 , training  accuracy 0.810494
step 252400 , loss : 0.565587
step 252500 , training  accuracy 0.82284
step 252500 , loss : 0.565406
step 252600 , training  accuracy 0.827778
step 252600 , loss : 0.562091
step 252700 , training  accuracy 0.819753
step 252700 , loss : 0.564327
step 252800 , training  accuracy 0.826543
step 252800 , loss : 0.558671
step 252900 , training  accuracy 0.82284
step 252900 , loss : 0.563064
step 253000 , training  accuracy 0.832716
step 253000 , loss : 0.561972
step 253100 , training  accuracy 0.825926
step 253100 , loss : 0.56343
step 253200 , training  accuracy 0.822222
step 253200 , loss : 0.5656
step 253300 , training  accuracy 0.818519
step 253300 , loss : 0.563223
step 253400 , training  accuracy 0.82037
step 253400 , loss : 0.558328
step 253500 , training  accuracy 0.819753
step 253500 , loss : 0.567405
step 253600 , training  accuracy 0.816049
step 253600 , loss : 0.571609
step 253700 , training  accuracy 0.814815
step 253700 , loss : 0.566577
step 253800 , training  accuracy 0.805556
step 253800 , loss : 0.571038
step 253900 , training  accuracy 0.816049
step 253900 , loss : 0.564954
step 254000 , training  accuracy 0.821605
step 254000 , loss : 0.561756
step 254100 , training  accuracy 0.808642
step 254100 , loss : 0.567901
step 254200 , training  accuracy 0.820988
step 254200 , loss : 0.558185
step 254300 , training  accuracy 0.834568
step 254300 , loss : 0.557466
step 254400 , training  accuracy 0.82284
step 254400 , loss : 0.56005
step 254500 , training  accuracy 0.815432
step 254500 , loss : 0.563202
step 254600 , training  accuracy 0.81358
step 254600 , loss : 0.563779
step 254700 , training  accuracy 0.816667
step 254700 , loss : 0.566977
step 254800 , training  accuracy 0.821605
step 254800 , loss : 0.566129
step 254900 , training  accuracy 0.823457
step 254900 , loss : 0.561552


#conv Neural Network
# tensorboard --logdir=/home/ncc/notebook/learn/tensorboard/log

import numpy as np 
import tensorflow as tf

import math
import time
import matplotlib.pyplot as plt
%matplotlib inline
import os 


sess = tf.InteractiveSession()
file_locate='/mnt/Jupyter/seongjung_mnt/Eye/Numpy_eye_np_64_N_vs_Un/'
test_img=np.load(file_locate+'test_img.npy');
print np.shape(test_img)
img_row = np.shape(test_img)[1]
img_col = np.shape(test_img)[1]


print img_row ,img_col
n_classes =2
in_ch =3
out_ch1=512
out_ch2=512
out_ch3=512
out_ch4=512
out_ch5=512
fully_ch1=1024

x= tf.placeholder("float",shape=[None,img_col , img_row , in_ch],  name = 'x-input')
y_=tf.placeholder("float",shape=[None , n_classes] , name = 'y-input')
keep_prob = tf.placeholder("float")

x_image= tf.reshape(x,[-1,img_row,img_col,in_ch])

iterate=300000
batch_size=20




weight_row =3 ; weight_col=3

pooling_row_size1=int(img_row/2)
pooling_row_size2=int(pooling_row_size1/2)
pooling_row_size3=int(pooling_row_size2/2)
pooling_row_size4=int(pooling_row_size3/2)
pooling_col_size1=int(img_col/2)
pooling_col_size2=int(pooling_col_size1/2)
pooling_col_size3=int(pooling_col_size2/2)
pooling_col_size4=int(pooling_col_size3/2)

print img_col , img_row
print pooling_row_size1 , pooling_col_size1
print pooling_row_size2 , pooling_col_size2
print pooling_row_size3 , pooling_col_size3
print pooling_row_size4 , pooling_col_size4
